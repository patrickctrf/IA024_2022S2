{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "ex02_175480_Patrick_Ferreira.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/patrickctrf/IA024_2022S2/blob/main/ex02_175480/ex02_175480_Patrick_Ferreira.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od7iUgHy5SSi"
   },
   "source": [
    "# Aula 2: Análise de Sentimentos usando Bag of Words\n",
    "\n",
    "Neste notebook iremos treinar um rede de uma única camada para fazer análise de sentimento usando o dataset IMDB."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "nome = \"Patrick de Carvalho Tavares Rezende Ferreira\"\n",
    "print(f'Meu nome é {nome}')"
   ],
   "metadata": {
    "id": "3vAaCL0u-zg3",
    "outputId": "f5f754e4-97f0-4693-8895-50c200eb0d19",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu nome é Patrick de Carvalho Tavares Rezende Ferreira\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importando as bibliotecas necessárias"
   ],
   "metadata": {
    "id": "Mhv5U8Muiyz_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from typing import List"
   ],
   "metadata": {
    "id": "qzTCVXoOiyIs"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXFdJz2KVeQw"
   },
   "source": [
    "# Preparando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHMi_Kq65fPM"
   },
   "source": [
    "Primeiro, fazemos download do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2wbnfzst5O3k",
    "outputId": "0d2a9649-4541-49cd-98e8-e6f6e980fc33",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "!wget -nc http: // files.fast.ai / data / examples / imdb_sample.tgz\n",
    "!tar -xzf imdb_sample.tgz"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-31 00:32:03--  ftp://http/\r\n",
      "           => ‘.listing’\r\n",
      "Resolving http (http)... failed: Temporary failure in name resolution.\r\n",
      "wget: unable to resolve host address ‘http’\r\n",
      "//: Scheme missing.\r\n",
      "File ‘index.html’ already there; not retrieving.\r\n",
      "\r\n",
      "/: Scheme missing.\r\n",
      "File ‘index.html’ already there; not retrieving.\r\n",
      "\r\n",
      "/: Scheme missing.\r\n",
      "File ‘index.html’ already there; not retrieving.\r\n",
      "\r\n",
      "/: Scheme missing.\r\n",
      "File ‘index.html’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Giyi5Rv_NIm"
   },
   "source": [
    "Carregamos o dataset .csv usando o pandas:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0HIN_xLI_TuT",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "outputId": "a46f5a7b-abfa-4aaa-ef8f-66c63ade5720"
   },
   "source": [
    "df = pd.read_csv('imdb_sample/texts.csv')\n",
    "df.shape\n",
    "df.head()"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "      label                                               text  is_valid\n0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n1  positive  This is a extremely well-made film. The acting...     False\n2  negative  Every once in a long while a movie will come a...     False\n3  positive  Name just says it all. I watched this movie wi...     False\n4  negative  This movie succeeds at being one of the most u...     False",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n      <th>is_valid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>positive</td>\n      <td>This is a extremely well-made film. The acting...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>Every once in a long while a movie will come a...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>positive</td>\n      <td>Name just says it all. I watched this movie wi...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>negative</td>\n      <td>This movie succeeds at being one of the most u...</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8dfjdJ-AV79"
   },
   "source": [
    "Iremos agora apenas selecionar 100 exemplos de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KCoftmPmAfXE",
    "outputId": "ff061e38-175b-4cf6-ec51-dc1ded494cd3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "treino = df[df['is_valid'] == False]  # Apenas treinamento, isto é, descartamos o dataset de validação.\n",
    "\n",
    "print('treino.shape original:', treino.shape)\n",
    "\n",
    "treino = treino[:100]  # Aqui truncamos o dataset para os 100 primeiros exemplos. \n",
    "\n",
    "print('treino.shape depois:', treino.shape)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treino.shape original: (800, 3)\n",
      "treino.shape depois: (100, 3)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHus6FH7DftH"
   },
   "source": [
    "Iremos dividir este conjunto em entrada (X) e saída desejada (Y, target) e converter as strings \"positive\" e \"negative\" do target para valores booleanos:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "46RdLFLkEW-X",
    "outputId": "68c6cd59-c51c-4a17-d67c-366e51b45a86",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "X_treino = treino['text']\n",
    "Y_treino = treino['label']\n",
    "\n",
    "print(f'Primeiras linhas de X_treino:\\n{X_treino.head()}\\n')\n",
    "print(f'Primeiras linhas de Y_treino:\\n{Y_treino.head()}\\n')\n",
    "\n",
    "mapeamento = {'positive': True, 'negative': False}\n",
    "Y_treino = Y_treino.map(mapeamento)\n",
    "Y_treino = torch.tensor(Y_treino.values, dtype=torch.long)\n",
    "print(f'Tamanho de Y_treino: {Y_treino.shape}')\n",
    "print(f'5 primeiras linhas de Y_treino: {Y_treino[:5]}')\n",
    "print(f'Número de exemplos positivos: {(Y_treino == True).sum()}')\n",
    "print(f'Número de exemplos negativos: {(Y_treino == False).sum()}')"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiras linhas de X_treino:\n",
      "0    Un-bleeping-believable! Meg Ryan doesn't even ...\n",
      "1    This is a extremely well-made film. The acting...\n",
      "2    Every once in a long while a movie will come a...\n",
      "3    Name just says it all. I watched this movie wi...\n",
      "4    This movie succeeds at being one of the most u...\n",
      "Name: text, dtype: object\n",
      "\n",
      "Primeiras linhas de Y_treino:\n",
      "0    negative\n",
      "1    positive\n",
      "2    negative\n",
      "3    positive\n",
      "4    negative\n",
      "Name: label, dtype: object\n",
      "\n",
      "Tamanho de Y_treino: torch.Size([100])\n",
      "5 primeiras linhas de Y_treino: tensor([0, 1, 0, 1, 0])\n",
      "Número de exemplos positivos: 51\n",
      "Número de exemplos negativos: 49\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLlaPgP0Z_D4"
   },
   "source": [
    "# Definindo o tokenizador\n",
    "\n",
    "Agora temos a função de tokenização, isto é, que converte strings para tokens."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YIpp1C_qZ-QX"
   },
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def tokenize(text: str):\n",
    "    \"\"\"\n",
    "    Convert string to a list of tokens (i.e., words).\n",
    "    This function lower cases everything and removes punctuation.\n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub('[' + string.punctuation + ']', '', text).lower().split()"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando a função com um exemplo simples\n"
   ],
   "metadata": {
    "id": "uE7kwbIYlkPn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "assert tokenize(\"?I? lik.e, t\\'o ea!t pizza.\") == ['i', 'like', 'to', 'eat', 'pizza'], \"Não passou no assert.\"\n",
    "print('Passou no assert!')"
   ],
   "metadata": {
    "id": "iS6QbpUwifyY",
    "outputId": "8d4dd79b-b038-4f17-c627-81d2d9a2a133",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passou no assert!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Definindo o vocabulário\n",
    "\n",
    "Selecionaremos os `max_tokens` (ex: 1000) tokens mais frequentes do dataset de treino como sendo nosso vocabulário."
   ],
   "metadata": {
    "id": "gbf_yO6XJ_hY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def create_vocab(texts: List[str], max_tokens: int):\n",
    "    \"\"\"\n",
    "    Returns a dictionary whose keys are tokens and values are token ids (from 0 to max_tokens - 1).\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for t in texts:\n",
    "        tokens.extend(tokenize(t))\n",
    "\n",
    "    return dict(Counter(tokens).most_common(max_tokens))"
   ],
   "metadata": {
    "id": "jVc8uucLK4pP"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando a função\n"
   ],
   "metadata": {
    "id": "aMYyPXpYlr64"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "L = ['f', 'a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a', 'd']\n",
    "k = 3\n",
    "resultado = create_vocab(L, k)\n",
    "\n",
    "assert f'resultado: {resultado}' == \"resultado: {'a': 4, 'd': 3, 'e': 3}\"\n",
    "print(\"Passou no assert!\")"
   ],
   "metadata": {
    "id": "fF_9u5e4FdD5"
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passou no assert!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Função para converter string para Bag-of-words"
   ],
   "metadata": {
    "id": "wki1S4LMKCrh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def convert_to_bow(text: str, vocab):\n",
    "    \"\"\"\n",
    "    Returns a bag-of-word vector of size len(vocab).\n",
    "    \"\"\"\n",
    "    bow = np.zeros(len(vocab))\n",
    "    vocab_words = list(vocab.keys())\n",
    "\n",
    "    for token in tokenize(text):\n",
    "        if token in vocab_words:\n",
    "            bow[vocab_words.index(token)] = 1\n",
    "\n",
    "    return torch.tensor(bow)"
   ],
   "metadata": {
    "id": "4TdUgA5RMl_4"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando a função"
   ],
   "metadata": {
    "id": "jLnkDcWil-Jd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X_assert = [\"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\",\n",
    "            \"Peter Piper picked a peck of pickled peppers. How many pickled peppers did Peter Piper pick?\",\n",
    "            \"She saw Sharif's shoes on the sofa. But was she so sure those were Sharif's shoes she saw?\", ]\n",
    "\n",
    "vocab = create_vocab(X_assert, max_tokens=1000)\n",
    "\n",
    "bow = convert_to_bow(X_assert[0], vocab)\n",
    "\n",
    "assert bow.tolist() == [1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]\n",
    "print(\"Passou no Assert!\")"
   ],
   "metadata": {
    "id": "kTu0EBGVGw_8"
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passou no Assert!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_6pddDHEM_r"
   },
   "source": [
    "## Definindo a Rede Neural\n",
    "\n",
    "**Entrada:**\n",
    "\n",
    "$x \\in R^{B \\times |V|}$     (bag-of-words)\n",
    "\n",
    "**Parametros:**\n",
    "\n",
    "$W \\in R^{|V| \\times K}$    (weights: matriz de pesos)\n",
    "\n",
    "$b \\in R^{K}$    (bias/viés)\n",
    "\n",
    "**Saída:**\n",
    "\n",
    "$p \\in R^{B \\times K}$  (probabilidade de cada classe)\n",
    "\n",
    "\n",
    "**Onde:**\n",
    "\n",
    "$K$ = número de classes\n",
    "\n",
    "$B$ = tamanho do batch\n",
    "\n",
    "$|V|$ = tamanho do vocabulário\n",
    "\n",
    "**Definição da rede:**\n",
    "\n",
    "$z = xW + b$   (camada linear. $z$ é chamado de logits)\n",
    "\n",
    "$p_i = \\frac{e^{z_i}}{\\sum_{j=0}^{K-1} e^{z_j}}$   (softmax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oLReRSuDEPLL"
   },
   "source": [
    "class MyModel():\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        self.weights = (2 * torch.randn((dim, 2), dtype=torch.double) - 1) * 0.01\n",
    "        self.bias = (2 * torch.randn((2,), dtype=torch.double) - 1) * 0.01\n",
    "\n",
    "        self.weights.requires_grad = self.bias.requires_grad = True\n",
    "\n",
    "    def __call__(self, x):\n",
    "        network_output = torch.matmul(x.double(), self.weights) - self.bias\n",
    "        probs = torch.softmax(network_output, -1)\n",
    "        return probs"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando modelo com uma entrada aleatória\n",
    "\n",
    "Escreva abaixo um pequeno código para testar se seu modelo processa uma matriz de entrada de tamanho `batch_size, dim`, ou seja, a matriz contém `batch_size` exemplos, cada um sendo representado por um vetor de tamanho `dim`."
   ],
   "metadata": {
    "id": "_8eWRvN79f4O"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 16\n",
    "dim = 8\n",
    "model = MyModel(dim)\n",
    "x = torch.randn(batch_size, dim)\n",
    "probs = model(x)\n",
    "\n",
    "print(probs)"
   ],
   "metadata": {
    "id": "R-lupvM_HxOJ"
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5197, 0.4803],\n",
      "        [0.5139, 0.4861],\n",
      "        [0.5079, 0.4921],\n",
      "        [0.5351, 0.4649],\n",
      "        [0.5565, 0.4435],\n",
      "        [0.5102, 0.4898],\n",
      "        [0.5167, 0.4833],\n",
      "        [0.5158, 0.4842],\n",
      "        [0.5294, 0.4706],\n",
      "        [0.5219, 0.4781],\n",
      "        [0.5116, 0.4884],\n",
      "        [0.5177, 0.4823],\n",
      "        [0.5642, 0.4358],\n",
      "        [0.4925, 0.5075],\n",
      "        [0.5047, 0.4953],\n",
      "        [0.5065, 0.4935]], dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Função de custo Entropia Cruzada\n",
    "\n",
    "$y \\in R^{K}$  (target),\n",
    "\n",
    "a equação da entropia cruzada associada a um exemplo é dada por:\n",
    "\n",
    "$L = \\sum_{i=0}^{K-1} -y_i \\log p_i$   (esta é a loss por exemplo)\n",
    "\n",
    "Se $y$ for um vetor one-hot (apenas um dos elementos é diferente de zero), podemos simplicar a equação acima para:\n",
    "\n",
    "$L = -\\log p_i$\n",
    "\n",
    "Onde $i$ é o indice da classe correta. Ou seja, $p_i$ é a probabilidade que o modelo colocou na classe correta.\n",
    "\n",
    "A função de custo é a **média** da entropia cruzada de cada exemplo no batch."
   ],
   "metadata": {
    "id": "O-EXAYdOvGcF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def cross_entropy_loss(probs, targets):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      probs: a float32 matrix of shape (batch_size, number of classes)\n",
    "      targets: a long (int64) array of shape (batch_size)\n",
    "\n",
    "    Returns:\n",
    "      Mean loss in the batch.\n",
    "    \"\"\"\n",
    "    # Rescreva o código abaixo sem usar laço.\n",
    "    # batch_size = probs.shape[0]\n",
    "    # losses = []\n",
    "    # for i in range(batch_size):\n",
    "    #     losses.append(-torch.log(probs[i, targets[i]]))\n",
    "    #\n",
    "    # losses = torch.stack(losses)\n",
    "    targets_one_hot = torch.nn.functional.one_hot(targets, num_classes=2)\n",
    "    return (targets_one_hot * probs).sum(axis=-1).mean()"
   ],
   "metadata": {
    "id": "YyHefTffhgmx"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando a função entropia cruzada com probabilidades de 50%\n",
    "\n",
    "Escreva abaixo um pequeno código para testar se a entropia cruzada confere com a resposta do problema 3.6 do exercício da semana passada. Crie um tensor para as probabilidades (50%) e um target também aleatório balanceado e calcule a cross entropia. Qual é o valor esperado da cross entropia nesse caso?"
   ],
   "metadata": {
    "id": "kIgECDC5uqRo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# escreva seu código aqui"
   ],
   "metadata": {
    "id": "-10K5Jecve--"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convertendo dataset de treino para uma matriz de bag-of-words"
   ],
   "metadata": {
    "id": "jWcTeNuJF1Mp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "vocab = create_vocab(X_treino, max_tokens=1000)\n",
    "bows = []\n",
    "for text in X_treino:\n",
    "    bow = convert_to_bow(text, vocab)\n",
    "    bows.append(bow)\n",
    "\n",
    "X = torch.stack(bows)\n",
    "print(X.shape)"
   ],
   "metadata": {
    "id": "kGIFOJZcFs7b"
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1000])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Laço de Treinamento"
   ],
   "metadata": {
    "id": "OSdBQozHlT59"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "num_iterations = 100\n",
    "learning_rate = 1.5\n",
    "\n",
    "model = MyModel(dim=len(vocab))\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Zera os gradientes\n",
    "    if model.weights.grad is not None:\n",
    "        model.weights.grad.data.zero_()\n",
    "        model.bias.grad.data.zero_()\n",
    "\n",
    "    probs = model(X)\n",
    "    loss = cross_entropy_loss(probs, Y_treino)\n",
    "    print(f'iteration: {i}  loss: {loss:.6f}  exp(loss): {torch.exp(loss):.4f}')\n",
    "    loss.backward()\n",
    "\n",
    "    #Atualiza os pesos\n",
    "    model.weights.data = model.weights.data - learning_rate * model.weights.grad.data\n",
    "    model.bias.data = model.bias.data - learning_rate * model.bias.grad.data"
   ],
   "metadata": {
    "id": "7j3a3FsBlWCy"
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0  loss: 0.487749  exp(loss): 1.6286\n",
      "iteration: 1  loss: 0.356911  exp(loss): 1.4289\n",
      "iteration: 2  loss: 0.418367  exp(loss): 1.5195\n",
      "iteration: 3  loss: 0.378514  exp(loss): 1.4601\n",
      "iteration: 4  loss: 0.424852  exp(loss): 1.5294\n",
      "iteration: 5  loss: 0.224874  exp(loss): 1.2522\n",
      "iteration: 6  loss: 0.449845  exp(loss): 1.5681\n",
      "iteration: 7  loss: 0.223017  exp(loss): 1.2498\n",
      "iteration: 8  loss: 0.447841  exp(loss): 1.5649\n",
      "iteration: 9  loss: 0.250297  exp(loss): 1.2844\n",
      "iteration: 10  loss: 0.391258  exp(loss): 1.4788\n",
      "iteration: 11  loss: 0.139870  exp(loss): 1.1501\n",
      "iteration: 12  loss: 0.246465  exp(loss): 1.2795\n",
      "iteration: 13  loss: 0.298586  exp(loss): 1.3480\n",
      "iteration: 14  loss: 0.149915  exp(loss): 1.1617\n",
      "iteration: 15  loss: 0.164458  exp(loss): 1.1788\n",
      "iteration: 16  loss: 0.191188  exp(loss): 1.2107\n",
      "iteration: 17  loss: 0.158788  exp(loss): 1.1721\n",
      "iteration: 18  loss: 0.139160  exp(loss): 1.1493\n",
      "iteration: 19  loss: 0.087306  exp(loss): 1.0912\n",
      "iteration: 20  loss: 0.063822  exp(loss): 1.0659\n",
      "iteration: 21  loss: 0.047577  exp(loss): 1.0487\n",
      "iteration: 22  loss: 0.037166  exp(loss): 1.0379\n",
      "iteration: 23  loss: 0.031378  exp(loss): 1.0319\n",
      "iteration: 24  loss: 0.027722  exp(loss): 1.0281\n",
      "iteration: 25  loss: 0.025068  exp(loss): 1.0254\n",
      "iteration: 26  loss: 0.023194  exp(loss): 1.0235\n",
      "iteration: 27  loss: 0.021772  exp(loss): 1.0220\n",
      "iteration: 28  loss: 0.020621  exp(loss): 1.0208\n",
      "iteration: 29  loss: 0.019633  exp(loss): 1.0198\n",
      "iteration: 30  loss: 0.018765  exp(loss): 1.0189\n",
      "iteration: 31  loss: 0.017991  exp(loss): 1.0182\n",
      "iteration: 32  loss: 0.017293  exp(loss): 1.0174\n",
      "iteration: 33  loss: 0.016661  exp(loss): 1.0168\n",
      "iteration: 34  loss: 0.016083  exp(loss): 1.0162\n",
      "iteration: 35  loss: 0.015552  exp(loss): 1.0157\n",
      "iteration: 36  loss: 0.015061  exp(loss): 1.0152\n",
      "iteration: 37  loss: 0.014607  exp(loss): 1.0147\n",
      "iteration: 38  loss: 0.014184  exp(loss): 1.0143\n",
      "iteration: 39  loss: 0.013788  exp(loss): 1.0139\n",
      "iteration: 40  loss: 0.013417  exp(loss): 1.0135\n",
      "iteration: 41  loss: 0.013069  exp(loss): 1.0132\n",
      "iteration: 42  loss: 0.012741  exp(loss): 1.0128\n",
      "iteration: 43  loss: 0.012431  exp(loss): 1.0125\n",
      "iteration: 44  loss: 0.012137  exp(loss): 1.0122\n",
      "iteration: 45  loss: 0.011859  exp(loss): 1.0119\n",
      "iteration: 46  loss: 0.011594  exp(loss): 1.0117\n",
      "iteration: 47  loss: 0.011342  exp(loss): 1.0114\n",
      "iteration: 48  loss: 0.011102  exp(loss): 1.0112\n",
      "iteration: 49  loss: 0.010873  exp(loss): 1.0109\n",
      "iteration: 50  loss: 0.010654  exp(loss): 1.0107\n",
      "iteration: 51  loss: 0.010445  exp(loss): 1.0105\n",
      "iteration: 52  loss: 0.010244  exp(loss): 1.0103\n",
      "iteration: 53  loss: 0.010051  exp(loss): 1.0101\n",
      "iteration: 54  loss: 0.009866  exp(loss): 1.0099\n",
      "iteration: 55  loss: 0.009688  exp(loss): 1.0097\n",
      "iteration: 56  loss: 0.009517  exp(loss): 1.0096\n",
      "iteration: 57  loss: 0.009352  exp(loss): 1.0094\n",
      "iteration: 58  loss: 0.009193  exp(loss): 1.0092\n",
      "iteration: 59  loss: 0.009040  exp(loss): 1.0091\n",
      "iteration: 60  loss: 0.008892  exp(loss): 1.0089\n",
      "iteration: 61  loss: 0.008749  exp(loss): 1.0088\n",
      "iteration: 62  loss: 0.008611  exp(loss): 1.0086\n",
      "iteration: 63  loss: 0.008477  exp(loss): 1.0085\n",
      "iteration: 64  loss: 0.008348  exp(loss): 1.0084\n",
      "iteration: 65  loss: 0.008223  exp(loss): 1.0083\n",
      "iteration: 66  loss: 0.008101  exp(loss): 1.0081\n",
      "iteration: 67  loss: 0.007983  exp(loss): 1.0080\n",
      "iteration: 68  loss: 0.007869  exp(loss): 1.0079\n",
      "iteration: 69  loss: 0.007758  exp(loss): 1.0078\n",
      "iteration: 70  loss: 0.007651  exp(loss): 1.0077\n",
      "iteration: 71  loss: 0.007546  exp(loss): 1.0076\n",
      "iteration: 72  loss: 0.007444  exp(loss): 1.0075\n",
      "iteration: 73  loss: 0.007345  exp(loss): 1.0074\n",
      "iteration: 74  loss: 0.007249  exp(loss): 1.0073\n",
      "iteration: 75  loss: 0.007155  exp(loss): 1.0072\n",
      "iteration: 76  loss: 0.007064  exp(loss): 1.0071\n",
      "iteration: 77  loss: 0.006975  exp(loss): 1.0070\n",
      "iteration: 78  loss: 0.006889  exp(loss): 1.0069\n",
      "iteration: 79  loss: 0.006804  exp(loss): 1.0068\n",
      "iteration: 80  loss: 0.006722  exp(loss): 1.0067\n",
      "iteration: 81  loss: 0.006642  exp(loss): 1.0067\n",
      "iteration: 82  loss: 0.006563  exp(loss): 1.0066\n",
      "iteration: 83  loss: 0.006487  exp(loss): 1.0065\n",
      "iteration: 84  loss: 0.006412  exp(loss): 1.0064\n",
      "iteration: 85  loss: 0.006339  exp(loss): 1.0064\n",
      "iteration: 86  loss: 0.006268  exp(loss): 1.0063\n",
      "iteration: 87  loss: 0.006198  exp(loss): 1.0062\n",
      "iteration: 88  loss: 0.006130  exp(loss): 1.0061\n",
      "iteration: 89  loss: 0.006064  exp(loss): 1.0061\n",
      "iteration: 90  loss: 0.005999  exp(loss): 1.0060\n",
      "iteration: 91  loss: 0.005935  exp(loss): 1.0060\n",
      "iteration: 92  loss: 0.005873  exp(loss): 1.0059\n",
      "iteration: 93  loss: 0.005812  exp(loss): 1.0058\n",
      "iteration: 94  loss: 0.005752  exp(loss): 1.0058\n",
      "iteration: 95  loss: 0.005693  exp(loss): 1.0057\n",
      "iteration: 96  loss: 0.005636  exp(loss): 1.0057\n",
      "iteration: 97  loss: 0.005580  exp(loss): 1.0056\n",
      "iteration: 98  loss: 0.005525  exp(loss): 1.0055\n",
      "iteration: 99  loss: 0.005471  exp(loss): 1.0055\n"
     ]
    }
   ]
  }
 ]
}