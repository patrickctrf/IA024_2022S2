{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "ex02_175480_Patrick_Ferreira.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/patrickctrf/IA024_2022S2/blob/main/ex02_175480/ex02_175480_Patrick_Ferreira.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od7iUgHy5SSi"
   },
   "source": [
    "# Aula 2: Análise de Sentimentos usando Bag of Words\n",
    "\n",
    "Neste notebook iremos treinar um rede de uma única camada para fazer análise de sentimento usando o dataset IMDB."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nome = \"Patrick de Carvalho Tavares Rezende Ferreira\"\n",
    "print(f'Meu nome é {nome}')"
   ],
   "metadata": {
    "id": "3vAaCL0u-zg3",
    "outputId": "f5f754e4-97f0-4693-8895-50c200eb0d19",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu nome é Patrick de Carvalho Tavares Rezende Ferreira\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importando as bibliotecas necessárias"
   ],
   "metadata": {
    "id": "Mhv5U8Muiyz_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from typing import List"
   ],
   "metadata": {
    "id": "qzTCVXoOiyIs"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXFdJz2KVeQw"
   },
   "source": [
    "# Preparando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHMi_Kq65fPM"
   },
   "source": [
    "Primeiro, fazemos download do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2wbnfzst5O3k",
    "outputId": "0d2a9649-4541-49cd-98e8-e6f6e980fc33",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "!wget -nc http: // files.fast.ai / data / examples / imdb_sample.tgz\n",
    "!tar -xzf imdb_sample.tgz"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-31 01:56:41--  ftp://http/\r\n",
      "           => ‘.listing’\r\n",
      "Resolving http (http)... failed: Temporary failure in name resolution.\r\n",
      "wget: unable to resolve host address ‘http’\r\n",
      "//: Scheme missing.\r\n",
      "File ‘index.html’ already there; not retrieving.\r\n",
      "\r\n",
      "/: Scheme missing.\r\n",
      "File ‘index.html’ already there; not retrieving.\r\n",
      "\r\n",
      "/: Scheme missing.\r\n",
      "File ‘index.html’ already there; not retrieving.\r\n",
      "\r\n",
      "/: Scheme missing.\r\n",
      "File ‘index.html’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Giyi5Rv_NIm"
   },
   "source": [
    "Carregamos o dataset .csv usando o pandas:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0HIN_xLI_TuT",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "outputId": "a46f5a7b-abfa-4aaa-ef8f-66c63ade5720"
   },
   "source": [
    "df = pd.read_csv('imdb_sample/texts.csv')\n",
    "df.shape\n",
    "df.head()"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "      label                                               text  is_valid\n0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n1  positive  This is a extremely well-made film. The acting...     False\n2  negative  Every once in a long while a movie will come a...     False\n3  positive  Name just says it all. I watched this movie wi...     False\n4  negative  This movie succeeds at being one of the most u...     False",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n      <th>is_valid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>negative</td>\n      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>positive</td>\n      <td>This is a extremely well-made film. The acting...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>Every once in a long while a movie will come a...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>positive</td>\n      <td>Name just says it all. I watched this movie wi...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>negative</td>\n      <td>This movie succeeds at being one of the most u...</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8dfjdJ-AV79"
   },
   "source": [
    "Iremos agora apenas selecionar 100 exemplos de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KCoftmPmAfXE",
    "outputId": "ff061e38-175b-4cf6-ec51-dc1ded494cd3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "treino = df[df['is_valid'] == False]  # Apenas treinamento, isto é, descartamos o dataset de validação.\n",
    "\n",
    "print('treino.shape original:', treino.shape)\n",
    "\n",
    "treino = treino[:100]  # Aqui truncamos o dataset para os 100 primeiros exemplos. \n",
    "\n",
    "print('treino.shape depois:', treino.shape)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treino.shape original: (800, 3)\n",
      "treino.shape depois: (100, 3)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHus6FH7DftH"
   },
   "source": [
    "Iremos dividir este conjunto em entrada (X) e saída desejada (Y, target) e converter as strings \"positive\" e \"negative\" do target para valores booleanos:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "46RdLFLkEW-X",
    "outputId": "68c6cd59-c51c-4a17-d67c-366e51b45a86",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "X_treino = treino['text']\n",
    "Y_treino = treino['label']\n",
    "\n",
    "print(f'Primeiras linhas de X_treino:\\n{X_treino.head()}\\n')\n",
    "print(f'Primeiras linhas de Y_treino:\\n{Y_treino.head()}\\n')\n",
    "\n",
    "mapeamento = {'positive': True, 'negative': False}\n",
    "Y_treino = Y_treino.map(mapeamento)\n",
    "Y_treino = torch.tensor(Y_treino.values, dtype=torch.long)\n",
    "print(f'Tamanho de Y_treino: {Y_treino.shape}')\n",
    "print(f'5 primeiras linhas de Y_treino: {Y_treino[:5]}')\n",
    "print(f'Número de exemplos positivos: {(Y_treino == True).sum()}')\n",
    "print(f'Número de exemplos negativos: {(Y_treino == False).sum()}')"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiras linhas de X_treino:\n",
      "0    Un-bleeping-believable! Meg Ryan doesn't even ...\n",
      "1    This is a extremely well-made film. The acting...\n",
      "2    Every once in a long while a movie will come a...\n",
      "3    Name just says it all. I watched this movie wi...\n",
      "4    This movie succeeds at being one of the most u...\n",
      "Name: text, dtype: object\n",
      "\n",
      "Primeiras linhas de Y_treino:\n",
      "0    negative\n",
      "1    positive\n",
      "2    negative\n",
      "3    positive\n",
      "4    negative\n",
      "Name: label, dtype: object\n",
      "\n",
      "Tamanho de Y_treino: torch.Size([100])\n",
      "5 primeiras linhas de Y_treino: tensor([0, 1, 0, 1, 0])\n",
      "Número de exemplos positivos: 51\n",
      "Número de exemplos negativos: 49\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLlaPgP0Z_D4"
   },
   "source": [
    "# Definindo o tokenizador\n",
    "\n",
    "Agora temos a função de tokenização, isto é, que converte strings para tokens."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YIpp1C_qZ-QX"
   },
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def tokenize(text: str):\n",
    "    \"\"\"\n",
    "    Convert string to a list of tokens (i.e., words).\n",
    "    This function lower cases everything and removes punctuation.\n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub('[' + string.punctuation + ']', '', text).lower().split()"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando a função com um exemplo simples\n"
   ],
   "metadata": {
    "id": "uE7kwbIYlkPn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "assert tokenize(\"?I? lik.e, t\\'o ea!t pizza.\") == ['i', 'like', 'to', 'eat', 'pizza'], \"Não passou no assert.\"\n",
    "print('Passou no assert!')"
   ],
   "metadata": {
    "id": "iS6QbpUwifyY",
    "outputId": "8d4dd79b-b038-4f17-c627-81d2d9a2a133",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passou no assert!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Definindo o vocabulário\n",
    "\n",
    "Selecionaremos os `max_tokens` (ex: 1000) tokens mais frequentes do dataset de treino como sendo nosso vocabulário."
   ],
   "metadata": {
    "id": "gbf_yO6XJ_hY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def create_vocab(texts: List[str], max_tokens: int):\n",
    "    \"\"\"\n",
    "    Returns a dictionary whose keys are tokens and values are token ids (from 0 to max_tokens - 1).\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for t in texts:\n",
    "        tokens.extend(tokenize(t))\n",
    "\n",
    "    return dict(Counter(tokens).most_common(max_tokens))"
   ],
   "metadata": {
    "id": "jVc8uucLK4pP"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando a função\n"
   ],
   "metadata": {
    "id": "aMYyPXpYlr64"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "L = ['f', 'a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a', 'd']\n",
    "k = 3\n",
    "resultado = create_vocab(L, k)\n",
    "\n",
    "assert f'resultado: {resultado}' == \"resultado: {'a': 4, 'd': 3, 'e': 3}\"\n",
    "print(\"Passou no assert!\")"
   ],
   "metadata": {
    "id": "fF_9u5e4FdD5"
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passou no assert!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Função para converter string para Bag-of-words"
   ],
   "metadata": {
    "id": "wki1S4LMKCrh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def convert_to_bow(text: str, vocab):\n",
    "    \"\"\"\n",
    "    Returns a bag-of-word vector of size len(vocab).\n",
    "    \"\"\"\n",
    "    bow = np.zeros(len(vocab))\n",
    "    vocab_words = list(vocab.keys())\n",
    "\n",
    "    for token in tokenize(text):\n",
    "        if token in vocab_words:\n",
    "            bow[vocab_words.index(token)] = 1\n",
    "\n",
    "    return torch.tensor(bow)"
   ],
   "metadata": {
    "id": "4TdUgA5RMl_4"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando a função"
   ],
   "metadata": {
    "id": "jLnkDcWil-Jd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X_assert = [\"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\",\n",
    "            \"Peter Piper picked a peck of pickled peppers. How many pickled peppers did Peter Piper pick?\",\n",
    "            \"She saw Sharif's shoes on the sofa. But was she so sure those were Sharif's shoes she saw?\", ]\n",
    "\n",
    "vocab = create_vocab(X_assert, max_tokens=1000)\n",
    "\n",
    "bow = convert_to_bow(X_assert[0], vocab)\n",
    "\n",
    "assert bow.tolist() == [1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]\n",
    "print(\"Passou no Assert!\")"
   ],
   "metadata": {
    "id": "kTu0EBGVGw_8"
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passou no Assert!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_6pddDHEM_r"
   },
   "source": [
    "## Definindo a Rede Neural\n",
    "\n",
    "**Entrada:**\n",
    "\n",
    "$x \\in R^{B \\times |V|}$     (bag-of-words)\n",
    "\n",
    "**Parametros:**\n",
    "\n",
    "$W \\in R^{|V| \\times K}$    (weights: matriz de pesos)\n",
    "\n",
    "$b \\in R^{K}$    (bias/viés)\n",
    "\n",
    "**Saída:**\n",
    "\n",
    "$p \\in R^{B \\times K}$  (probabilidade de cada classe)\n",
    "\n",
    "\n",
    "**Onde:**\n",
    "\n",
    "$K$ = número de classes\n",
    "\n",
    "$B$ = tamanho do batch\n",
    "\n",
    "$|V|$ = tamanho do vocabulário\n",
    "\n",
    "**Definição da rede:**\n",
    "\n",
    "$z = xW + b$   (camada linear. $z$ é chamado de logits)\n",
    "\n",
    "$p_i = \\frac{e^{z_i}}{\\sum_{j=0}^{K-1} e^{z_j}}$   (softmax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oLReRSuDEPLL"
   },
   "source": [
    "def softmax(x, dim=-1):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim, keepdim=True)\n",
    "\n",
    "class MyModel():\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        self.weights = torch.randn((dim, 2), dtype=torch.double) * 0.01\n",
    "        self.bias = torch.randn((2,), dtype=torch.double) * 0.01\n",
    "\n",
    "        self.weights.requires_grad = self.bias.requires_grad = True\n",
    "\n",
    "    def __call__(self, x):\n",
    "        network_output = torch.matmul(x.double(), self.weights) - self.bias\n",
    "        probs = softmax(network_output, -1)\n",
    "        return probs"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando modelo com uma entrada aleatória\n",
    "\n",
    "Escreva abaixo um pequeno código para testar se seu modelo processa uma matriz de entrada de tamanho `batch_size, dim`, ou seja, a matriz contém `batch_size` exemplos, cada um sendo representado por um vetor de tamanho `dim`."
   ],
   "metadata": {
    "id": "_8eWRvN79f4O"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 16\n",
    "dim = 8\n",
    "model = MyModel(dim)\n",
    "x = torch.randn(batch_size, dim)\n",
    "probs = model(x)\n",
    "\n",
    "print(probs)"
   ],
   "metadata": {
    "id": "R-lupvM_HxOJ"
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5208, 0.4792],\n",
      "        [0.5111, 0.4889],\n",
      "        [0.4854, 0.5146],\n",
      "        [0.4772, 0.5228],\n",
      "        [0.4774, 0.5226],\n",
      "        [0.5005, 0.4995],\n",
      "        [0.5057, 0.4943],\n",
      "        [0.5055, 0.4945],\n",
      "        [0.5255, 0.4745],\n",
      "        [0.5039, 0.4961],\n",
      "        [0.5048, 0.4952],\n",
      "        [0.4986, 0.5014],\n",
      "        [0.4836, 0.5164],\n",
      "        [0.4995, 0.5005],\n",
      "        [0.5235, 0.4765],\n",
      "        [0.5168, 0.4832]], dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Função de custo Entropia Cruzada\n",
    "\n",
    "$y \\in R^{K}$  (target),\n",
    "\n",
    "a equação da entropia cruzada associada a um exemplo é dada por:\n",
    "\n",
    "$L = \\sum_{i=0}^{K-1} -y_i \\log p_i$   (esta é a loss por exemplo)\n",
    "\n",
    "Se $y$ for um vetor one-hot (apenas um dos elementos é diferente de zero), podemos simplicar a equação acima para:\n",
    "\n",
    "$L = -\\log p_i$\n",
    "\n",
    "Onde $i$ é o indice da classe correta. Ou seja, $p_i$ é a probabilidade que o modelo colocou na classe correta.\n",
    "\n",
    "A função de custo é a **média** da entropia cruzada de cada exemplo no batch."
   ],
   "metadata": {
    "id": "O-EXAYdOvGcF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def cross_entropy_loss(probs, targets):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      probs: a float32 matrix of shape (batch_size, number of classes)\n",
    "      targets: a long (int64) array of shape (batch_size)\n",
    "\n",
    "    Returns:\n",
    "      Mean loss in the batch.\n",
    "    \"\"\"\n",
    "    # Rescreva o código abaixo sem usar laço.\n",
    "    # batch_size = probs.shape[0]\n",
    "    # losses = []\n",
    "    # for i in range(batch_size):\n",
    "    #     losses.append(-torch.log(probs[i, targets[i]]))\n",
    "    #\n",
    "    # losses = torch.stack(losses)\n",
    "    targets_one_hot = torch.nn.functional.one_hot(targets, num_classes=2)\n",
    "    return -torch.log((targets_one_hot * probs).sum(axis=-1)).mean()"
   ],
   "metadata": {
    "id": "YyHefTffhgmx"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando a função entropia cruzada com probabilidades de 50%\n",
    "\n",
    "Escreva abaixo um pequeno código para testar se a entropia cruzada confere com a resposta do problema 3.6 do exercício da semana passada. Crie um tensor para as probabilidades (50%) e um target também aleatório balanceado e calcule a cross entropia. Qual é o valor esperado da cross entropia nesse caso?"
   ],
   "metadata": {
    "id": "kIgECDC5uqRo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "n_classes = 2\n",
    "\n",
    "rand_probs = softmax(torch.rand(1024, n_classes), dim=-1)\n",
    "rand_targets = torch.rand(1024, 1).long()\n",
    "\n",
    "custo_medio = cross_entropy_loss(rand_probs, rand_targets)\n",
    "\n",
    "print(\"Valor medio cross entropy 2 classes: \", custo_medio.item())\n",
    "print(\"Valor predito cross entropy 2 classes: \", math.log(n_classes))\n",
    "\n",
    "assert round(math.log(n_classes), 1) == (10 * custo_medio).round() / 10\n",
    "print(\"Passou no assert!\")"
   ],
   "metadata": {
    "id": "-10K5Jecve--"
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor medio cross entropy 2 classes:  0.7174175977706909\n",
      "Valor predito cross entropy 2 classes:  0.6931471805599453\n",
      "Passou no assert!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convertendo dataset de treino para uma matriz de bag-of-words"
   ],
   "metadata": {
    "id": "jWcTeNuJF1Mp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "vocab = create_vocab(X_treino, max_tokens=1000)\n",
    "bows = []\n",
    "for text in X_treino:\n",
    "    bow = convert_to_bow(text, vocab)\n",
    "    bows.append(bow)\n",
    "\n",
    "X = torch.stack(bows)\n",
    "print(X.shape)"
   ],
   "metadata": {
    "id": "kGIFOJZcFs7b"
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1000])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Laço de Treinamento"
   ],
   "metadata": {
    "id": "OSdBQozHlT59"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "num_iterations = 100\n",
    "learning_rate = 0.9\n",
    "\n",
    "model = MyModel(dim=len(vocab))\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Zera os gradientes\n",
    "    if model.weights.grad is not None:\n",
    "        model.weights.grad.data.zero_()\n",
    "        model.bias.grad.data.zero_()\n",
    "\n",
    "    probs = model(X)\n",
    "    loss = cross_entropy_loss(probs, Y_treino)\n",
    "    print(f'iteration: {i}  loss: {loss:.6f}  exp(loss): {torch.exp(loss):.4f}')\n",
    "    loss.backward()\n",
    "\n",
    "    #Atualiza os pesos\n",
    "    model.weights.data = model.weights.data - learning_rate * model.weights.grad.data\n",
    "    model.bias.data = model.bias.data - learning_rate * model.bias.grad.data"
   ],
   "metadata": {
    "id": "7j3a3FsBlWCy"
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0  loss: 0.690541  exp(loss): 1.9948\n",
      "iteration: 1  loss: 0.470423  exp(loss): 1.6007\n",
      "iteration: 2  loss: 3.319959  exp(loss): 27.6592\n",
      "iteration: 3  loss: 7.978537  exp(loss): 2917.6582\n",
      "iteration: 4  loss: 1.699730  exp(loss): 5.4725\n",
      "iteration: 5  loss: 7.535725  exp(loss): 1873.8025\n",
      "iteration: 6  loss: 0.842951  exp(loss): 2.3232\n",
      "iteration: 7  loss: 3.706811  exp(loss): 40.7237\n",
      "iteration: 8  loss: 3.369181  exp(loss): 29.0547\n",
      "iteration: 9  loss: 3.947530  exp(loss): 51.8073\n",
      "iteration: 10  loss: 1.582555  exp(loss): 4.8674\n",
      "iteration: 11  loss: 2.525795  exp(loss): 12.5008\n",
      "iteration: 12  loss: 0.991284  exp(loss): 2.6947\n",
      "iteration: 13  loss: 0.299292  exp(loss): 1.3489\n",
      "iteration: 14  loss: 0.022101  exp(loss): 1.0223\n",
      "iteration: 15  loss: 0.017076  exp(loss): 1.0172\n",
      "iteration: 16  loss: 0.014478  exp(loss): 1.0146\n",
      "iteration: 17  loss: 0.012735  exp(loss): 1.0128\n",
      "iteration: 18  loss: 0.011439  exp(loss): 1.0115\n",
      "iteration: 19  loss: 0.010425  exp(loss): 1.0105\n",
      "iteration: 20  loss: 0.009608  exp(loss): 1.0097\n",
      "iteration: 21  loss: 0.008935  exp(loss): 1.0090\n",
      "iteration: 22  loss: 0.008371  exp(loss): 1.0084\n",
      "iteration: 23  loss: 0.007891  exp(loss): 1.0079\n",
      "iteration: 24  loss: 0.007478  exp(loss): 1.0075\n",
      "iteration: 25  loss: 0.007118  exp(loss): 1.0071\n",
      "iteration: 26  loss: 0.006802  exp(loss): 1.0068\n",
      "iteration: 27  loss: 0.006523  exp(loss): 1.0065\n",
      "iteration: 28  loss: 0.006273  exp(loss): 1.0063\n",
      "iteration: 29  loss: 0.006048  exp(loss): 1.0061\n",
      "iteration: 30  loss: 0.005845  exp(loss): 1.0059\n",
      "iteration: 31  loss: 0.005660  exp(loss): 1.0057\n",
      "iteration: 32  loss: 0.005491  exp(loss): 1.0055\n",
      "iteration: 33  loss: 0.005336  exp(loss): 1.0053\n",
      "iteration: 34  loss: 0.005193  exp(loss): 1.0052\n",
      "iteration: 35  loss: 0.005060  exp(loss): 1.0051\n",
      "iteration: 36  loss: 0.004937  exp(loss): 1.0049\n",
      "iteration: 37  loss: 0.004823  exp(loss): 1.0048\n",
      "iteration: 38  loss: 0.004716  exp(loss): 1.0047\n",
      "iteration: 39  loss: 0.004616  exp(loss): 1.0046\n",
      "iteration: 40  loss: 0.004521  exp(loss): 1.0045\n",
      "iteration: 41  loss: 0.004433  exp(loss): 1.0044\n",
      "iteration: 42  loss: 0.004349  exp(loss): 1.0044\n",
      "iteration: 43  loss: 0.004270  exp(loss): 1.0043\n",
      "iteration: 44  loss: 0.004195  exp(loss): 1.0042\n",
      "iteration: 45  loss: 0.004124  exp(loss): 1.0041\n",
      "iteration: 46  loss: 0.004056  exp(loss): 1.0041\n",
      "iteration: 47  loss: 0.003991  exp(loss): 1.0040\n",
      "iteration: 48  loss: 0.003930  exp(loss): 1.0039\n",
      "iteration: 49  loss: 0.003871  exp(loss): 1.0039\n",
      "iteration: 50  loss: 0.003815  exp(loss): 1.0038\n",
      "iteration: 51  loss: 0.003761  exp(loss): 1.0038\n",
      "iteration: 52  loss: 0.003710  exp(loss): 1.0037\n",
      "iteration: 53  loss: 0.003660  exp(loss): 1.0037\n",
      "iteration: 54  loss: 0.003612  exp(loss): 1.0036\n",
      "iteration: 55  loss: 0.003567  exp(loss): 1.0036\n",
      "iteration: 56  loss: 0.003522  exp(loss): 1.0035\n",
      "iteration: 57  loss: 0.003480  exp(loss): 1.0035\n",
      "iteration: 58  loss: 0.003439  exp(loss): 1.0034\n",
      "iteration: 59  loss: 0.003399  exp(loss): 1.0034\n",
      "iteration: 60  loss: 0.003361  exp(loss): 1.0034\n",
      "iteration: 61  loss: 0.003324  exp(loss): 1.0033\n",
      "iteration: 62  loss: 0.003288  exp(loss): 1.0033\n",
      "iteration: 63  loss: 0.003253  exp(loss): 1.0033\n",
      "iteration: 64  loss: 0.003220  exp(loss): 1.0032\n",
      "iteration: 65  loss: 0.003187  exp(loss): 1.0032\n",
      "iteration: 66  loss: 0.003155  exp(loss): 1.0032\n",
      "iteration: 67  loss: 0.003125  exp(loss): 1.0031\n",
      "iteration: 68  loss: 0.003095  exp(loss): 1.0031\n",
      "iteration: 69  loss: 0.003066  exp(loss): 1.0031\n",
      "iteration: 70  loss: 0.003037  exp(loss): 1.0030\n",
      "iteration: 71  loss: 0.003010  exp(loss): 1.0030\n",
      "iteration: 72  loss: 0.002983  exp(loss): 1.0030\n",
      "iteration: 73  loss: 0.002957  exp(loss): 1.0030\n",
      "iteration: 74  loss: 0.002931  exp(loss): 1.0029\n",
      "iteration: 75  loss: 0.002907  exp(loss): 1.0029\n",
      "iteration: 76  loss: 0.002882  exp(loss): 1.0029\n",
      "iteration: 77  loss: 0.002859  exp(loss): 1.0029\n",
      "iteration: 78  loss: 0.002836  exp(loss): 1.0028\n",
      "iteration: 79  loss: 0.002813  exp(loss): 1.0028\n",
      "iteration: 80  loss: 0.002791  exp(loss): 1.0028\n",
      "iteration: 81  loss: 0.002769  exp(loss): 1.0028\n",
      "iteration: 82  loss: 0.002748  exp(loss): 1.0028\n",
      "iteration: 83  loss: 0.002728  exp(loss): 1.0027\n",
      "iteration: 84  loss: 0.002708  exp(loss): 1.0027\n",
      "iteration: 85  loss: 0.002688  exp(loss): 1.0027\n",
      "iteration: 86  loss: 0.002669  exp(loss): 1.0027\n",
      "iteration: 87  loss: 0.002650  exp(loss): 1.0027\n",
      "iteration: 88  loss: 0.002631  exp(loss): 1.0026\n",
      "iteration: 89  loss: 0.002613  exp(loss): 1.0026\n",
      "iteration: 90  loss: 0.002595  exp(loss): 1.0026\n",
      "iteration: 91  loss: 0.002577  exp(loss): 1.0026\n",
      "iteration: 92  loss: 0.002560  exp(loss): 1.0026\n",
      "iteration: 93  loss: 0.002543  exp(loss): 1.0025\n",
      "iteration: 94  loss: 0.002527  exp(loss): 1.0025\n",
      "iteration: 95  loss: 0.002511  exp(loss): 1.0025\n",
      "iteration: 96  loss: 0.002495  exp(loss): 1.0025\n",
      "iteration: 97  loss: 0.002479  exp(loss): 1.0025\n",
      "iteration: 98  loss: 0.002464  exp(loss): 1.0025\n",
      "iteration: 99  loss: 0.002449  exp(loss): 1.0025\n"
     ]
    }
   ]
  }
 ]
}