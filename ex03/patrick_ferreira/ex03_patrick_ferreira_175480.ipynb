{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/patrickctrf/IA024_2022S2/blob/main/ex03/patrick_ferreira/ex03_patrick_ferreira_175480.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "nome = 'Patrick de Carvalho Tavares Rezende Ferreira'\n",
    "print(f'Meu nome é {nome}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxGWfhA5jxNG",
    "outputId": "b528f839-fea5-48d9-fe45-b6ad50538cec"
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu nome é Patrick de Carvalho Tavares Rezende Ferreira\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od7iUgHy5SSi"
   },
   "source": [
    "## Instruções\n",
    "\n",
    "- Treinar uma rede neural de duas camadas como classificador binário na tarefa de análise de sentimentos usando dataset IMDB usando TF-IDF como entrada.\n",
    "\n",
    "Deve-se implementar o laço de treinamento e validação da rede neural.\n",
    "\n",
    "Neste exercício usaremos o IMDB com 20k exemplos para treino, 5k para desenvolvimento e 25k para teste."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importando os pacotes necessários"
   ],
   "metadata": {
    "id": "W_dfOgTUffR2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import numpy as np"
   ],
   "metadata": {
    "id": "lb8DJ6YaTtyI"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Verificando se a GPU está disponível"
   ],
   "metadata": {
    "id": "3HA9p2iEUZj-"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HPbiUIrHZlun",
    "outputId": "eb43979a-aea8-4074-b6c6-88aafc942ecc"
   },
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "    print(torch.cuda.get_device_name(dev))\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "print(dev)\n",
    "device = torch.device(dev)"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXFdJz2KVeQw"
   },
   "source": [
    "## Preparando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHMi_Kq65fPM"
   },
   "source": [
    "Primeiro, fazemos download do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2wbnfzst5O3k",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f999a0f8-9398-4b72-b5aa-a7cb2d2afa52"
   },
   "source": [
    "!wget -nc http: // files.fast.ai / data / aclImdb.tgz\n",
    "!tar -xzf aclImdb.tgz"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-08 03:24:36--  ftp://http/\r\n",
      "           => ‘.listing’\r\n",
      "Resolving http (http)... failed: Temporary failure in name resolution.\r\n",
      "wget: unable to resolve host address ‘http’\r\n",
      "//: Scheme missing.\r\n",
      "File ‘index.html’ already there; not retrieving.\r\n",
      "\r\n",
      "/: Scheme missing.\r\n",
      "File ‘index.html’ already there; not retrieving.\r\n",
      "\r\n",
      "/: Scheme missing.\r\n",
      "File ‘index.html’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Giyi5Rv_NIm"
   },
   "source": [
    "## Carregando o dataset\n",
    "\n",
    "Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n",
    "\n",
    "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0HIN_xLI_TuT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "53038c65-ed12-442e-adfe-1cb105fe86b5"
   },
   "source": [
    "def load_texts(folder):\n",
    "    texts = []\n",
    "    for path in os.listdir(folder):\n",
    "        with open(os.path.join(folder, path)) as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "\n",
    "x_train_pos = load_texts('aclImdb/train/pos')\n",
    "x_train_neg = load_texts('aclImdb/train/neg')\n",
    "x_test_pos = load_texts('aclImdb/test/pos')\n",
    "x_test_neg = load_texts('aclImdb/test/neg')\n",
    "\n",
    "x_train = x_train_pos + x_train_neg\n",
    "x_test = x_test_pos + x_test_neg\n",
    "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
    "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
    "\n",
    "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
    "c = list(zip(x_train, y_train))\n",
    "random.shuffle(c)\n",
    "x_train, y_train = zip(*c)\n",
    "\n",
    "n_train = int(0.8 * len(x_train))\n",
    "\n",
    "x_valid = x_train[n_train:]\n",
    "y_valid = y_train[n_train:]\n",
    "x_train = x_train[:n_train]\n",
    "y_train = y_train[:n_train]\n",
    "\n",
    "print(len(x_train), 'amostras de treino.')\n",
    "print(len(x_valid), 'amostras de desenvolvimento.')\n",
    "print(len(x_test), 'amostras de teste.')\n",
    "\n",
    "print('3 primeiras amostras treino:')\n",
    "for x, y in zip(x_train[:3], y_train[:3]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 últimas amostras treino:')\n",
    "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 primeiras amostras validação:')\n",
    "for x, y in zip(x_valid[:3], y_test[:3]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 últimas amostras validação:')\n",
    "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
    "    print(y, x[:100])"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 amostras de treino.\n",
      "5000 amostras de desenvolvimento.\n",
      "25000 amostras de teste.\n",
      "3 primeiras amostras treino:\n",
      "False I watched the Malayalam movie \"Boeing Boeing\" made in 1985 (which in turn is probably inspired by an\n",
      "False As I am from Hungary I have heard many people saying better and better things about Üvegtigris so fa\n",
      "False I saw this film yesterday.. I rented the DVD from Blockbuster.. In fact, I know one of the actresses\n",
      "3 últimas amostras treino:\n",
      "False What the heck was this. Somebody obviously read Stephen King and Sartre in the same semester. We get\n",
      "False Universal Soldier: The Return is not the worst movie ever made. No, that honor would have to go to a\n",
      "True NOTHING (3+ outta 5 stars) Another weird premise from the director of the movie \"Cube\". This time ar\n",
      "3 primeiras amostras validação:\n",
      "True I think you would have to be from the USA to get a lot of the jokes. But if you liked Princess Bride\n",
      "True I'm so confused. I've been a huge Seagal fan for 25 years. I've seen all of his films, and many of t\n",
      "True Oh dear. Some of the best talent in British TV made this serial, and so I can only assume that they \n",
      "3 últimas amostras validação:\n",
      "False Definitely an odd debut for Michael Madsen. Madsen plays Cecil Moe, an alcoholic family man whose li\n",
      "True Strangely enough this movie never made it to the big screen in Denmark, so I had to wait for the vid\n",
      "True It's a little disconcerting to have a character named Gig Young in a movie...played by Gig Young. Bu\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definindo funções de manipulação de texto."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def tokenize(text: str):\n",
    "    \"\"\"\n",
    "    Convert string to a list of tokens (i.e., words).\n",
    "    This function lower cases everything and removes punctuation.\n",
    "    \"\"\"\n",
    "\n",
    "    # return re.sub('[' + string.punctuation + ']', '', text.lower()).split()\n",
    "\n",
    "    pattern = r'\\W+'\n",
    "\n",
    "    return re.split(pattern, text.lower())\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def create_vocab(texts: List[str], max_tokens: int):\n",
    "    \"\"\"\n",
    "    Returns a dictionary whose keys are tokens and values are token ids (from 0 to max_tokens - 1).\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for t in texts:\n",
    "        tokens.extend(tokenize(t))\n",
    "\n",
    "    return dict(Counter(tokens).most_common(max_tokens))\n",
    "\n",
    "\n",
    "def concatenate_list_of_str(texts: List[str]):\n",
    "    return \"\".join(texts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Criando classe do dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class TfIdfDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, documents, targets, max_tokens=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        print(\"Iniciando montagem dataset\")\n",
    "\n",
    "        self.targets = targets\n",
    "\n",
    "        every_text = concatenate_list_of_str(documents)\n",
    "        tokens_ocurrences = create_vocab(tokenize(every_text), max_tokens=max_tokens)\n",
    "\n",
    "        lista_do_vocabulario = list(tokens_ocurrences.keys())\n",
    "\n",
    "        tf = np.zeros((len(documents), len(lista_do_vocabulario)))\n",
    "        for i, doc in tqdm(enumerate(documents), total=len(documents)):\n",
    "            tokenized_doc = tokenize(doc)[:-1]\n",
    "            array_contagem_ocorrencias = np.array([0] * len(lista_do_vocabulario))\n",
    "            for j, token in enumerate(lista_do_vocabulario):\n",
    "                array_contagem_ocorrencias[j] += tokenized_doc.count(token)\n",
    "\n",
    "            tf[i] = array_contagem_ocorrencias / len(tokenized_doc)\n",
    "\n",
    "        idf_denominator = np.zeros((len(lista_do_vocabulario),))\n",
    "        for i, doc in tqdm(enumerate(documents), total=len(documents)):\n",
    "            tokenized_doc = tokenize(doc)[:-1]\n",
    "            for j, token in enumerate(lista_do_vocabulario):\n",
    "                if token in tokenized_doc:\n",
    "                    idf_denominator[j] += 1\n",
    "\n",
    "        idf = np.log(len(documents) / idf_denominator)\n",
    "\n",
    "        self.tfidf = tf * idf\n",
    "\n",
    "        print(\"Finalizada montagem dataset\")\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.tfidf[i]), torch.tensor([1]) if self.targets[i] == 1 else torch.tensor([0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tfidf.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testando dataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 3749.38it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 4638.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando montagem dataset\n",
      "Finalizada montagem dataset\n",
      "Passou no assert!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Teste retirado do exemplo do site:\n",
    "# https://medium.com/analytics-vidhya/tf-idf-term-frequency-technique-easiest-explanation-for-text-classification-in-nlp-with-code-8ca3912e58c3\n",
    "\n",
    "x_assert = [\"It is going to rain today.\",\n",
    "            \"Today I am not going outside.\",\n",
    "            \"I am going to watch the season premiere.\"]\n",
    "\n",
    "y_assert = [False, True, True]\n",
    "\n",
    "dataset = TfIdfDataset(x_assert, y_assert, 8)\n",
    "\n",
    "tfidf_target =  [[0., 0.06757752, 0.06757752, 0., 0., 0.18310205, 0.18310205, 0.18310205],\n",
    "                [0., 0., 0.06757752, 0.06757752, 0.06757752, 0., 0., 0., ],\n",
    "                [0., 0.05068314, 0., 0.05068314, 0.05068314, 0., 0., 0., ]]\n",
    "\n",
    "tfidf_target = np.array(tfidf_target)\n",
    "\n",
    "assert np.isclose(dataset.tfidf, tfidf_target).all()\n",
    "print(\"Passou no assert!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define o modelo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size=1000, hidden_size=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden = nn.Linear(vocab_size, hidden_size)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.output_layer(self.activation(self.hidden(x))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Treina a rede"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_dataset = TfIdfDataset(x_train, y_train)\n",
    "valid_dataset = TfIdfDataset(x_valid, y_valid)\n",
    "test_dataset = TfIdfDataset(x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 13.82 . perplexity: 1.99 . accuracy: 507.48\n",
      "train_loss: 13.63 . perplexity: 1.96 . accuracy: 507.48\n",
      "train_loss: 13.27 . perplexity: 1.92 . accuracy: 507.48\n",
      "train_loss: 12.66 . perplexity: 1.86 . accuracy: 507.48\n",
      "train_loss: 11.93 . perplexity: 1.76 . accuracy: 507.48\n",
      "train_loss: 11.17 . perplexity: 1.72 . accuracy: 507.48\n",
      "train_loss: 10.44 . perplexity: 1.66 . accuracy: 507.48\n",
      "train_loss: 9.80 . perplexity: 1.61 . accuracy: 507.48\n",
      "train_loss: 9.23 . perplexity: 1.55 . accuracy: 507.48\n",
      "train_loss: 8.78 . perplexity: 1.54 . accuracy: 507.48\n",
      "train_loss: 8.39 . perplexity: 1.52 . accuracy: 507.48\n",
      "train_loss: 8.07 . perplexity: 1.52 . accuracy: 507.48\n",
      "train_loss: 7.79 . perplexity: 1.45 . accuracy: 507.48\n",
      "train_loss: 7.56 . perplexity: 1.45 . accuracy: 507.48\n",
      "train_loss: 7.38 . perplexity: 1.46 . accuracy: 507.48\n",
      "train_loss: 7.19 . perplexity: 1.41 . accuracy: 507.48\n",
      "train_loss: 7.06 . perplexity: 1.42 . accuracy: 507.48\n",
      "train_loss: 6.93 . perplexity: 1.41 . accuracy: 507.48\n",
      "train_loss: 6.81 . perplexity: 1.37 . accuracy: 507.48\n",
      "train_loss: 6.72 . perplexity: 1.37 . accuracy: 507.48\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, valid_dataloader, criterion):\n",
    "    accuracy = 0\n",
    "    model.eval()\n",
    "    num_examples = 0\n",
    "    loss = 0\n",
    "    for x, y in valid_dataloader:\n",
    "        num_examples += x.shape[0]\n",
    "        with torch.no_grad():\n",
    "            logits = model(x.to(device, torch.float))\n",
    "            loss += criterion(logits, y.to(device, torch.float)).detach() * x.shape[0]\n",
    "        preds = logits.argmax(dim=1)\n",
    "        accuracy += (preds == y).sum()\n",
    "\n",
    "    return accuracy / num_examples, loss / num_examples / 10\n",
    "\n",
    "def train(model, train_dataloader, valid_dataloader, optimizer, criterion, num_epochs: int = 10):\n",
    "    for _ in range(num_epochs):\n",
    "        model.train()\n",
    "        acumular_loss = 0\n",
    "        for x, y in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x.to(device, torch.float))\n",
    "            loss = criterion(logits, y.to(device, torch.float))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acumular_loss += loss.detach()\n",
    "\n",
    "        accuracy, valid_loss= evaluate(model, valid_dataloader, criterion)\n",
    "        print(\"train_loss: {:.2f}\".format(acumular_loss.item()), \". perplexity: {:.2f}\".format(torch.exp(loss).item()), \". accuracy: {:.2f}\".format(accuracy.item()))\n",
    "\n",
    "lr = 1e-3\n",
    "epochs = 20\n",
    "\n",
    "model = Classifier().to(device)\n",
    "\n",
    "train_loader = DataLoader(training_dataset, batch_size=1024, shuffle=True, num_workers=4)\n",
    "validation_loader = DataLoader(valid_dataset, batch_size=1024, num_workers=4, )\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "train(model, train_loader, validation_loader, optimizer, criterion, epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Métricas de teste"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 2.76 . perplexity: 1.76 . accuracy: 50.75\n"
     ]
    }
   ],
   "source": [
    "acumular_loss = 0\n",
    "test_loader = DataLoader(valid_dataset, batch_size=1024, num_workers=4, )\n",
    "for x, y in test_loader:\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(x.to(device, torch.float))\n",
    "    loss = criterion(logits, y.to(device, torch.float))\n",
    "    acumular_loss += loss.detach()\n",
    "\n",
    "accuracy, valid_loss= evaluate(model, test_loader, criterion)\n",
    "print(\"test_loss: {:.2f}\".format(acumular_loss.item()), \". perplexity: {:.2f}\".format(torch.exp(loss).item()), \". accuracy: {:.2f}\".format(accuracy.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}