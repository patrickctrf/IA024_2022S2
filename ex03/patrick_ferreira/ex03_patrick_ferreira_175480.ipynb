{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/patrickctrf/IA024_2022S2/blob/main/ex03/patrick_ferreira/ex03_patrick_ferreira_175480.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "nome = 'Patrick de Carvalho Tavares Rezende Ferreira'\n",
    "print(f'Meu nome é {nome}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxGWfhA5jxNG",
    "outputId": "b528f839-fea5-48d9-fe45-b6ad50538cec"
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu nome é Patrick de Carvalho Tavares Rezende Ferreira\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od7iUgHy5SSi"
   },
   "source": [
    "## Instruções\n",
    "\n",
    "- Treinar uma rede neural de duas camadas como classificador binário na tarefa de análise de sentimentos usando dataset IMDB usando TF-IDF como entrada.\n",
    "\n",
    "Deve-se implementar o laço de treinamento e validação da rede neural.\n",
    "\n",
    "Neste exercício usaremos o IMDB com 20k exemplos para treino, 5k para desenvolvimento e 25k para teste."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importando os pacotes necessários"
   ],
   "metadata": {
    "id": "W_dfOgTUffR2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import numpy as np"
   ],
   "metadata": {
    "id": "lb8DJ6YaTtyI"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Verificando se a GPU está disponível"
   ],
   "metadata": {
    "id": "3HA9p2iEUZj-"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HPbiUIrHZlun",
    "outputId": "eb43979a-aea8-4074-b6c6-88aafc942ecc"
   },
   "source": [
    "if torch.cuda.is_available(): \n",
    "   dev = \"cuda:0\"\n",
    "   print(torch. cuda. get_device_name(dev))\n",
    "else:\n",
    "   dev = \"cpu\" \n",
    "print(dev)\n",
    "device = torch.device(dev)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce 940MX\n",
      "cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXFdJz2KVeQw"
   },
   "source": [
    "## Preparando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHMi_Kq65fPM"
   },
   "source": [
    "Primeiro, fazemos download do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2wbnfzst5O3k",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f999a0f8-9398-4b72-b5aa-a7cb2d2afa52"
   },
   "source": [
    "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
    "!tar -xzf aclImdb.tgz"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘aclImdb.tgz’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Giyi5Rv_NIm"
   },
   "source": [
    "## Carregando o dataset\n",
    "\n",
    "Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n",
    "\n",
    "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0HIN_xLI_TuT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "53038c65-ed12-442e-adfe-1cb105fe86b5"
   },
   "source": [
    "def load_texts(folder):\n",
    "    texts = []\n",
    "    for path in os.listdir(folder):\n",
    "        with open(os.path.join(folder, path)) as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "x_train_pos = load_texts('aclImdb/train/pos')\n",
    "x_train_neg = load_texts('aclImdb/train/neg')\n",
    "x_test_pos = load_texts('aclImdb/test/pos')\n",
    "x_test_neg = load_texts('aclImdb/test/neg')\n",
    "\n",
    "x_train = x_train_pos + x_train_neg\n",
    "x_test = x_test_pos + x_test_neg\n",
    "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
    "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
    "\n",
    "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
    "c = list(zip(x_train, y_train))\n",
    "random.shuffle(c)\n",
    "x_train, y_train = zip(*c)\n",
    "\n",
    "n_train = int(0.8 * len(x_train))\n",
    "\n",
    "x_valid = x_train[n_train:]\n",
    "y_valid = y_train[n_train:]\n",
    "x_train = x_train[:n_train]\n",
    "y_train = y_train[:n_train]\n",
    "\n",
    "print(len(x_train), 'amostras de treino.')\n",
    "print(len(x_valid), 'amostras de desenvolvimento.')\n",
    "print(len(x_test), 'amostras de teste.')\n",
    "\n",
    "print('3 primeiras amostras treino:')\n",
    "for x, y in zip(x_train[:3], y_train[:3]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 últimas amostras treino:')\n",
    "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 primeiras amostras validação:')\n",
    "for x, y in zip(x_valid[:3], y_test[:3]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 últimas amostras validação:')\n",
    "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
    "    print(y, x[:100])"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 amostras de treino.\n",
      "5000 amostras de desenvolvimento.\n",
      "25000 amostras de teste.\n",
      "3 primeiras amostras treino:\n",
      "True This great film is composed mostly of documentary footage is currently contained on a DVD along with\n",
      "False For all its visual delights, how much better Renaissance would have been in live action. The animati\n",
      "False This movie in away was super-clever. It's theme rhymes with every single horror movie ever made. Val\n",
      "3 últimas amostras treino:\n",
      "False Michael Keaton has really never been a good actor; in the Tim Burton Batman movies he always falls i\n",
      "False I have seen a lot of movies...this is the first one I ever walked out of the theater on. Don't even \n",
      "True I saw this film when I was a young child on television (thank-you Canadian Broadcasting Corporation)\n",
      "3 primeiras amostras validação:\n",
      "True Having read many of the other reviews for this film on the IMDb there is ostensibly a consensus amon\n",
      "True It would help to know why it took so long for a book as movie-ready' as \"The House Next Door\" to be \n",
      "True I have read over 100 of the Nancy Drew books, and if you are not bright enough to catch on yet, Nanc\n",
      "3 últimas amostras validação:\n",
      "False Student Seduction finds Saved By The Bell Alumni Elizabeth Berkley on the other side of the desk and\n",
      "True Just after I saw the movie, the true magic feeling of the Walt Disney movies came up in me and I rea\n",
      "False Wow, i'm a huge Henry VIII/Tudor era fan and, well, this was .... interesting. The only one I watche\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definindo funções de manipulação de texto."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def tokenize(text: str):\n",
    "    \"\"\"\n",
    "    Convert string to a list of tokens (i.e., words).\n",
    "    This function lower cases everything and removes punctuation.\n",
    "    \"\"\"\n",
    "\n",
    "    # return re.sub('[' + string.punctuation + ']', '', text.lower()).split()\n",
    "\n",
    "    pattern = r'\\W+'\n",
    "\n",
    "    return re.split(pattern, text.lower())\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def create_vocab(texts: List[str], max_tokens: int):\n",
    "    \"\"\"\n",
    "    Returns a dictionary whose keys are tokens and values are token ids (from 0 to max_tokens - 1).\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for t in texts:\n",
    "        tokens.extend(tokenize(t))\n",
    "\n",
    "    return dict(Counter(tokens).most_common(max_tokens))\n",
    "\n",
    "def concatenate_list_of_str(texts: List[str]):\n",
    "    return \"\".join(texts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Criando classe do dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class TfIdfDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, documents, max_tokens=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.every_text = concatenate_list_of_str(documents)\n",
    "        self.tokens_ocurrences = create_vocab(tokenize(self.every_text), max_tokens=max_tokens)\n",
    "\n",
    "        self.lista_do_vocabulario = list(self.tokens_ocurrences.keys())\n",
    "\n",
    "        self.tf = np.zeros((len(documents), len(self.lista_do_vocabulario)))\n",
    "        for i, doc in enumerate(documents):\n",
    "            tokenized_doc = tokenize(doc)[:-1]\n",
    "            array_contagem_ocorrencias = np.array([0] * len(self.lista_do_vocabulario))\n",
    "            for j, token in enumerate(self.lista_do_vocabulario):\n",
    "                array_contagem_ocorrencias[j] += tokenized_doc.count(token)\n",
    "\n",
    "            self.tf[i] = array_contagem_ocorrencias / len(tokenized_doc)\n",
    "\n",
    "        self.idf_denominator = np.zeros((len(self.lista_do_vocabulario),))\n",
    "        for i, doc in enumerate(documents):\n",
    "            tokenized_doc = tokenize(doc)[:-1]\n",
    "            for j, token in enumerate(self.lista_do_vocabulario):\n",
    "                if token in tokenized_doc:\n",
    "                    self.idf_denominator[j] += 1\n",
    "\n",
    "        self.idf = np.log(len(documents) / self.idf_denominator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testando dataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "x_assert = [\"It is going to rain today.\",\n",
    "            \"Today I am not going outside.\",\n",
    "            \"I am going to watch the season premiere.\"]\n",
    "\n",
    "y_assert = [False, True, True]\n",
    "\n",
    "dataset = TfIdfDataset(x_assert, 8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}