{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "pycharm-362cd54d",
   "language": "python",
   "display_name": "PyCharm (IA025 - Lotufo)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/patrickctrf/IA024_2022S2/blob/main/ex05/patrick_ferreira/ex05_175480_patrick_ferreira.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "nome = \"Patrick de Carvalho Tavares Rezende Ferreira\"\n",
    "print(f'Meu nome é {nome}')"
   ],
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu nome é Patrick de Carvalho Tavares Rezende Ferreira\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Exercício: Modelo de Linguagem com auto-atenção"
   ],
   "metadata": {
    "id": "jOdQB41_4ZxG",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IbuChoAPMEn"
   },
   "source": [
    "Este exercício é similar ao da aula 4, mas iremos agora treinar uma rede neural *com auto-atenção* para prever a próxima palavra de um texto, data as palavras anteriores como entrada.\n",
    "\n",
    "Na camada de auto-atenção, deve-se implementar (vide slide 80):\n",
    "- Embeddings de posição\n",
    "- Projeções lineares (WQ, WK, WV, WO)\n",
    "- Camada de feed forward (2-layer MLP)\n",
    "\n",
    "Instrucões:\n",
    "- É necessário fazer duas implementações da camada de auto-atenção: uma usando laços (ineficiente mas fácil de entender) e outra matricial (eficiente mas difícil de entender).\n",
    "\n",
    "- Fazer um assert para garantir que o resultado das duas implementações é exatamente igual.\n",
    "\n",
    "- No treinamento, usar apenas a implementação matricial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_DBb0-Klwf2"
   },
   "source": [
    "## Importação dos pacotes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YnyhJZtTRNMx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from operator import itemgetter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from typing import List"
   ],
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qlIOVCajPWcU"
   },
   "source": [
    "# Check which GPU we are using\n",
    "!nvidia-smi"
   ],
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep 28 22:46:42 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.129.06   Driver Version: 470.129.06   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   54C    P8    N/A /  N/A |   1334MiB /  2004MiB |     36%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1053      G   /usr/lib/xorg/Xorg                 20MiB |\r\n",
      "|    0   N/A  N/A      1684      G   /usr/lib/xorg/Xorg                 76MiB |\r\n",
      "|    0   N/A  N/A      1893      G   /usr/bin/gnome-shell               21MiB |\r\n",
      "|    0   N/A  N/A      3221      G   ...950217607726532263,131072       17MiB |\r\n",
      "|    0   N/A  N/A      5507      G   ...f_3784.log --shared-files       22MiB |\r\n",
      "|    0   N/A  N/A     12195      C   ...trf/anaconda3/bin/python3      593MiB |\r\n",
      "|    0   N/A  N/A     12408      C   ...trf/anaconda3/bin/python3      569MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print('Using {}'.format(device))"
   ],
   "metadata": {
    "id": "w9f3PfifAwpU"
   },
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Carregamento do dataset\n",
    "\n",
    "Primeiro, fazemos download do dataset:"
   ],
   "metadata": {
    "id": "whTCe2i7AtoV",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4LfrHHouleJ0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!wget -nc http://files.fast.ai/data/aclImdb.tgz\n",
    "!tar -xzf aclImdb.tgz"
   ],
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wbnfzst5O3k",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Carregando o dataset\n",
    "\n",
    "Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n",
    "\n",
    "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0Giyi5Rv_NIm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def load_texts(folder):\n",
    "    texts = []\n",
    "    for path in os.listdir(folder):\n",
    "        with open(os.path.join(folder, path)) as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "x_train_pos = load_texts('aclImdb/train/pos')\n",
    "x_train_neg = load_texts('aclImdb/train/neg')\n",
    "x_test_pos = load_texts('aclImdb/test/pos')\n",
    "x_test_neg = load_texts('aclImdb/test/neg')\n",
    "\n",
    "x_train = x_train_pos + x_train_neg\n",
    "x_test = x_test_pos + x_test_neg\n",
    "\n",
    "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
    "random.shuffle(x_train)\n",
    "\n",
    "n_train = int(0.8 * len(x_train))\n",
    "\n",
    "x_valid = x_train[n_train:]\n",
    "x_train = x_train[:n_train]\n",
    "\n",
    "print(len(x_train), 'amostras de treino.')\n",
    "print(len(x_valid), 'amostras de desenvolvimento.')\n",
    "print(len(x_test), 'amostras de teste.')\n",
    "\n",
    "print('3 primeiras amostras treino:')\n",
    "for x in x_train[:3]:\n",
    "    print(x[:100])\n",
    "\n",
    "print('3 últimas amostras treino:')\n",
    "for x in x_train[-3:]:\n",
    "    print(x[:100])\n",
    "\n",
    "print('3 primeiras amostras validação:')\n",
    "for x in x_valid[:3]:\n",
    "    print(x[:100])\n",
    "\n",
    "print('3 últimas amostras validação:')\n",
    "for x in x_valid[-3:]:\n",
    "    print(x[:100])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ],
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 amostras de treino.\n",
      "5000 amostras de desenvolvimento.\n",
      "25000 amostras de teste.\n",
      "3 primeiras amostras treino:\n",
      "This is my second time through for A Perfect Spy. I watched it 2 or 3 years ago and liked it. I like\n",
      "This was a great romantic comedy! Historically inaccurate (Einstein had no nieces or nephews as note\n",
      "Siskel & Ebert were terrific on this show whether you agreed with them or not because of the genuine\n",
      "3 últimas amostras treino:\n",
      "This obscure de Sica delivers the goods. And it is said \"the meek shall inherit the earth.\" This tal\n",
      "Trey Parker and Matt Stone, the creators of the show 'South Park' , return with something entirely d\n",
      "You might be tempted to rent this film because Peter Sellers appears in it. That would be a mistake.\n",
      "3 primeiras amostras validação:\n",
      "This is an \"odysessy through time\" via computer animation, supposedly th work of over 300 artists. M\n",
      "The Caprica episode (S01E01) is well done as a pilot. Really, this episode is the exact same content\n",
      "This is one of the best Bollywood movies i have seen up until now. Family and friends feel the same \n",
      "3 últimas amostras validação:\n",
      "This 1947 film stars and was directed and written by Orson Welles (with a funky Irish accent) and al\n",
      "The first 2 parts seek to reduce to absurdity the rise of wasteful wars and rule by nationalist barb\n",
      "I notice from the comments that most of the people discussing this movie are basing their remarks on\n"
     ]
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f0f74103e70>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definindo funções de manipulação de texto."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def tokenize(text: str):\n",
    "    \"\"\"\n",
    "    Convert string to a list of tokens (i.e., words).\n",
    "    This function lower cases everything and removes punctuation.\n",
    "    \"\"\"\n",
    "\n",
    "    return re.findall(r\"[\\w']+|[.,!?;]\", text.lower())\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def create_vocab(texts: List[str], max_tokens: int):\n",
    "    \"\"\"\n",
    "    Returns a dictionary whose keys are tokens and values are token ids (from 0 to max_tokens - 1).\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for t in texts:\n",
    "        tokens.extend(tokenize(t))\n",
    "\n",
    "    return dict(Counter(tokens).most_common(max_tokens))\n",
    "\n",
    "def items_em_comum(amostra, corpus):\n",
    "    c = [value for value in amostra if value in corpus]\n",
    "    return c\n",
    "\n",
    "\n",
    "def concatenate_list_of_str(texts: List[str]):\n",
    "    return \".\".join(texts)\n",
    "\n",
    "class Tokenizador():\n",
    "    def __init__(self, texts, tokenizer=tokenize, vocab_size=3000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.every_text = concatenate_list_of_str(texts)\n",
    "        self.tokens_ocurrences = create_vocab(tokenize(self.every_text), max_tokens=vocab_size)\n",
    "\n",
    "        self.lista_do_vocabulario = list(self.tokens_ocurrences.keys())\n",
    "\n",
    "        self.dicionario_tokens = dict(zip(self.lista_do_vocabulario, range(len(self.lista_do_vocabulario))))\n",
    "\n",
    "    def __call__(self, text):\n",
    "        tokens = self.tokenizer(text)\n",
    "\n",
    "        # Pegamos somente os tokens que existem no vocabulario\n",
    "        tokens_keys = list(items_em_comum(tokens, self.lista_do_vocabulario))\n",
    "\n",
    "        return list(itemgetter(*tokens_keys)(self.dicionario_tokens))\n",
    "\n",
    "    def decode(self, ids):\n",
    "        reversed_dict = dict(zip(self.dicionario_tokens.values(), self.dicionario_tokens.keys()))\n",
    "        return list(itemgetter(*ids)(reversed_dict))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### Criando classe do dataset\n",
    "\n",
    "class MyDataset():\n",
    "    def __init__(self, texts: List[str], tokenizer, context_size: int, vocab_size=1000):\n",
    "        try:\n",
    "            self.x = np.load(\"x_\" + str(len(texts)) + \".npy\", mmap_mode=\"r\", allow_pickle=True)\n",
    "            self.y = np.load(\"y_\" + str(len(texts)) + \".npy\", mmap_mode=\"r\", allow_pickle=True)\n",
    "            print(\"Carregando dataset preprocessado\")\n",
    "        except Exception as e:\n",
    "            # print(\"Excecao: \", e)\n",
    "            print(\"Montando dataset\")\n",
    "\n",
    "            self.x = list()\n",
    "            self.y = list()\n",
    "\n",
    "            for text in tqdm(texts):\n",
    "                tokens_key = tokenizer(text)\n",
    "                for i in range(len(tokens_key)-context_size):\n",
    "                    self.x.append(tokens_key[i:i+context_size])\n",
    "                    self.y.append(tokens_key[i+context_size])\n",
    "\n",
    "            self.x = np.array(self.x)\n",
    "            self.y = np.array(self.y)\n",
    "\n",
    "            np.save(\"x_\" + str(len(texts)) + \".npy\", self.x)\n",
    "            np.save(\"y_\" + str(len(texts)) + \".npy\", self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.x[idx]).long(), torch.tensor(self.y[idx]).long()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testando Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-d56443720919>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"<ipython-input-14-d56443720919>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    Testando Dataset\u001B[0m\n\u001B[0m                   ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dataset preprocessado\n",
      "Passou no assert de tamanho do dataset\n",
      "Passou no assert de input\n",
      "Passou no assert de target\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 3000\n",
    "\n",
    "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
    "\n",
    "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=Tokenizador(dummy_texts, tokenize, vocab_size), context_size=3, vocab_size=vocab_size)\n",
    "dummy_loader = DataLoader(dummy_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "assert len(dummy_dataset) == 4\n",
    "print('Passou no assert de tamanho do dataset')\n",
    "\n",
    "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
    "\n",
    "correct_first_batch_input = torch.LongTensor(\n",
    "    [[1, 2, 0],\n",
    "     [5, 6, 7]])\n",
    "\n",
    "correct_first_batch_target = torch.LongTensor([3,   0, ])\n",
    "\n",
    "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
    "print('Passou no assert de input')\n",
    "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
    "print('Passou no assert de target')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dados de treino, validação e teste"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dataset preprocessado\n",
      "Carregando dataset preprocessado\n",
      "Carregando dataset preprocessado\n",
      "training examples: 4429162\n",
      "valid examples: 1121917\n",
      "test examples: 5420591\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "context_size = 9\n",
    "\n",
    "tokenizer = Tokenizador(x_train, tokenize, vocab_size=3000)\n",
    "\n",
    "training_dataset = MyDataset(texts=x_train, tokenizer=tokenizer, context_size=context_size)\n",
    "valid_dataset = MyDataset(texts=x_valid, tokenizer=tokenizer, context_size=context_size)\n",
    "test_dataset = MyDataset(texts=x_test, tokenizer=tokenizer, context_size=context_size)\n",
    "\n",
    "print(f'training examples: {len(training_dataset)}')\n",
    "print(f'valid examples: {len(valid_dataset)}')\n",
    "print(f'test examples: {len(test_dataset)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "class AttentionLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim: int, max_seq_length: int):\n",
    "        \"\"\"\n",
    "        Implements the Self-attention, decoder-only.\"\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the input vocabulary.\n",
    "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
    "            dim (int): Dimension of the embedding layer for each word in the context.\n",
    "            n_layers (int): number of self-attention layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.context_size = max_seq_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # hidden_size for MLP\n",
    "        hidden_size = 512\n",
    "\n",
    "        # Linear projections\n",
    "        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_0 = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        # output MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "        # cast to probabilities\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Matriz triangular de mascara, convertida para Booleano\n",
    "        # Onde vale 1, o valor deve ser substituida por um valor negativo alto no tensor de scores.\n",
    "        self.casual_mask = torch.ones((max_seq_length, max_seq_length), device=device).triu(diagonal=1) == 1.0\n",
    "\n",
    "    def forward(self, x_embeddings):\n",
    "\n",
    "        k = self.w_k(x_embeddings)\n",
    "        v = self.w_v(x_embeddings)\n",
    "        q = self.w_q(x_embeddings)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(1, 2))\n",
    "\n",
    "        # Onde a mascara vale 1, retornamos um valor negativo grande.\n",
    "        # Onde a mascara vale zero, matemos intacto.\n",
    "        probabilities = self.softmax(scores.masked_fill(self.casual_mask, -1e4))\n",
    "\n",
    "        e = self.w_0(self.norm1(x_embeddings + torch.matmul(probabilities, v)))\n",
    "\n",
    "        logits = self.mlp(e)\n",
    "\n",
    "        return self.norm2(self.activation(logits + e))\n",
    "\n",
    "class LanguageModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, max_seq_length: int, dim: int, n_layers: int,):\n",
    "        \"\"\"\n",
    "        Implements the Self-attention, decoder-only.\"\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the input vocabulary.\n",
    "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
    "            dim (int): Dimension of the embedding layer for each word in the context.\n",
    "            n_layers (int): number of self-attention layers.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        embedding_dim = dim\n",
    "        context_size = max_seq_length\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # hidden_size for MLP\n",
    "        hidden_size = 512\n",
    "\n",
    "        # tokens (words indexes) embedding and positional embedding\n",
    "        self.c_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.p_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.attention = nn.Sequential(*[AttentionLayer(embedding_dim=embedding_dim, max_seq_length=max_seq_length) for i in range(n_layers)])\n",
    "\n",
    "        self.linear_output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "        # cast to probabilities\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.positional_indexes = torch.arange(self.context_size, device=device).view(1, -1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs is a LongTensor of shape (batch_size, max_seq_length)\n",
    "\n",
    "        Returns:\n",
    "            logits of shape (batch_size, max_seq_length, vocab_size)\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        input_embeddings = self.c_embedding(inputs)\n",
    "\n",
    "        positional_embeddings = self.p_embedding(self.positional_indexes.repeat(inputs.shape[0], 1))\n",
    "\n",
    "        x_embeddings = positional_embeddings + input_embeddings\n",
    "\n",
    "        logits = self.attention(x_embeddings)\n",
    "\n",
    "        return self.linear_output(logits)[:, -1, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testando se o modelo processa os dados corretamente"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 3000])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_length=context_size,\n",
    "    dim=32,\n",
    "    n_layers=1,\n",
    ").to(device)\n",
    "\n",
    "sample_train, _ = next(iter(DataLoader(training_dataset)))\n",
    "sample_train_gpu = sample_train.to(device)\n",
    "model(sample_train_gpu).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Verificando a perplexidade"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape:  torch.Size([10, 3000])\n",
      "my perplexity:              3684\n",
      "correct initial perplexity: 3000\n",
      "Passou o no assert da perplexidade\n"
     ]
    }
   ],
   "source": [
    "def perplexity(logits, target):\n",
    "    \"\"\"\n",
    "    Computes the perplexity.\n",
    "\n",
    "    Args:\n",
    "        logits: a FloatTensor of shape (batch_size, vocab_size)\n",
    "        target: a LongTensor of shape (batch_size,)\n",
    "\n",
    "    Returns:\n",
    "        A float corresponding to the perplexity.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    crossentropy =  nn.functional.cross_entropy(logits,target)\n",
    "\n",
    "    return crossentropy.exp()\n",
    "\n",
    "\n",
    "n_examples = 100\n",
    "\n",
    "sample_train, target_token_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
    "sample_train_gpu = sample_train.to(device)\n",
    "target_token_ids = target_token_ids.to(device)\n",
    "logits = model(sample_train_gpu)\n",
    "\n",
    "print(\"logits shape: \", logits.shape)\n",
    "\n",
    "my_perplexity = perplexity(logits=logits, target=target_token_ids)\n",
    "\n",
    "print(f'my perplexity:              {int(my_perplexity)}')\n",
    "print(f'correct initial perplexity: {vocab_size}')\n",
    "\n",
    "assert math.isclose(my_perplexity, vocab_size, abs_tol=2000)\n",
    "print('Passou o no assert da perplexidade')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TREINAMENTO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "class DataManager(Thread):\n",
    "    def __init__(self, data_loader, buffer_size=3, device=torch.device(\"cpu\"), data_type=torch.float32):\n",
    "        \"\"\"\n",
    "This manager intends to load a PyTorch dataloader like from disk into memory,\n",
    "reducing the acess time. It does not easily overflow memory, because we set a\n",
    "buffer size limiting how many samples will be loaded at once. Everytime a sample\n",
    "is consumed by the calling thread, another one is replaced in the\n",
    "buffer (unless we reach the end of dataloader).\n",
    "\n",
    "A manger may be called exactly like a dataloader, an it's based in an internal\n",
    "thread that loads samples into memory in parallel. This is specially useful\n",
    "when you are training in GPU and processor is almost idle.\n",
    "\n",
    "        :param data_loader: Base dataloader to load in parallel.\n",
    "        :param buffer_size: How many samples to keep loaded (caution to not overflow RAM). Default: 3.\n",
    "        :param device: Torch device to put samples in, like torch.device(\"cpu\") (default). It saves time by transfering in parallel.\n",
    "        :param data_type: Automatically casts tensor type. Default: torch.float32.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.buffer_queue = Queue(maxsize=buffer_size)\n",
    "        self.data_loader = data_loader\n",
    "        self.buffer_size = buffer_size\n",
    "        self.device = device\n",
    "        self.data_type = data_type\n",
    "\n",
    "        self.dataloader_finished = False\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "Runs the internal thread that iterates over the dataloader until fulfilling the\n",
    "buffer or the end of samples.\n",
    "        \"\"\"\n",
    "        for i, (x, y) in enumerate(self.data_loader):\n",
    "            # Important to set before put in queue to avoid race condition\n",
    "            # would happen if trying to get() in next() method before setting this flag\n",
    "            if i >= len(self) - 1:\n",
    "                self.dataloader_finished = True\n",
    "\n",
    "            self.buffer_queue.put([x.to(self.data_type).to(self.device),\n",
    "                                   y.to(self.data_type).to(self.device)])\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "Returns an iterable of itself.\n",
    "\n",
    "        :return: Iterable around this class.\n",
    "        \"\"\"\n",
    "        self.start()\n",
    "        self.dataloader_finished = False\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "Intended to be used as iterator.\n",
    "\n",
    "        :return: Next iteration element.\n",
    "        \"\"\"\n",
    "        if self.dataloader_finished is True and self.buffer_queue.empty():\n",
    "            raise StopIteration()\n",
    "\n",
    "        return self.buffer_queue.get()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3072/600000 [00:28<3:10:13, 52.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps; 0 examples so far; train ppl: 3680.99, valid ppl: 3687.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600064it [01:12, 8238.61it/s]                             \n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "max_examples = 600000\n",
    "eval_every_steps = 5000\n",
    "lr = 4e-5\n",
    "use_amp = True\n",
    "\n",
    "model = LanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_length=context_size,\n",
    "    dim=64,\n",
    "    n_layers=2,\n",
    ").to(device)\n",
    "train_loader = DataLoader(training_dataset, batch_size=1024, shuffle=True, num_workers=4)\n",
    "validation_loader = DataLoader(valid_dataset, batch_size=1024, num_workers=4, )\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.9, min_lr=3e-5, patience=15000, threshold=1e-1, verbose=True)\n",
    "scaler=GradScaler()\n",
    "\n",
    "\n",
    "def train_step(input_ids, target_ids):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    with autocast(enabled=use_amp):\n",
    "        logits = model(input_ids)\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "        target_ids = target_ids.reshape(-1)\n",
    "    loss = nn.functional.cross_entropy(logits, target_ids, )\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation_step(input_ids, target_ids):\n",
    "    model.eval()\n",
    "    with autocast(enabled=use_amp):\n",
    "        logits = model(input_ids)\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "        target_ids = target_ids.reshape(-1)\n",
    "        loss = nn.functional.cross_entropy(logits, target_ids,)\n",
    "    return loss.item()\n",
    "\n",
    "best_validation_ppl = 9999\n",
    "train_losses = []\n",
    "n_examples = 0\n",
    "step = 0\n",
    "pbar = tqdm(total=max_examples)\n",
    "while n_examples < max_examples:\n",
    "    for train_input_ids, train_target_ids in DataManager(train_loader, device=device, buffer_size=1, data_type=None):\n",
    "        loss = train_step(train_input_ids.to(device), train_target_ids.to(device))\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        # LR scheduler\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        if step % eval_every_steps == 0:\n",
    "            train_ppl = np.exp(np.average(train_losses))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_ppl = np.exp(np.average([\n",
    "                    validation_step(val_input_ids.to(device), val_target_ids.to(device))\n",
    "                    for val_input_ids, val_target_ids in DataManager(validation_loader, device=device, buffer_size=1, data_type=None)]))\n",
    "                # Checkpoint to best models found.\n",
    "                if best_validation_ppl > valid_ppl:\n",
    "                    # Update the new best perplexity.\n",
    "                    best_validation_ppl = valid_ppl\n",
    "                    model.eval()\n",
    "                    torch.save(model, \"best_model.pth\")\n",
    "\n",
    "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
    "            train_losses = []\n",
    "\n",
    "        n_examples += len(train_input_ids)  # Increment of batch size\n",
    "        step += 1\n",
    "        pbar.update(len(train_input_ids))\n",
    "        if n_examples >= max_examples:\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Restore best model (checkpoint) found\n",
    "model = torch.load(\"best_model.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Avaliação no dataset de Teste"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 11601/84697 [00:57<06:03, 200.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-50-056435ed93d0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m     test_ppl = np.exp(np.average([\n\u001B[1;32m      5\u001B[0m         \u001B[0mvalidation_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m     ]))\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-50-056435ed93d0>\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     test_ppl = np.exp(np.average([\n\u001B[0;32m----> 5\u001B[0;31m         \u001B[0mvalidation_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m     ]))\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/tqdm/std.py\u001B[0m in \u001B[0;36m__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1128\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1129\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1130\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0miterable\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1131\u001B[0m                 \u001B[0;32myield\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m                 \u001B[0;31m# Update and possibly print the progressbar.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    515\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    516\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 517\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    518\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    519\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    555\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    556\u001B[0m         \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 557\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    558\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    559\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-42-5396f05d1e70>\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0midx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlong\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlong\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_ppl = np.exp(np.average([\n",
    "        validation_step(input.to(device), target.to(device))\n",
    "        for input, target in tqdm(test_loader)\n",
    "    ]))\n",
    "\n",
    "print(f'test perplexity: {test_ppl}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testando gerar uma sentença"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = 'Frankenstein tells the story of gifted scientist Victor Frankenstein who succeeds in giving life to '\n",
    "max_output_tokens = 10\n",
    "\n",
    "for _ in range(max_output_tokens):\n",
    "    input_ids = tokenizer(text=prompt)\n",
    "    input_ids_truncated = input_ids[-context_size:]# Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
    "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
    "    # Ao usarmos o argmax, a saída do modelo em cada passo é token de maior probabilidade.\n",
    "    # Isso se chama decodificação gulosa (greedy decoding).\n",
    "    predicted_id = torch.argmax(logits).item()\n",
    "    predicted_word = tokenizer.decode([predicted_id,])\n",
    "    prompt += predicted_word[0]\n",
    "    print(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}