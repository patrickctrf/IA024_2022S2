{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "pycharm-79e5302d",
   "language": "python",
   "display_name": "PyCharm (IA024_2022S2)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/patrickctrf/IA024_2022S2/blob/main/ex06/patrick_ferreira/ex06_175480_patrick_ferreira.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook de referência \n",
    "\n",
    "Nome: Patrick de Carvalho Tavares Rezende Ferreira"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instruções:\n",
    "\n",
    "\n",
    "Treinar e medir a acurácia de um modelo BERT (ou variantes) para classificação binária usando o dataset do IMDB (20k/5k amostras de treino/validação).\n",
    "\n",
    "Importante: \n",
    "- Deve-se implementar o próprio laço de treinamento.\n",
    "- Implementar o acumulo de gradiente.\n",
    "\n",
    "Dicas:\n",
    "- BERT geralmente costuma aprender bem uma tarefa com poucas épocas (de 3 a 5 épocas). Se tiver demorando mais de 5 épocas para chegar em 80% de acurácia, ajuste os hiperparametros.\n",
    "\n",
    "- Solução para erro de memória:\n",
    "  - Usar bfloat16 permite quase dobrar o batch size\n",
    "\n",
    "Opcional:\n",
    "- Pode-se usar a função trainer da biblioteca Transformers/HuggingFace para verificar se seu laço de treinamento está correto. Note que ainda assim é obrigatório implementar o laço próprio.\n",
    "\n",
    "- Usar pytorch lightning. Para entender como o pytorch lightning funciona, veja uma implementação simplificada no notebook `06_01_Treino_Validação_MNIST_Lightning_Lite.ipynb`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fixando a seed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print('Using {}'.format(device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparando Dados"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Primeiro, fazemos download do dataset:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# !wget -nc http://files.fast.ai/data/aclImdb.tgz\n",
    "# !tar -xzf aclImdb.tgz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (4.19.2)\r\n",
      "Requirement already satisfied: requests in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (2.24.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (0.12.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (1.19.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (20.4)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (4.48.2)\r\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (1.7.0)\r\n",
      "Requirement already satisfied: filelock in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (3.0.12)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (2020.7.14)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (0.6.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (5.3.1)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests->transformers) (2.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests->transformers) (1.25.10)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.7)\r\n",
      "Requirement already satisfied: six in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from packaging>=20.0->transformers) (1.15.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Carregando o dataset\n",
    "\n",
    "Criaremos uma divisão de treino (20k exemplos) e validação (5k exemplos) artificialmente."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/pos'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-5f538527f19f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mcwd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetcwd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m \u001B[0mx_train_pos\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_texts\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcwd\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'aclImdb'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msep\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'train'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msep\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'pos'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0mx_train_neg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_texts\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcwd\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'aclImdb'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msep\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'train'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msep\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'neg'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0mx_test_pos\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_texts\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcwd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'aclImdb'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msep\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'test'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msep\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'pos'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-5-5f538527f19f>\u001B[0m in \u001B[0;36mload_texts\u001B[0;34m(folder)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mload_texts\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfolder\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0mtexts\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlistdir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfolder\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfolder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m             \u001B[0mtexts\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/pos'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "max_valid = 5000\n",
    "\n",
    "def load_texts(folder):\n",
    "    texts = []\n",
    "    for path in tqdm(os.listdir(folder)):\n",
    "        with open(os.path.join(folder, path)) as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "x_train_pos = load_texts(os.path.join(cwd,'aclImdb' + os.sep + 'train' + os.sep + 'pos'))\n",
    "x_train_neg = load_texts(os.path.join(cwd,'aclImdb' + os.sep + 'train' + os.sep + 'neg'))\n",
    "x_test_pos = load_texts(os.path.join(cwd, 'aclImdb' + os.sep + 'test' + os.sep + 'pos'))\n",
    "x_test_neg = load_texts(os.path.join(cwd, 'aclImdb' + os.sep + 'test' + os.sep + 'neg'))\n",
    "\n",
    "x_train = x_train_pos + x_train_neg\n",
    "x_test = x_test_pos + x_test_neg\n",
    "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
    "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
    "\n",
    "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
    "c = list(zip(x_train, y_train))\n",
    "random.shuffle(c)\n",
    "x_train, y_train = zip(*c)\n",
    "\n",
    "x_valid = x_train[-max_valid:]\n",
    "y_valid = y_train[-max_valid:]\n",
    "x_train = x_train[:-max_valid]\n",
    "y_train = y_train[:-max_valid]\n",
    "\n",
    "print(len(x_train), 'amostras de treino.')\n",
    "print(len(x_valid), 'amostras de desenvolvimento.')\n",
    "print(len(x_test), 'amostras de teste.')\n",
    "\n",
    "print('3 primeiras amostras treino:')\n",
    "for x, y in zip(x_train[:3], y_train[:3]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 últimas amostras treino:')\n",
    "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 primeiras amostras validação:')\n",
    "for x, y in zip(x_valid[:3], y_test[:3]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 últimas amostras validação:')\n",
    "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
    "    print(y, x[:100])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Criando classe do dataset\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts: List[str], labels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], torch.tensor([0., 1.]) if self.labels[idx] else torch.tensor([1., 0.])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dados de treino, validação e teste"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_dataset = MyDataset(x_train, y_train)\n",
    "valid_dataset = MyDataset(x_valid, y_valid)\n",
    "test_dataset = MyDataset(x_test, y_test)\n",
    "\n",
    "print(f'training examples: {len(training_dataset)}')\n",
    "print(f'valid examples: {len(valid_dataset)}')\n",
    "print(f'test examples: {len(test_dataset)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testando se o modelo processa os dados corretamente"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "model.train().to(device)\n",
    "\n",
    "sample_train, _ = next(iter(DataLoader(training_dataset, batch_size=4)))\n",
    "\n",
    "sample_train = tokenizer.batch_encode_plus(sample_train, padding=True, return_tensors=\"pt\", truncation=True, max_length=200).to(device)\n",
    "\n",
    "model(**sample_train).logits.shape\n",
    "\n",
    "del sample_train\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TREINAMENTO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "max_examples = 60_000_000\n",
    "eval_every_steps = 5000\n",
    "lr = 4e-5\n",
    "use_amp = True\n",
    "\n",
    "train_loader = DataLoader(training_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "validation_loader = DataLoader(valid_dataset, batch_size=128, num_workers=4, )\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.9, min_lr=3e-5, patience=15000, threshold=1e-1, verbose=True)\n",
    "scaler=GradScaler()\n",
    "\n",
    "\n",
    "def train_step(input_ids, target_ids):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    with autocast(enabled=use_amp):\n",
    "        logits = model(**input_ids).logits\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "    loss = nn.functional.cross_entropy(logits, target_ids, )\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation_step(input_ids, target_ids):\n",
    "    model.eval()\n",
    "    accuracy = 0\n",
    "    with autocast(enabled=use_amp):\n",
    "        logits = model(**input_ids).logits\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "        loss = nn.functional.cross_entropy(logits, target_ids,)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        accuracy += (preds == y).sum()\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "best_validation_loss = 9999\n",
    "train_losses = []\n",
    "n_examples = 0\n",
    "step = 0\n",
    "pbar = tqdm(total=max_examples)\n",
    "while n_examples < max_examples:\n",
    "    for train_input_ids, train_target_ids in train_loader:\n",
    "        train_input_ids = tokenizer.batch_encode_plus(train_input_ids, padding=True, return_tensors=\"pt\", truncation=True, max_length=200).to(device)\n",
    "        loss = train_step(train_input_ids, train_target_ids.to(device))\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        # LR scheduler\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        if step % eval_every_steps == 0:\n",
    "            train_loss = np.average(train_losses)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_loss = np.average([\n",
    "                    validation_step(tokenizer.batch_encode_plus(val_input_ids, padding=True, return_tensors=\"pt\", truncation=True, max_length=200).to(device), val_target_ids.to(device))[0]\n",
    "                    for val_input_ids, val_target_ids in validation_loader])\n",
    "                # Checkpoint to best models found.\n",
    "                if best_validation_loss > valid_loss:\n",
    "                    # Update the new best perplexity.\n",
    "                    best_validation_loss = valid_loss\n",
    "                    model.eval()\n",
    "                    torch.save(model, \"best_model.pth\")\n",
    "\n",
    "            print(f'{step} steps; {n_examples} examples so far; train loss: {train_loss:.2f}, valid loss: {valid_loss:.2f}')\n",
    "            train_losses = []\n",
    "\n",
    "        n_examples += len(train_input_ids)  # Increment of batch size\n",
    "        step += 1\n",
    "        pbar.update(len(train_input_ids))\n",
    "        if n_examples >= max_examples:\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Restore best model (checkpoint) found\n",
    "model = torch.load(\"best_model.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Avaliação no dataset de Teste"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss = np.average([\n",
    "        validation_step(input.to(device), target.to(device))[0]\n",
    "        for input, target in tqdm(test_loader)\n",
    "    ])\n",
    "\n",
    "    test_accuracy = np.average([\n",
    "        validation_step(input.to(device), target.to(device))[1]\n",
    "        for input, target in tqdm(test_loader)\n",
    "    ])\n",
    "\n",
    "print(f'test loss: {test_loss}', f'test acc: {test_accuracy}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}