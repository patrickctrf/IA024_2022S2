{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patrickctrf/IA024_2022S2/blob/main/ex07/patrick_ferreira/ex07_patrick_ferreira_175480.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OG5DT_dm6mk"
      },
      "source": [
        "# Notebook de referência \n",
        "\n",
        "Nome: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od7iUgHy5SSi"
      },
      "source": [
        "## Instruções\n",
        "\n",
        "Neste colab iremos treinar um modelo T5 para traduzir de inglês para português. Iremos treiná-lo com o data Paracrawl.\n",
        "\n",
        "- Usaremos o dataset Paracrawl Inglês-Português. Truncamos o dataset de treino para apenas 100k pares para deixar o treinamento mais rápido. Quem quiser pode treinar com mais amostras. Se demorar muito para treinar, truncar o dataset ainda mais.\n",
        "\n",
        "- Usaremos o BLEU como métrica. Usaremos o SacreBLEU pois sempre faz o mesmo pré-processamento (tokenização, lowercase). Não usaremos torchnlp.metrics.bleu, torchtext.data.metrics.bleu_score, etc. SacreBLEU é lento: usar poucas amostras de validação (ex: 5k)\n",
        "\n",
        "\n",
        "Usaremos o modelo PTT5 disponível no model hub da HuggingFace:\n",
        "\n",
        "https://huggingface.co/unicamp-dl/ptt5-small-portuguese-vocab\n",
        "\n",
        "Este é  um T5 pré-treinado em textos em português e com tokenizador em português.\n",
        "\n",
        "É recomendável salvar os pesos do modelo e estado dos otimizadores, pois o treinamento é longo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgW-boJLU0wU"
      },
      "source": [
        "# Configurações gerais\n",
        "model_name = \"unicamp-dl/ptt5-small-portuguese-vocab\"\n",
        "batch_size = 64\n",
        "accumulate_grad_batches = 2\n",
        "source_max_length = 128\n",
        "target_max_length = 128\n",
        "learning_rate = 1e-3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mXaMmG4cb-F"
      },
      "source": [
        "! pip install sacrebleu\n",
        "! pip install transformers\n",
        "! pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob7qL6kUVjbu"
      },
      "source": [
        "# Importar todos os pacotes de uma só vez para evitar duplicados ao longo do notebook.\n",
        "import gzip\n",
        "import os\n",
        "import random\n",
        "import sacrebleu\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from transformers import T5ForConditionalGeneration\n",
        "from transformers import T5Tokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from typing import Dict\n",
        "from typing import List\n",
        "from typing import Tuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJlZDb1VY29r"
      },
      "source": [
        "# Important: Fix seeds so we can replicate results\n",
        "seed = 123\n",
        "random.seed(seed)\n",
        "torch.random.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETfkvMGl4JA1"
      },
      "source": [
        "Iremos salvar os checkpoints (pesos do modelo) no google drive, para que possamos continuar o treino de onde paramos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-Co8U6O4Gl3"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM"
      },
      "source": [
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wbnfzst5O3k"
      },
      "source": [
        "! wget -nc https://storage.googleapis.com/unicamp-dl/ia024a_2022s2/paracrawl_enpt_train.tsv.gz\n",
        "! wget -nc https://storage.googleapis.com/unicamp-dl/ia024a_2022s2/paracrawl_enpt_test.tsv.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (100k pares) e val (5k pares) artificialmente.\n",
        "\n",
        "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HIN_xLI_TuT"
      },
      "source": [
        "def load_text_pairs(path):\n",
        "    text_pairs = []\n",
        "    for line in gzip.open(path, mode='rt'):\n",
        "        text_pairs.append(line.strip().split('\\t'))\n",
        "    return text_pairs\n",
        "\n",
        "x_train = load_text_pairs('paracrawl_enpt_train.tsv.gz')\n",
        "x_test = load_text_pairs('paracrawl_enpt_test.tsv.gz')\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/val.\n",
        "random.shuffle(x_train)\n",
        "\n",
        "# Truncamos o dataset para 100k pares de treino e 5k pares de validação.\n",
        "x_val = x_train[100000:105000]\n",
        "x_train = x_train[:100000]\n",
        "\n",
        "for set_name, x in [('treino', x_train), ('validação', x_val), ('test', x_test)]:\n",
        "    print(f'\\n{len(x)} amostras de {set_name}')\n",
        "    print(f'3 primeiras amostras {set_name}:')\n",
        "    for i, (source, target) in enumerate(x[:3]):\n",
        "        print(f'{i}: source: {source}\\n   target: {target}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXnoYK5YXKgk"
      },
      "source": [
        "Criando Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLrftKzSPBs_"
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMen-JFKLFCb"
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, text_pairs: List[Tuple[str]], tokenizer,\n",
        "                 source_max_length: int = 32, target_max_length: int = 32):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_pairs = text_pairs\n",
        "        self.source_max_length = source_max_length\n",
        "        self.target_max_length = target_max_length\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.text_pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        source, target = self.text_pairs[idx]\n",
        "        # TODO: tokenizar texto\n",
        "        \n",
        "        return (source_token_ids, source_mask, target_token_ids, target_mask, original_source, original_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cloyt0tIwIiD"
      },
      "source": [
        "## Testando o DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoKiQXCvwGrP"
      },
      "source": [
        "text_pairs = [('we like pizza', 'eu gosto de pizza')]\n",
        "dataset_debug = MyDataset(\n",
        "    text_pairs=text_pairs,\n",
        "    tokenizer=tokenizer,\n",
        "    source_max_length=source_max_length,\n",
        "    target_max_length=target_max_length)\n",
        "\n",
        "dataloader_debug = DataLoader(dataset_debug, batch_size=10, shuffle=True, \n",
        "                              num_workers=0)\n",
        "\n",
        "source_token_ids, source_mask, target_token_ids, target_mask, _, _ = next(iter(dataloader_debug))\n",
        "print('source_token_ids:\\n', source_token_ids)\n",
        "print('source_mask:\\n', source_mask)\n",
        "print('target_token_ids:\\n', target_token_ids)\n",
        "print('target_mask:\\n', target_mask)\n",
        "\n",
        "print('source_token_ids.shape:', source_token_ids.shape)\n",
        "print('source_mask.shape:', source_mask.shape)\n",
        "print('target_token_ids.shape:', target_token_ids.shape)\n",
        "print('target_mask.shape:', target_mask.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBptbyTvXBhC"
      },
      "source": [
        "## Criando DataLoaders de Treino/Val/Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2_Fcs0VXD5W"
      },
      "source": [
        "dataset_train = MyDataset(text_pairs=x_train,\n",
        "                          tokenizer=tokenizer,\n",
        "                          source_max_length=source_max_length,\n",
        "                          target_max_length=target_max_length)\n",
        "\n",
        "dataset_val = MyDataset(text_pairs=x_val,\n",
        "                        tokenizer=tokenizer,\n",
        "                        source_max_length=source_max_length,\n",
        "                        target_max_length=target_max_length)\n",
        "\n",
        "dataset_test = MyDataset(text_pairs=x_test,\n",
        "                         tokenizer=tokenizer,\n",
        "                         source_max_length=source_max_length,\n",
        "                         target_max_length=target_max_length)\n",
        "\n",
        "train_dataloader = DataLoader(dataset_train, batch_size=batch_size,\n",
        "                              shuffle=True, num_workers=0)\n",
        "\n",
        "val_dataloader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, \n",
        "                            num_workers=0)\n",
        "\n",
        "test_dataloader = DataLoader(dataset_test, batch_size=batch_size,\n",
        "                             shuffle=False, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}