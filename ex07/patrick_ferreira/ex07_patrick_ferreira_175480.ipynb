{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "pycharm-79e5302d",
   "language": "python",
   "display_name": "PyCharm (IA024_2022S2)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/patrickctrf/IA024_2022S2/blob/main/ex07/patrick_ferreira/ex07_patrick_ferreira_175480.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OG5DT_dm6mk"
   },
   "source": [
    "# Notebook de referência \n",
    "\n",
    "Nome: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od7iUgHy5SSi"
   },
   "source": [
    "## Instruções\n",
    "\n",
    "Neste colab iremos treinar um modelo T5 para traduzir de inglês para português. Iremos treiná-lo com o data Paracrawl.\n",
    "\n",
    "- Usaremos o dataset Paracrawl Inglês-Português. Truncamos o dataset de treino para apenas 100k pares para deixar o treinamento mais rápido. Quem quiser pode treinar com mais amostras. Se demorar muito para treinar, truncar o dataset ainda mais.\n",
    "\n",
    "- Usaremos o BLEU como métrica. Usaremos o SacreBLEU pois sempre faz o mesmo pré-processamento (tokenização, lowercase). Não usaremos torchnlp.metrics.bleu, torchtext.data.metrics.bleu_score, etc. SacreBLEU é lento: usar poucas amostras de validação (ex: 5k)\n",
    "\n",
    "\n",
    "Usaremos o modelo PTT5 disponível no model hub da HuggingFace:\n",
    "\n",
    "https://huggingface.co/unicamp-dl/ptt5-small-portuguese-vocab\n",
    "\n",
    "Este é  um T5 pré-treinado em textos em português e com tokenizador em português.\n",
    "\n",
    "É recomendável salvar os pesos do modelo e estado dos otimizadores, pois o treinamento é longo.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FgW-boJLU0wU"
   },
   "source": [
    "# Configurações gerais\n",
    "from asyncio import Queue\n",
    "from threading import Thread\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"unicamp-dl/ptt5-small-portuguese-vocab\"\n",
    "batch_size = 64\n",
    "accumulate_grad_batches = 2\n",
    "source_max_length = 128\n",
    "target_max_length = 128\n",
    "learning_rate = 1e-3"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0mXaMmG4cb-F"
   },
   "source": [
    "! pip install sacrebleu\n",
    "! pip install transformers\n",
    "! pip install sentencepiece"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (2.2.1)\r\n",
      "Requirement already satisfied: portalocker in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from sacrebleu) (2.5.1)\r\n",
      "Requirement already satisfied: lxml in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from sacrebleu) (4.5.2)\r\n",
      "Requirement already satisfied: colorama in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from sacrebleu) (0.4.3)\r\n",
      "Requirement already satisfied: regex in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from sacrebleu) (2020.7.14)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from sacrebleu) (1.19.1)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from sacrebleu) (0.9.0)\r\n",
      "Requirement already satisfied: transformers in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (4.19.2)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (0.6.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (4.48.2)\r\n",
      "Requirement already satisfied: requests in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (2.24.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (5.3.1)\r\n",
      "Requirement already satisfied: filelock in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (3.0.12)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (1.19.1)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (0.12.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (20.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (2020.7.14)\r\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (1.7.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests->transformers) (2.10)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests->transformers) (1.25.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\r\n",
      "Requirement already satisfied: six in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from packaging>=20.0->transformers) (1.15.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.7)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.1.0)\r\n",
      "Requirement already satisfied: sentencepiece in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (0.1.97)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ob7qL6kUVjbu"
   },
   "source": [
    "# Importar todos os pacotes de uma só vez para evitar duplicados ao longo do notebook.\n",
    "import gzip\n",
    "import os\n",
    "import random\n",
    "import sacrebleu\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from google.colab import drive\n",
    "\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import T5Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Tuple"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bJlZDb1VY29r"
   },
   "source": [
    "# Important: Fix seeds so we can replicate results\n",
    "seed = 123\n",
    "random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print('Using {}'.format(device))"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETfkvMGl4JA1"
   },
   "source": [
    "Iremos salvar os checkpoints (pesos do modelo) no google drive, para que possamos continuar o treino de onde paramos."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J-Co8U6O4Gl3"
   },
   "source": [
    "# drive.mount('/content/drive')"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXFdJz2KVeQw"
   },
   "source": [
    "## Preparando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHMi_Kq65fPM"
   },
   "source": [
    "Primeiro, fazemos download do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2wbnfzst5O3k"
   },
   "source": [
    "! wget -nc https://storage.googleapis.com/unicamp-dl/ia024a_2022s2/paracrawl_enpt_train.tsv.gz\n",
    "! wget -nc https://storage.googleapis.com/unicamp-dl/ia024a_2022s2/paracrawl_enpt_test.tsv.gz"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘paracrawl_enpt_train.tsv.gz’ already there; not retrieving.\r\n",
      "\r\n",
      "File ‘paracrawl_enpt_test.tsv.gz’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Giyi5Rv_NIm"
   },
   "source": [
    "## Carregando o dataset\n",
    "\n",
    "Criaremos uma divisão de treino (100k pares) e val (5k pares) artificialmente.\n",
    "\n",
    "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0HIN_xLI_TuT"
   },
   "source": [
    "def load_text_pairs(path):\n",
    "    text_pairs = []\n",
    "    for line in gzip.open(path, mode='rt'):\n",
    "        text_pairs.append(line.strip().split('\\t'))\n",
    "    return text_pairs\n",
    "\n",
    "x_train = load_text_pairs('paracrawl_enpt_train.tsv.gz')\n",
    "x_test = load_text_pairs('paracrawl_enpt_test.tsv.gz')\n",
    "\n",
    "# Embaralhamos o treino para depois fazermos a divisão treino/val.\n",
    "random.shuffle(x_train)\n",
    "\n",
    "# Truncamos o dataset para 100k pares de treino e 5k pares de validação.\n",
    "x_val = x_train[100000:105000]\n",
    "x_train = x_train[:100000]\n",
    "\n",
    "for set_name, x in [('treino', x_train), ('validação', x_val), ('test', x_test)]:\n",
    "    print(f'\\n{len(x)} amostras de {set_name}')\n",
    "    print(f'3 primeiras amostras {set_name}:')\n",
    "    for i, (source, target) in enumerate(x[:3]):\n",
    "        print(f'{i}: source: {source}\\n   target: {target}')"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100000 amostras de treino\n",
      "3 primeiras amostras treino:\n",
      "0: source: More Croatian words and phrases\n",
      "   target: Mais palavras e frases em croata\n",
      "1: source: Jerseys and pullovers, containing at least 50Â % by weight of wool and weighing 600Â g or more per article 6110 11 10 (PCE)\n",
      "   target: Camisolas e pulôveres, com pelo menos 50 %, em peso, de lã e pesando 600g ou mais por unidade 6110 11 10 (PCE)\n",
      "2: source: Atex Colombia SAS makes available its lead product, 100% natural liquid latex, excellent quality and price. ... Welding manizales caldas Colombia a DuckDuckGo\n",
      "   target: Atex Colômbia SAS torna principal produto está disponível, látex líquido 100% natural, excelente qualidade e preço. ...\n",
      "\n",
      "5000 amostras de validação\n",
      "3 primeiras amostras validação:\n",
      "0: source: «You have hidden these things from the wise and the learned you have revealed them to the childlike»\n",
      "   target: «Escondeste estas coisas aos sábios e entendidos e as revelaste aos pequenos»\n",
      "1: source: Repair of computers, application programming, network installations, web design, graphic design, and also the most. Computer consulting in Santa Lucía\n",
      "   target: Reparação de computadores, programação de aplicações, instalações de rede, web design, design gráfico, e também a.\n",
      "2: source: He was born in Fafe (Braga) and he graduated in Law in Coimbra University.\n",
      "   target: É natural de Fafe (Braga) e Licenciado em Direito pela Universidade de Coimbra.\n",
      "\n",
      "20000 amostras de test\n",
      "3 primeiras amostras test:\n",
      "0: source: In this way, the civil life of a nation matures, making it possible for all citizens to enjoy the fruits of genuine tolerance and mutual respect.\n",
      "   target: Deste modo, a vida civil de uma nação amadurece, fazendo com que todos os cidadãos gozem dos frutos da tolerância genuína e do respeito mútuo.\n",
      "1: source: 1999 XIII. Winnipeg, Canada July 23 to August 8\n",
      "   target: 1999 XIII. Winnipeg, Canadá 23 de julho a 8 de agosto\n",
      "2: source: In the mystery of Christmas, Christ's light shines on the earth, spreading, as it were, in concentric circles.\n",
      "   target: No mistério do Natal, a luz de Cristo irradia-se sobre a terra, difundindo-se como círculos concêntricos.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXnoYK5YXKgk"
   },
   "source": [
    "Criando Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pLrftKzSPBs_"
   },
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OMen-JFKLFCb"
   },
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, text_pairs: List[Tuple[str]], tokenizer,\n",
    "                 source_max_length: int = 32, target_max_length: int = 32):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_pairs = text_pairs\n",
    "        self.source_max_length = source_max_length\n",
    "        self.target_max_length = target_max_length\n",
    "\n",
    "        self.sources, self.targets = list(zip(*text_pairs))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # source, target = self.text_pairs[idx]\n",
    "        # TODO: tokenizar texto\n",
    "\n",
    "        source_token_ids =  tokenizer(self.sources, padding=True, truncation=True, max_length=self.source_max_length, return_tensors = \"pt\").input_ids\n",
    "        source_mask =       tokenizer(self.sources, padding=True, truncation=True, max_length=self.source_max_length, return_tensors = \"pt\").attention_mask\n",
    "        target_token_ids =  tokenizer(self.targets, padding=True, truncation=True, max_length=self.source_max_length, return_tensors = \"pt\").input_ids\n",
    "        target_mask =       tokenizer(self.targets, padding=True, truncation=True, max_length=self.source_max_length, return_tensors = \"pt\").attention_mask\n",
    "\n",
    "        return source_token_ids, source_mask, target_token_ids, target_mask, self.sources[idx], self.targets[idx]"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cloyt0tIwIiD"
   },
   "source": [
    "## Testando o DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZoKiQXCvwGrP"
   },
   "source": [
    "text_pairs = [('we like pizza', 'eu gosto de pizza')]\n",
    "dataset_debug = MyDataset(\n",
    "    text_pairs=text_pairs,\n",
    "    tokenizer=tokenizer,\n",
    "    source_max_length=source_max_length,\n",
    "    target_max_length=target_max_length)\n",
    "\n",
    "dataloader_debug = DataLoader(dataset_debug, batch_size=10, shuffle=True, \n",
    "                              num_workers=0)\n",
    "\n",
    "source_token_ids, source_mask, target_token_ids, target_mask, _, _ = next(iter(dataloader_debug))\n",
    "print('source_token_ids:\\n', source_token_ids)\n",
    "print('source_mask:\\n', source_mask)\n",
    "print('target_token_ids:\\n', target_token_ids)\n",
    "print('target_mask:\\n', target_mask)\n",
    "\n",
    "print('source_token_ids.shape:', source_token_ids.shape)\n",
    "print('source_mask.shape:', source_mask.shape)\n",
    "print('target_token_ids.shape:', target_token_ids.shape)\n",
    "print('target_mask.shape:', target_mask.shape)"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_token_ids:\n",
      " tensor([[[  31, 1528, 1079,  634, 1241, 7531,    1]]])\n",
      "source_mask:\n",
      " tensor([[[1, 1, 1, 1, 1, 1, 1]]])\n",
      "target_token_ids:\n",
      " tensor([[[2077, 6618,    4, 1241, 7531,    1]]])\n",
      "target_mask:\n",
      " tensor([[[1, 1, 1, 1, 1, 1]]])\n",
      "source_token_ids.shape: torch.Size([1, 1, 7])\n",
      "source_mask.shape: torch.Size([1, 1, 7])\n",
      "target_token_ids.shape: torch.Size([1, 1, 6])\n",
      "target_mask.shape: torch.Size([1, 1, 6])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBptbyTvXBhC"
   },
   "source": [
    "## Criando DataLoaders de Treino/Val/Test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i2_Fcs0VXD5W"
   },
   "source": [
    "dataset_train = MyDataset(text_pairs=x_train,\n",
    "                          tokenizer=tokenizer,\n",
    "                          source_max_length=source_max_length,\n",
    "                          target_max_length=target_max_length)\n",
    "\n",
    "dataset_val = MyDataset(text_pairs=x_val,\n",
    "                        tokenizer=tokenizer,\n",
    "                        source_max_length=source_max_length,\n",
    "                        target_max_length=target_max_length)\n",
    "\n",
    "dataset_test = MyDataset(text_pairs=x_test,\n",
    "                         tokenizer=tokenizer,\n",
    "                         source_max_length=source_max_length,\n",
    "                         target_max_length=target_max_length)\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=batch_size,\n",
    "                              shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, \n",
    "                            num_workers=0)\n",
    "\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=batch_size,\n",
    "                             shuffle=False, num_workers=0)"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utilidade para converter dados de device em paralelo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DataManager(Thread):\n",
    "    def __init__(self, data_loader, buffer_size=3, device=torch.device(\"cpu\"), data_type=torch.float32):\n",
    "        \"\"\"\n",
    "This manager intends to load a PyTorch dataloader like from disk into memory,\n",
    "reducing the acess time. It does not easily overflow memory, because we set a\n",
    "buffer size limiting how many samples will be loaded at once. Everytime a sample\n",
    "is consumed by the calling thread, another one is replaced in the\n",
    "buffer (unless we reach the end of dataloader).\n",
    "\n",
    "A manger may be called exactly like a dataloader, an it's based in an internal\n",
    "thread that loads samples into memory in parallel. This is specially useful\n",
    "when you are training in GPU and processor is almost idle.\n",
    "\n",
    "        :param data_loader: Base dataloader to load in parallel.\n",
    "        :param buffer_size: How many samples to keep loaded (caution to not overflow RAM). Must be integer > 0. Default: 3.\n",
    "        :param device: Torch device to put samples in, like torch.device(\"cpu\") (default). It saves time by transfering in parallel.\n",
    "        :param data_type: Automatically casts tensor type. Default: torch.float32.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.buffer_queue = Queue(maxsize=buffer_size)\n",
    "        self.data_loader = data_loader\n",
    "        self.buffer_size = buffer_size\n",
    "        self.device = device\n",
    "        self.data_type = data_type\n",
    "\n",
    "        self.dataloader_finished = False\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "Runs the internal thread that iterates over the dataloader until fulfilling the\n",
    "buffer or the end of samples.\n",
    "        \"\"\"\n",
    "        for i, sample in enumerate(self.data_loader):\n",
    "            # Important to set before put in queue to avoid race condition\n",
    "            # would happen if trying to get() in next() method before setting this flag\n",
    "            if i >= len(self) - 1:\n",
    "                self.dataloader_finished = True\n",
    "\n",
    "            self.buffer_queue.put([x.to(dtype=self.data_type, device=self.device, non_blocking=True) for x in sample])\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "Returns an iterable of itself.\n",
    "\n",
    "        :return: Iterable around this class.\n",
    "        \"\"\"\n",
    "        self.start()\n",
    "        self.dataloader_finished = False\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "Intended to be used as iterator.\n",
    "\n",
    "        :return: Next iteration element.\n",
    "        \"\"\"\n",
    "        if self.dataloader_finished is True and self.buffer_queue.empty():\n",
    "            raise StopIteration()\n",
    "\n",
    "        return self.buffer_queue.get()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TREINAMENTO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "max_examples = 100_000\n",
    "eval_every_steps = 1000\n",
    "lr = 1e-3\n",
    "use_amp = True\n",
    "\n",
    "train_loader = DataManager(train_dataloader, device=device, data_type=None)\n",
    "validation_loader = DataManager(val_dataloader, device=device, data_type=None)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.9, min_lr=3e-5, patience=15000, threshold=1e-1, verbose=True)\n",
    "scaler=GradScaler()\n",
    "\n",
    "\n",
    "def train_step(source_tokens, source_mask, target_tokens, target_mask):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    with autocast(enabled=use_amp):\n",
    "        loss = model(input_ids = source_tokens, attention_mask = source_mask, decoder_attention_mask = target_mask, labels = target_tokens).loss\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation_step(source_tokens, source_mask, target_tokens, target_mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad:\n",
    "        with autocast(enabled=use_amp):\n",
    "            loss = model(input_ids = source_tokens, attention_mask = source_mask, decoder_attention_mask = target_mask, labels = target_tokens).loss\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "best_validation_loss = 9999\n",
    "train_losses = []\n",
    "n_examples = 0\n",
    "step = 0\n",
    "pbar = tqdm(total=max_examples)\n",
    "while n_examples < max_examples:\n",
    "    for mini_batch in train_loader:\n",
    "        loss = train_step(*mini_batch)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        # LR scheduler\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        if step % eval_every_steps == 0:\n",
    "            train_loss = np.average(train_losses)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_loss = np.average([\n",
    "                    validation_step(*mini_batch)\n",
    "                    for mini_batch in validation_loader])\n",
    "                # Checkpoint to best models found.\n",
    "                if best_validation_loss > valid_loss:\n",
    "                    # Update the new best perplexity.\n",
    "                    best_validation_loss = valid_loss\n",
    "                    model.eval()\n",
    "                    torch.save(model, \"best_model.pth\")\n",
    "\n",
    "            print(f'{step} steps; {n_examples} examples so far; train loss: {train_loss:.2f}, valid loss: {valid_loss:.2f}')\n",
    "            train_losses = []\n",
    "\n",
    "        n_examples += mini_batch[0].shape[0]  # Increment of batch size\n",
    "        step += 1\n",
    "        pbar.update(mini_batch[0].shape[0])\n",
    "        if n_examples >= max_examples:\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Restore best model (checkpoint) found\n",
    "model = torch.load(\"best_model.pth\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}