{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "<a href=\"https://colab.research.google.com/github/patrickctrf/IA024_2022S2/blob/main/ex07/patrick_ferreira/ex07_patrick_ferreira_175480.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OG5DT_dm6mk"
   },
   "source": [
    "# Notebook de referência \n",
    "\n",
    "Nome: Patrick de Carvalho Tavares Rezende Ferreira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od7iUgHy5SSi"
   },
   "source": [
    "## Instruções\n",
    "\n",
    "Neste colab iremos treinar um modelo T5 para traduzir de inglês para português. Iremos treiná-lo com o data Paracrawl.\n",
    "\n",
    "- Usaremos o dataset Paracrawl Inglês-Português. Truncamos o dataset de treino para apenas 100k pares para deixar o treinamento mais rápido. Quem quiser pode treinar com mais amostras. Se demorar muito para treinar, truncar o dataset ainda mais.\n",
    "\n",
    "- Usaremos o BLEU como métrica. Usaremos o SacreBLEU pois sempre faz o mesmo pré-processamento (tokenização, lowercase). Não usaremos torchnlp.metrics.bleu, torchtext.data.metrics.bleu_score, etc. SacreBLEU é lento: usar poucas amostras de validação (ex: 5k)\n",
    "\n",
    "\n",
    "Usaremos o modelo PTT5 disponível no model hub da HuggingFace:\n",
    "\n",
    "https://huggingface.co/unicamp-dl/ptt5-small-portuguese-vocab\n",
    "\n",
    "Este é  um T5 pré-treinado em textos em português e com tokenizador em português.\n",
    "\n",
    "É recomendável salvar os pesos do modelo e estado dos otimizadores, pois o treinamento é longo.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FgW-boJLU0wU"
   },
   "source": [
    "# Configurações gerais\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"unicamp-dl/ptt5-small-portuguese-vocab\"\n",
    "batch_size = 64\n",
    "accumulate_grad_batches = 2\n",
    "source_max_length = 128\n",
    "target_max_length = 128\n",
    "learning_rate = 5e-4"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0mXaMmG4cb-F"
   },
   "source": [
    "! pip install sacrebleu\n",
    "! pip install transformers\n",
    "! pip install sentencepiece"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (2.2.1)\r\n",
      "Requirement already satisfied: portalocker in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from sacrebleu) (2.5.1)\r\n",
      "Requirement already satisfied: colorama in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from sacrebleu) (0.4.4)\r\n",
      "Requirement already satisfied: lxml in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from sacrebleu) (4.8.0)\r\n",
      "Requirement already satisfied: regex in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from sacrebleu) (2022.3.15)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from sacrebleu) (1.21.5)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from sacrebleu) (0.8.9)\r\n",
      "Requirement already satisfied: transformers in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (4.23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: filelock in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.1)\r\n",
      "Requirement already satisfied: requests in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (0.10.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\r\n",
      "Requirement already satisfied: sentencepiece in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (0.1.97)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ob7qL6kUVjbu"
   },
   "source": [
    "# Importar todos os pacotes de uma só vez para evitar duplicados ao longo do notebook.\n",
    "import gzip\n",
    "import os\n",
    "import random\n",
    "import sacrebleu\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from google.colab import drive\n",
    "\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import T5Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Tuple"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bJlZDb1VY29r"
   },
   "source": [
    "# Important: Fix seeds so we can replicate results\n",
    "seed = 123\n",
    "random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print('Using {}'.format(device))"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETfkvMGl4JA1"
   },
   "source": [
    "Iremos salvar os checkpoints (pesos do modelo) no google drive, para que possamos continuar o treino de onde paramos."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J-Co8U6O4Gl3"
   },
   "source": [
    "# drive.mount('/content/drive')"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXFdJz2KVeQw"
   },
   "source": [
    "## Preparando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHMi_Kq65fPM"
   },
   "source": [
    "Primeiro, fazemos download do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2wbnfzst5O3k"
   },
   "source": [
    "! wget -nc https://storage.googleapis.com/unicamp-dl/ia024a_2022s2/paracrawl_enpt_train.tsv.gz\n",
    "! wget -nc https://storage.googleapis.com/unicamp-dl/ia024a_2022s2/paracrawl_enpt_test.tsv.gz"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘paracrawl_enpt_train.tsv.gz’ already there; not retrieving.\r\n",
      "\r\n",
      "File ‘paracrawl_enpt_test.tsv.gz’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Giyi5Rv_NIm"
   },
   "source": [
    "## Carregando o dataset\n",
    "\n",
    "Criaremos uma divisão de treino (100k pares) e val (5k pares) artificialmente.\n",
    "\n",
    "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0HIN_xLI_TuT"
   },
   "source": [
    "def load_text_pairs(path):\n",
    "    text_pairs = []\n",
    "    for line in gzip.open(path, mode='rt'):\n",
    "        text_pairs.append(line.strip().split('\\t'))\n",
    "    return text_pairs\n",
    "\n",
    "x_train = load_text_pairs('paracrawl_enpt_train.tsv.gz')\n",
    "x_test = load_text_pairs('paracrawl_enpt_test.tsv.gz')\n",
    "\n",
    "# Embaralhamos o treino para depois fazermos a divisão treino/val.\n",
    "random.shuffle(x_train)\n",
    "\n",
    "# Truncamos o dataset para 100k pares de treino e 5k pares de validação.\n",
    "truncate_size = 500000\n",
    "x_val = x_train[truncate_size:truncate_size + 5000]\n",
    "x_train = x_train[:truncate_size]\n",
    "\n",
    "for set_name, x in [('treino', x_train), ('validação', x_val), ('test', x_test)]:\n",
    "    print(f'\\n{len(x)} amostras de {set_name}')\n",
    "    print(f'3 primeiras amostras {set_name}:')\n",
    "    for i, (source, target) in enumerate(x[:3]):\n",
    "        print(f'{i}: source: {source}\\n   target: {target}')"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "500000 amostras de treino\n",
      "3 primeiras amostras treino:\n",
      "0: source: More Croatian words and phrases\n",
      "   target: Mais palavras e frases em croata\n",
      "1: source: Jerseys and pullovers, containing at least 50Â % by weight of wool and weighing 600Â g or more per article 6110 11 10 (PCE)\n",
      "   target: Camisolas e pulôveres, com pelo menos 50 %, em peso, de lã e pesando 600g ou mais por unidade 6110 11 10 (PCE)\n",
      "2: source: Atex Colombia SAS makes available its lead product, 100% natural liquid latex, excellent quality and price. ... Welding manizales caldas Colombia a DuckDuckGo\n",
      "   target: Atex Colômbia SAS torna principal produto está disponível, látex líquido 100% natural, excelente qualidade e preço. ...\n",
      "\n",
      "5000 amostras de validação\n",
      "3 primeiras amostras validação:\n",
      "0: source: Cum on face and Fisting watch online\n",
      "   target: Gozada no rosto e Fisting assistir online\n",
      "1: source: Cylinders in Huila, Colombia\n",
      "   target: Cilindros em Cesar, Colômbia\n",
      "2: source: Brooms and brushes in Santa Rita (Chalatenango, El Salvador)\n",
      "   target: Vassouras e escovas em San Miguel, El Salvador\n",
      "\n",
      "20000 amostras de test\n",
      "3 primeiras amostras test:\n",
      "0: source: In this way, the civil life of a nation matures, making it possible for all citizens to enjoy the fruits of genuine tolerance and mutual respect.\n",
      "   target: Deste modo, a vida civil de uma nação amadurece, fazendo com que todos os cidadãos gozem dos frutos da tolerância genuína e do respeito mútuo.\n",
      "1: source: 1999 XIII. Winnipeg, Canada July 23 to August 8\n",
      "   target: 1999 XIII. Winnipeg, Canadá 23 de julho a 8 de agosto\n",
      "2: source: In the mystery of Christmas, Christ's light shines on the earth, spreading, as it were, in concentric circles.\n",
      "   target: No mistério do Natal, a luz de Cristo irradia-se sobre a terra, difundindo-se como círculos concêntricos.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXnoYK5YXKgk"
   },
   "source": [
    "Criando Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pLrftKzSPBs_"
   },
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OMen-JFKLFCb"
   },
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, text_pairs: List[Tuple[str]], tokenizer,\n",
    "                 source_max_length: int = 32, target_max_length: int = 32):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_pairs = text_pairs\n",
    "        self.source_max_length = source_max_length\n",
    "        self.target_max_length = target_max_length\n",
    "\n",
    "        sources, targets = list(zip(*text_pairs))\n",
    "\n",
    "        self.sources_tokenizadas = tokenizer(sources, padding=True, truncation=True, max_length=self.source_max_length, return_tensors = \"pt\")\n",
    "        self.targets_tokenizadas = tokenizer(targets, padding=True, truncation=True, max_length=self.source_max_length, return_tensors = \"pt\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source, target = self.text_pairs[idx]\n",
    "        # TODO: tokenizar texto\n",
    "\n",
    "        source_token_ids =  self.sources_tokenizadas.input_ids[idx]\n",
    "        source_mask =       self.sources_tokenizadas.attention_mask[idx]\n",
    "        target_token_ids =  self.targets_tokenizadas.input_ids[idx]\n",
    "        target_mask =       self.targets_tokenizadas.attention_mask[idx]\n",
    "\n",
    "        return source_token_ids, source_mask, target_token_ids, target_mask, source, target"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cloyt0tIwIiD"
   },
   "source": [
    "## Testando o DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZoKiQXCvwGrP"
   },
   "source": [
    "text_pairs = [('we like pizza', 'eu gosto de pizza')]\n",
    "dataset_debug = MyDataset(\n",
    "    text_pairs=text_pairs,\n",
    "    tokenizer=tokenizer,\n",
    "    source_max_length=source_max_length,\n",
    "    target_max_length=target_max_length)\n",
    "\n",
    "dataloader_debug = DataLoader(dataset_debug, batch_size=10, shuffle=True, \n",
    "                              num_workers=0)\n",
    "\n",
    "source_token_ids, source_mask, target_token_ids, target_mask, _, _ = next(iter(dataloader_debug))\n",
    "print('source_token_ids:\\n', source_token_ids)\n",
    "print('source_mask:\\n', source_mask)\n",
    "print('target_token_ids:\\n', target_token_ids)\n",
    "print('target_mask:\\n', target_mask)\n",
    "\n",
    "print('source_token_ids.shape:', source_token_ids.shape)\n",
    "print('source_mask.shape:', source_mask.shape)\n",
    "print('target_token_ids.shape:', target_token_ids.shape)\n",
    "print('target_mask.shape:', target_mask.shape)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_token_ids:\n",
      " tensor([[  31, 1528, 1079,  634, 1241, 7531,    1]])\n",
      "source_mask:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1]])\n",
      "target_token_ids:\n",
      " tensor([[2077, 6618,    4, 1241, 7531,    1]])\n",
      "target_mask:\n",
      " tensor([[1, 1, 1, 1, 1, 1]])\n",
      "source_token_ids.shape: torch.Size([1, 7])\n",
      "source_mask.shape: torch.Size([1, 7])\n",
      "target_token_ids.shape: torch.Size([1, 6])\n",
      "target_mask.shape: torch.Size([1, 6])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBptbyTvXBhC"
   },
   "source": [
    "## Criando DataLoaders de Treino/Val/Test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i2_Fcs0VXD5W"
   },
   "source": [
    "dataset_train = MyDataset(text_pairs=x_train,\n",
    "                          tokenizer=tokenizer,\n",
    "                          source_max_length=source_max_length,\n",
    "                          target_max_length=target_max_length)\n",
    "\n",
    "dataset_val = MyDataset(text_pairs=x_val,\n",
    "                        tokenizer=tokenizer,\n",
    "                        source_max_length=source_max_length,\n",
    "                        target_max_length=target_max_length)\n",
    "\n",
    "dataset_test = MyDataset(text_pairs=x_test,\n",
    "                         tokenizer=tokenizer,\n",
    "                         source_max_length=source_max_length,\n",
    "                         target_max_length=target_max_length)\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=batch_size,\n",
    "                              shuffle=True, num_workers=4)\n",
    "\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, \n",
    "                            num_workers=4)\n",
    "\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=batch_size,\n",
    "                             shuffle=False, num_workers=4)"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utilidade para converter dados de device em paralelo\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TREINAMENTO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 64/500000 [00:10<23:11:57,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps; 0 examples so far; train loss: 23.53, valid loss: 30.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 12864/500000 [01:29<6:34:16, 20.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 steps; 12800 examples so far; train loss: 1.38, valid loss: 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 25664/500000 [02:47<6:39:06, 19.81it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 steps; 25600 examples so far; train loss: 0.87, valid loss: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 38464/500000 [04:08<6:35:57, 19.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 steps; 38400 examples so far; train loss: 0.81, valid loss: 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 40064/500000 [04:17<42:39, 179.71it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00626: reducing learning rate of group 0 to 4.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 51264/500000 [05:29<6:23:12, 19.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 steps; 51200 examples so far; train loss: 0.77, valid loss: 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 64064/500000 [06:52<6:14:23, 19.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 steps; 64000 examples so far; train loss: 0.74, valid loss: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 76864/500000 [08:13<5:51:17, 20.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200 steps; 76800 examples so far; train loss: 0.72, valid loss: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 76928/500000 [08:14<4:17:50, 27.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01202: reducing learning rate of group 0 to 4.0500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 89664/500000 [09:33<5:35:15, 20.40it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400 steps; 89600 examples so far; train loss: 0.70, valid loss: 0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 101632/500000 [10:37<35:25, 187.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01588: reducing learning rate of group 0 to 3.6450e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 102464/500000 [10:51<5:21:00, 20.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 steps; 102400 examples so far; train loss: 0.68, valid loss: 0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 114496/500000 [11:55<34:46, 184.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01789: reducing learning rate of group 0 to 3.2805e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 115264/500000 [12:09<5:10:16, 20.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 steps; 115200 examples so far; train loss: 0.66, valid loss: 0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 128064/500000 [13:27<5:02:59, 20.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 steps; 128000 examples so far; train loss: 0.66, valid loss: 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 132928/500000 [13:53<32:37, 187.50it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02077: reducing learning rate of group 0 to 2.9525e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 140864/500000 [14:45<4:50:44, 20.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200 steps; 140800 examples so far; train loss: 0.65, valid loss: 0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 153280/500000 [15:52<31:19, 184.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02395: reducing learning rate of group 0 to 2.6572e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 153664/500000 [16:04<4:42:11, 20.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400 steps; 153600 examples so far; train loss: 0.64, valid loss: 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 166464/500000 [17:22<4:30:12, 20.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2600 steps; 166400 examples so far; train loss: 0.63, valid loss: 0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 179264/500000 [18:40<4:20:32, 20.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800 steps; 179200 examples so far; train loss: 0.62, valid loss: 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 188608/500000 [19:31<28:17, 183.46it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02947: reducing learning rate of group 0 to 2.3915e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 192064/500000 [19:59<4:11:19, 20.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 steps; 192000 examples so far; train loss: 0.60, valid loss: 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 201472/500000 [20:50<26:46, 185.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03148: reducing learning rate of group 0 to 2.1523e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 204864/500000 [21:18<4:08:37, 19.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200 steps; 204800 examples so far; train loss: 0.60, valid loss: 0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 217664/500000 [22:38<3:49:45, 20.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3400 steps; 217600 examples so far; train loss: 0.60, valid loss: 0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 224256/500000 [23:15<25:42, 178.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03504: reducing learning rate of group 0 to 1.9371e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 230464/500000 [23:59<3:44:18, 20.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600 steps; 230400 examples so far; train loss: 0.59, valid loss: 0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 240640/500000 [24:54<23:13, 186.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03760: reducing learning rate of group 0 to 1.7434e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 243264/500000 [25:18<3:27:59, 20.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3800 steps; 243200 examples so far; train loss: 0.58, valid loss: 0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 256064/500000 [26:36<3:22:06, 20.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 steps; 256000 examples so far; train loss: 0.58, valid loss: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 260352/500000 [26:59<21:30, 185.73it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04068: reducing learning rate of group 0 to 1.5691e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 268864/500000 [27:55<3:07:25, 20.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200 steps; 268800 examples so far; train loss: 0.58, valid loss: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 273216/500000 [28:18<20:38, 183.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04269: reducing learning rate of group 0 to 1.4121e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 281664/500000 [29:14<3:00:35, 20.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4400 steps; 281600 examples so far; train loss: 0.59, valid loss: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 286080/500000 [29:38<19:08, 186.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04470: reducing learning rate of group 0 to 1.2709e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 294464/500000 [30:33<2:47:04, 20.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4600 steps; 294400 examples so far; train loss: 0.58, valid loss: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 298944/500000 [30:57<17:57, 186.68it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04671: reducing learning rate of group 0 to 1.1438e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 307264/500000 [31:51<2:39:10, 20.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800 steps; 307200 examples so far; train loss: 0.57, valid loss: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 311808/500000 [32:16<16:44, 187.34it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04872: reducing learning rate of group 0 to 1.0295e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 320064/500000 [33:09<2:26:39, 20.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 steps; 320000 examples so far; train loss: 0.57, valid loss: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 327552/500000 [33:50<15:45, 182.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05118: reducing learning rate of group 0 to 9.2651e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 332864/500000 [34:28<2:16:47, 20.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5200 steps; 332800 examples so far; train loss: 0.57, valid loss: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 340416/500000 [35:09<14:24, 184.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05319: reducing learning rate of group 0 to 8.3386e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 345664/500000 [35:46<2:07:10, 20.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400 steps; 345600 examples so far; train loss: 0.57, valid loss: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 358464/500000 [37:05<1:55:51, 20.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600 steps; 358400 examples so far; train loss: 0.56, valid loss: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 365184/500000 [37:41<12:37, 178.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05706: reducing learning rate of group 0 to 7.5047e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 371264/500000 [38:24<1:47:58, 19.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5800 steps; 371200 examples so far; train loss: 0.56, valid loss: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 378048/500000 [39:01<10:56, 185.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05907: reducing learning rate of group 0 to 6.7543e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 384064/500000 [39:42<1:34:10, 20.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 steps; 384000 examples so far; train loss: 0.55, valid loss: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 390912/500000 [40:19<09:43, 186.99it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06108: reducing learning rate of group 0 to 6.0788e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 396864/500000 [41:00<1:24:04, 20.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6200 steps; 396800 examples so far; train loss: 0.57, valid loss: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 403776/500000 [41:38<08:41, 184.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06309: reducing learning rate of group 0 to 5.4709e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 409664/500000 [42:19<1:13:58, 20.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400 steps; 409600 examples so far; train loss: 0.56, valid loss: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 416640/500000 [42:57<07:26, 186.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06510: reducing learning rate of group 0 to 4.9239e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 422464/500000 [43:38<1:03:22, 20.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600 steps; 422400 examples so far; train loss: 0.55, valid loss: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 435264/500000 [44:57<53:43, 20.08it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6800 steps; 435200 examples so far; train loss: 0.56, valid loss: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 435968/500000 [45:01<06:39, 160.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06812: reducing learning rate of group 0 to 4.4315e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 448064/500000 [46:15<39:18, 22.02it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 steps; 448000 examples so far; train loss: 0.55, valid loss: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 448832/500000 [46:20<05:15, 162.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07013: reducing learning rate of group 0 to 3.9883e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 460864/500000 [47:35<32:13, 20.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200 steps; 460800 examples so far; train loss: 0.56, valid loss: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 461696/500000 [47:40<03:50, 166.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07214: reducing learning rate of group 0 to 3.5895e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 473664/500000 [48:55<21:46, 20.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7400 steps; 473600 examples so far; train loss: 0.55, valid loss: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 474560/500000 [48:59<02:23, 176.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07415: reducing learning rate of group 0 to 3.2305e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 486464/500000 [50:13<10:59, 20.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7600 steps; 486400 examples so far; train loss: 0.56, valid loss: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 487424/500000 [50:18<01:12, 172.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07616: reducing learning rate of group 0 to 3.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499264/500000 [51:32<00:35, 20.60it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7800 steps; 499200 examples so far; train loss: 0.55, valid loss: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [51:36<00:00, 161.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "max_examples = truncate_size\n",
    "eval_every_steps = 200\n",
    "lr = learning_rate\n",
    "use_amp = True\n",
    "\n",
    "# DataManager(train_dataloader, device=device, data_type=None)\n",
    "# DataManager(val_dataloader, device=device, data_type=None)\n",
    "\n",
    "train_loader = train_dataloader\n",
    "validation_loader = val_dataloader\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.9, min_lr=3e-5, patience=200, threshold=1e-2, verbose=True)\n",
    "scaler=GradScaler()\n",
    "\n",
    "accumulated_grad_batches_until_now = 0\n",
    "\n",
    "def train_step(source_tokens, source_mask, target_tokens, target_mask, original_source, original_target):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    with autocast(enabled=use_amp):\n",
    "        loss = model(input_ids = source_tokens.to(device), attention_mask = source_mask.to(device), decoder_attention_mask = target_mask.to(device), labels = target_tokens.to(device)).loss\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation_step(source_tokens, source_mask, target_tokens, target_mask, original_source, original_target):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with autocast(enabled=use_amp):\n",
    "            loss = model(input_ids = source_tokens.to(device), attention_mask = source_mask.to(device), decoder_attention_mask = target_mask.to(device), labels = target_tokens.to(device)).loss\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "best_validation_loss = 9999\n",
    "train_losses = []\n",
    "n_examples = 0\n",
    "step = 0\n",
    "pbar = tqdm(total=max_examples)\n",
    "while n_examples < max_examples:\n",
    "    for mini_batch in train_dataloader:\n",
    "        loss = train_step(*mini_batch)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        # LR scheduler\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        if step % eval_every_steps == 0:\n",
    "            train_loss = np.average(train_losses)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_loss = np.average([\n",
    "                    validation_step(*mini_batch)\n",
    "                    for mini_batch in val_dataloader])\n",
    "                # Checkpoint to best models found.\n",
    "                if best_validation_loss > valid_loss:\n",
    "                    # Update the new best perplexity.\n",
    "                    best_validation_loss = valid_loss\n",
    "                    model.eval()\n",
    "                    torch.save(model, \"best_model.pth\")\n",
    "\n",
    "            print(f'{step} steps; {n_examples} examples so far; train loss: {train_loss:.2f}, valid loss: {valid_loss:.2f}')\n",
    "            train_losses = []\n",
    "\n",
    "        n_examples += mini_batch[0].shape[0]  # Increment of batch size\n",
    "        step += 1\n",
    "        pbar.update(mini_batch[0].shape[0])\n",
    "        if n_examples >= max_examples:\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Restore best model (checkpoint) found\n",
    "model = torch.load(\"best_model.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Avaliando BLEU score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (0.10.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from torchmetrics) (1.21.5)\r\n",
      "Requirement already satisfied: packaging in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from torchmetrics) (21.3)\r\n",
      "Requirement already satisfied: torch>=1.3.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from torchmetrics) (1.12.1+cu113)\r\n",
      "Requirement already satisfied: typing-extensions in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.1.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics\n",
    "\n",
    "# Rotina de avaliação inspirada no notebook de Bruno da Silvia\n",
    "from torchmetrics import SacreBLEUScore\n",
    "\n",
    "def evaluate_bleu_score(model, target_dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    pred_translations, targets = [], []\n",
    "\n",
    "    for i, batch in tqdm(enumerate(target_dataloader), total=len(target_dataloader)):\n",
    "        inputs = batch[0]\n",
    "        inputs_mask = batch[1]\n",
    "        targets += [[i] for i in batch[-1]]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = model.generate(input_ids=inputs.to(device), attention_mask=inputs_mask.to(device), max_length=target_max_length)\n",
    "            pred_translations += tokenizer.batch_decode(model_output, skip_special_tokens=True)\n",
    "\n",
    "    metric = SacreBLEUScore()\n",
    "    return metric(pred_translations, targets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [10:17<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final BLEU score on test: 26.85\n"
     ]
    }
   ],
   "source": [
    "# Restore best model (checkpoint) found\n",
    "model = torch.load(\"best_model.pth\")\n",
    "model.eval()\n",
    "\n",
    "bleu = evaluate_bleu_score(model, test_dataloader)\n",
    "print(f'\\nFinal BLEU score on test: {bleu.item()*100:.2f}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Traduzindo alguns exemplos\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "To have your own server, a company or professional, you may find different options, from buying the physical server, or hardware equipment, to hire a dedicated server on the Internet, through the possibility of hiring a VPS or even a Reseller service, depending on the actual needs that have or will have in the future. It also depends on the potential knowledge, professional or employee of the company, on the administration server.\n",
      "\tPortuguese Target: Para ter seu próprio servidor, uma empresa ou um profissional, você pode encontrar opções diferentes, desde a compra de servidores físicos, ou equipamento de hardware, para contratar os serviços de um servidor dedicado na Internet, através da possibilidade de contratar um VPS ou até mesmo um serviço de Revenda, dependendo das necessidades reais tem ou terá no futuro. Também depende do conhecimento que possa ter, ou funcionário profissional da empresa, sobre a administração de servidores.\n",
      "\tPortuguese Output: Para ter o seu próprio servidor, uma empresa ou profissional, você pode encontrar diferentes opções, de comprar o servidor físico ou equipamento de hardware, para ter um servidor dedicado na Internet, através da possibilidade de ter um VPS ou mesmo um serviço Reseller, dependendo das necessidades atuais que têm ou existirão no futuro. Também depende do conhecimento potencial\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Therefore, cars must become more economical and more environmentally sustainable.\n",
      "\tPortuguese Target: Portanto, os carros devem tornar-se mais econômicos e mais sustentáveis ambientalmente.\n",
      "\tPortuguese Output: Portanto, os carros devem tornar-se mais econômicos e mais sustentável.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Main image belongs to Thomas Mc Gowan licensed under these terms.\n",
      "\tPortuguese Target: A imagem principal pertence a Thomas Mc Gowan com este license.\n",
      "\tPortuguese Output: Imagem de imagem de Thomas Mc Gowan licensed sob estes termos.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Status: A fix is already committed for this issue and it will be fixed in 8.1\n",
      "\tPortuguese Target: Situação: Uma correção já foi submetida para esse problema e ele será corrigido na 8.1\n",
      "\tPortuguese Output: Status: Um ficheiro já está confirmado para esta questão e será ficheiro em 8.1\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "The Government and PT Compras sign a protocol\n",
      "\tPortuguese Target: Governo e PT Compras assinam protocolo\n",
      "\tPortuguese Output: O governo e as Compras de PT enviam um documento\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rotina de geração de sentenças inspirada no notebook de Mateus Lindino\n",
    "\n",
    "model.eval()\n",
    "randomlist = random.sample(range(0, len(dataset_test)), 5)\n",
    "\n",
    "for i in randomlist:\n",
    "    item           = dataset_test[i]\n",
    "    input_ids      = item[0]\n",
    "    attention_mask = item[1]\n",
    "    sample_en      = item[-2]\n",
    "    sample_pt      = item[-1]\n",
    "\n",
    "    pred = model.generate(input_ids=input_ids.reshape(1, -1).to(device), attention_mask=attention_mask.reshape(1, -1).to(device), max_length=target_max_length)[0]\n",
    "    pred = tokenizer.decode(pred, skip_special_tokens=True)\n",
    "\n",
    "    print('-'*200)\n",
    "    print(f'{sample_en}\\n\\tPortuguese Target: {sample_pt}\\n\\tPortuguese Output: {pred}\\n\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}