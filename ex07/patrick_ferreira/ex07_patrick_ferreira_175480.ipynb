{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "<a href=\"https://colab.research.google.com/github/patrickctrf/IA024_2022S2/blob/main/ex07/patrick_ferreira/ex07_patrick_ferreira_175480.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OG5DT_dm6mk"
   },
   "source": [
    "# Notebook de referência \n",
    "\n",
    "Nome: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od7iUgHy5SSi"
   },
   "source": [
    "## Instruções\n",
    "\n",
    "Neste colab iremos treinar um modelo T5 para traduzir de inglês para português. Iremos treiná-lo com o data Paracrawl.\n",
    "\n",
    "- Usaremos o dataset Paracrawl Inglês-Português. Truncamos o dataset de treino para apenas 100k pares para deixar o treinamento mais rápido. Quem quiser pode treinar com mais amostras. Se demorar muito para treinar, truncar o dataset ainda mais.\n",
    "\n",
    "- Usaremos o BLEU como métrica. Usaremos o SacreBLEU pois sempre faz o mesmo pré-processamento (tokenização, lowercase). Não usaremos torchnlp.metrics.bleu, torchtext.data.metrics.bleu_score, etc. SacreBLEU é lento: usar poucas amostras de validação (ex: 5k)\n",
    "\n",
    "\n",
    "Usaremos o modelo PTT5 disponível no model hub da HuggingFace:\n",
    "\n",
    "https://huggingface.co/unicamp-dl/ptt5-small-portuguese-vocab\n",
    "\n",
    "Este é  um T5 pré-treinado em textos em português e com tokenizador em português.\n",
    "\n",
    "É recomendável salvar os pesos do modelo e estado dos otimizadores, pois o treinamento é longo.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FgW-boJLU0wU"
   },
   "source": [
    "# Configurações gerais\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"unicamp-dl/ptt5-small-portuguese-vocab\"\n",
    "batch_size = 64\n",
    "accumulate_grad_batches = 2\n",
    "source_max_length = 128\n",
    "target_max_length = 128\n",
    "learning_rate = 5e-4"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0mXaMmG4cb-F"
   },
   "source": [
    "! pip install sacrebleu\n",
    "! pip install transformers\n",
    "! pip install sentencepiece"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (2.2.1)\r\n",
      "Requirement already satisfied: colorama in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from sacrebleu) (0.4.4)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from sacrebleu) (0.8.9)\r\n",
      "Requirement already satisfied: portalocker in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from sacrebleu) (2.5.1)\r\n",
      "Requirement already satisfied: regex in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from sacrebleu) (2022.3.15)\r\n",
      "Requirement already satisfied: lxml in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from sacrebleu) (4.8.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from sacrebleu) (1.21.5)\r\n",
      "Requirement already satisfied: transformers in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (4.23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (0.10.1)\r\n",
      "Requirement already satisfied: filelock in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\r\n",
      "Requirement already satisfied: requests in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\r\n",
      "Requirement already satisfied: sentencepiece in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (0.1.97)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ob7qL6kUVjbu"
   },
   "source": [
    "# Importar todos os pacotes de uma só vez para evitar duplicados ao longo do notebook.\n",
    "import gzip\n",
    "import os\n",
    "import random\n",
    "import sacrebleu\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from google.colab import drive\n",
    "\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import T5Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Tuple"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bJlZDb1VY29r"
   },
   "source": [
    "# Important: Fix seeds so we can replicate results\n",
    "seed = 123\n",
    "random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print('Using {}'.format(device))"
   ],
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETfkvMGl4JA1"
   },
   "source": [
    "Iremos salvar os checkpoints (pesos do modelo) no google drive, para que possamos continuar o treino de onde paramos."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J-Co8U6O4Gl3"
   },
   "source": [
    "# drive.mount('/content/drive')"
   ],
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXFdJz2KVeQw"
   },
   "source": [
    "## Preparando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHMi_Kq65fPM"
   },
   "source": [
    "Primeiro, fazemos download do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2wbnfzst5O3k"
   },
   "source": [
    "! wget -nc https://storage.googleapis.com/unicamp-dl/ia024a_2022s2/paracrawl_enpt_train.tsv.gz\n",
    "! wget -nc https://storage.googleapis.com/unicamp-dl/ia024a_2022s2/paracrawl_enpt_test.tsv.gz"
   ],
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘paracrawl_enpt_train.tsv.gz’ already there; not retrieving.\r\n",
      "\r\n",
      "File ‘paracrawl_enpt_test.tsv.gz’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Giyi5Rv_NIm"
   },
   "source": [
    "## Carregando o dataset\n",
    "\n",
    "Criaremos uma divisão de treino (100k pares) e val (5k pares) artificialmente.\n",
    "\n",
    "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0HIN_xLI_TuT"
   },
   "source": [
    "def load_text_pairs(path):\n",
    "    text_pairs = []\n",
    "    for line in gzip.open(path, mode='rt'):\n",
    "        text_pairs.append(line.strip().split('\\t'))\n",
    "    return text_pairs\n",
    "\n",
    "x_train = load_text_pairs('paracrawl_enpt_train.tsv.gz')\n",
    "x_test = load_text_pairs('paracrawl_enpt_test.tsv.gz')\n",
    "\n",
    "# Embaralhamos o treino para depois fazermos a divisão treino/val.\n",
    "random.shuffle(x_train)\n",
    "\n",
    "# Truncamos o dataset para 100k pares de treino e 5k pares de validação.\n",
    "x_val = x_train[100000:105000]\n",
    "x_train = x_train[:100000]\n",
    "\n",
    "for set_name, x in [('treino', x_train), ('validação', x_val), ('test', x_test)]:\n",
    "    print(f'\\n{len(x)} amostras de {set_name}')\n",
    "    print(f'3 primeiras amostras {set_name}:')\n",
    "    for i, (source, target) in enumerate(x[:3]):\n",
    "        print(f'{i}: source: {source}\\n   target: {target}')"
   ],
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100000 amostras de treino\n",
      "3 primeiras amostras treino:\n",
      "0: source: More Croatian words and phrases\n",
      "   target: Mais palavras e frases em croata\n",
      "1: source: Jerseys and pullovers, containing at least 50Â % by weight of wool and weighing 600Â g or more per article 6110 11 10 (PCE)\n",
      "   target: Camisolas e pulôveres, com pelo menos 50 %, em peso, de lã e pesando 600g ou mais por unidade 6110 11 10 (PCE)\n",
      "2: source: Atex Colombia SAS makes available its lead product, 100% natural liquid latex, excellent quality and price. ... Welding manizales caldas Colombia a DuckDuckGo\n",
      "   target: Atex Colômbia SAS torna principal produto está disponível, látex líquido 100% natural, excelente qualidade e preço. ...\n",
      "\n",
      "5000 amostras de validação\n",
      "3 primeiras amostras validação:\n",
      "0: source: «You have hidden these things from the wise and the learned you have revealed them to the childlike»\n",
      "   target: «Escondeste estas coisas aos sábios e entendidos e as revelaste aos pequenos»\n",
      "1: source: Repair of computers, application programming, network installations, web design, graphic design, and also the most. Computer consulting in Santa Lucía\n",
      "   target: Reparação de computadores, programação de aplicações, instalações de rede, web design, design gráfico, e também a.\n",
      "2: source: He was born in Fafe (Braga) and he graduated in Law in Coimbra University.\n",
      "   target: É natural de Fafe (Braga) e Licenciado em Direito pela Universidade de Coimbra.\n",
      "\n",
      "20000 amostras de test\n",
      "3 primeiras amostras test:\n",
      "0: source: In this way, the civil life of a nation matures, making it possible for all citizens to enjoy the fruits of genuine tolerance and mutual respect.\n",
      "   target: Deste modo, a vida civil de uma nação amadurece, fazendo com que todos os cidadãos gozem dos frutos da tolerância genuína e do respeito mútuo.\n",
      "1: source: 1999 XIII. Winnipeg, Canada July 23 to August 8\n",
      "   target: 1999 XIII. Winnipeg, Canadá 23 de julho a 8 de agosto\n",
      "2: source: In the mystery of Christmas, Christ's light shines on the earth, spreading, as it were, in concentric circles.\n",
      "   target: No mistério do Natal, a luz de Cristo irradia-se sobre a terra, difundindo-se como círculos concêntricos.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXnoYK5YXKgk"
   },
   "source": [
    "Criando Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pLrftKzSPBs_"
   },
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ],
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OMen-JFKLFCb"
   },
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, text_pairs: List[Tuple[str]], tokenizer,\n",
    "                 source_max_length: int = 32, target_max_length: int = 32):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_pairs = text_pairs\n",
    "        self.source_max_length = source_max_length\n",
    "        self.target_max_length = target_max_length\n",
    "\n",
    "        sources, targets = list(zip(*text_pairs))\n",
    "\n",
    "        self.sources_tokenizadas = tokenizer(sources, padding=True, truncation=True, max_length=self.source_max_length, return_tensors = \"pt\")\n",
    "        self.targets_tokenizadas = tokenizer(targets, padding=True, truncation=True, max_length=self.source_max_length, return_tensors = \"pt\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source, target = self.text_pairs[idx]\n",
    "        # TODO: tokenizar texto\n",
    "\n",
    "        source_token_ids =  self.sources_tokenizadas.input_ids[idx]\n",
    "        source_mask =       self.sources_tokenizadas.attention_mask[idx]\n",
    "        target_token_ids =  self.targets_tokenizadas.input_ids[idx]\n",
    "        target_mask =       self.targets_tokenizadas.attention_mask[idx]\n",
    "\n",
    "        return source_token_ids, source_mask, target_token_ids, target_mask, source, target"
   ],
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cloyt0tIwIiD"
   },
   "source": [
    "## Testando o DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZoKiQXCvwGrP"
   },
   "source": [
    "text_pairs = [('we like pizza', 'eu gosto de pizza')]\n",
    "dataset_debug = MyDataset(\n",
    "    text_pairs=text_pairs,\n",
    "    tokenizer=tokenizer,\n",
    "    source_max_length=source_max_length,\n",
    "    target_max_length=target_max_length)\n",
    "\n",
    "dataloader_debug = DataLoader(dataset_debug, batch_size=10, shuffle=True, \n",
    "                              num_workers=0)\n",
    "\n",
    "source_token_ids, source_mask, target_token_ids, target_mask, _, _ = next(iter(dataloader_debug))\n",
    "print('source_token_ids:\\n', source_token_ids)\n",
    "print('source_mask:\\n', source_mask)\n",
    "print('target_token_ids:\\n', target_token_ids)\n",
    "print('target_mask:\\n', target_mask)\n",
    "\n",
    "print('source_token_ids.shape:', source_token_ids.shape)\n",
    "print('source_mask.shape:', source_mask.shape)\n",
    "print('target_token_ids.shape:', target_token_ids.shape)\n",
    "print('target_mask.shape:', target_mask.shape)"
   ],
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_token_ids:\n",
      " tensor([[  31, 1528, 1079,  634, 1241, 7531,    1]])\n",
      "source_mask:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1]])\n",
      "target_token_ids:\n",
      " tensor([[2077, 6618,    4, 1241, 7531,    1]])\n",
      "target_mask:\n",
      " tensor([[1, 1, 1, 1, 1, 1]])\n",
      "source_token_ids.shape: torch.Size([1, 7])\n",
      "source_mask.shape: torch.Size([1, 7])\n",
      "target_token_ids.shape: torch.Size([1, 6])\n",
      "target_mask.shape: torch.Size([1, 6])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBptbyTvXBhC"
   },
   "source": [
    "## Criando DataLoaders de Treino/Val/Test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i2_Fcs0VXD5W"
   },
   "source": [
    "dataset_train = MyDataset(text_pairs=x_train,\n",
    "                          tokenizer=tokenizer,\n",
    "                          source_max_length=source_max_length,\n",
    "                          target_max_length=target_max_length)\n",
    "\n",
    "dataset_val = MyDataset(text_pairs=x_val,\n",
    "                        tokenizer=tokenizer,\n",
    "                        source_max_length=source_max_length,\n",
    "                        target_max_length=target_max_length)\n",
    "\n",
    "dataset_test = MyDataset(text_pairs=x_test,\n",
    "                         tokenizer=tokenizer,\n",
    "                         source_max_length=source_max_length,\n",
    "                         target_max_length=target_max_length)\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=batch_size,\n",
    "                              shuffle=True, num_workers=4)\n",
    "\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, \n",
    "                            num_workers=4)\n",
    "\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=batch_size,\n",
    "                             shuffle=False, num_workers=4)"
   ],
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utilidade para converter dados de device em paralelo\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TREINAMENTO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [01:15<?, ?it/s]\n",
      "  0%|          | 64/100000 [00:09<4:19:08,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps; 0 examples so far; train loss: 23.27, valid loss: 30.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64064/100000 [06:10<30:28, 19.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 steps; 64000 examples so far; train loss: 0.82, valid loss: 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [09:25<00:00, 176.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "max_examples = 100_000\n",
    "eval_every_steps = 200\n",
    "lr = learning_rate\n",
    "use_amp = True\n",
    "\n",
    "# DataManager(train_dataloader, device=device, data_type=None)\n",
    "# DataManager(val_dataloader, device=device, data_type=None)\n",
    "\n",
    "train_loader = train_dataloader\n",
    "validation_loader = val_dataloader\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.9, min_lr=3e-5, patience=15000, threshold=1e-1, verbose=True)\n",
    "scaler=GradScaler()\n",
    "\n",
    "accumulated_grad_batches_until_now = 0\n",
    "\n",
    "def train_step(source_tokens, source_mask, target_tokens, target_mask, original_source, original_target):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    with autocast(enabled=use_amp):\n",
    "        loss = model(input_ids = source_tokens.to(device), attention_mask = source_mask.to(device), decoder_attention_mask = target_mask.to(device), labels = target_tokens.to(device)).loss\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation_step(source_tokens, source_mask, target_tokens, target_mask, original_source, original_target):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with autocast(enabled=use_amp):\n",
    "            loss = model(input_ids = source_tokens.to(device), attention_mask = source_mask.to(device), decoder_attention_mask = target_mask.to(device), labels = target_tokens.to(device)).loss\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "best_validation_loss = 9999\n",
    "train_losses = []\n",
    "n_examples = 0\n",
    "step = 0\n",
    "pbar = tqdm(total=max_examples)\n",
    "while n_examples < max_examples:\n",
    "    for mini_batch in train_dataloader:\n",
    "        loss = train_step(*mini_batch)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        # LR scheduler\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        if step % eval_every_steps == 0:\n",
    "            train_loss = np.average(train_losses)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_loss = np.average([\n",
    "                    validation_step(*mini_batch)\n",
    "                    for mini_batch in val_dataloader])\n",
    "                # Checkpoint to best models found.\n",
    "                if best_validation_loss > valid_loss:\n",
    "                    # Update the new best perplexity.\n",
    "                    best_validation_loss = valid_loss\n",
    "                    model.eval()\n",
    "                    torch.save(model, \"best_model.pth\")\n",
    "\n",
    "            print(f'{step} steps; {n_examples} examples so far; train loss: {train_loss:.2f}, valid loss: {valid_loss:.2f}')\n",
    "            train_losses = []\n",
    "\n",
    "        n_examples += mini_batch[0].shape[0]  # Increment of batch size\n",
    "        step += 1\n",
    "        pbar.update(mini_batch[0].shape[0])\n",
    "        if n_examples >= max_examples:\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Restore best model (checkpoint) found\n",
    "model = torch.load(\"best_model.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Avaliando BLEU score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\r\n",
      "  Downloading torchmetrics-0.10.0-py3-none-any.whl (529 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 529 kB 654 kB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17.2 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from torchmetrics) (1.21.5)\r\n",
      "Requirement already satisfied: packaging in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from torchmetrics) (21.3)\r\n",
      "Requirement already satisfied: torch>=1.3.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from torchmetrics) (1.12.1+cu113)\r\n",
      "Requirement already satisfied: typing-extensions in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.1.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.4)\r\n",
      "Installing collected packages: torchmetrics\r\n",
      "Successfully installed torchmetrics-0.10.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics\n",
    "\n",
    "# Rotina de avaliação inspirada no notebook de Bruno da Silvia\n",
    "from torchmetrics import SacreBLEUScore\n",
    "\n",
    "def evaluate_bleu_score(model, target_dataloader):\n",
    "\n",
    "    pred_translations, targets = [], []\n",
    "\n",
    "    for i, batch in tqdm(enumerate(target_dataloader), total=len(target_dataloader)):\n",
    "        inputs = batch[0]\n",
    "        inputs_mask = batch[1]\n",
    "        targets += [[i] for i in batch[-1]]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = model.generate(input_ids=inputs.to(device), attention_mask=inputs_mask.to(device), max_length=target_max_length)\n",
    "            pred_translations += tokenizer.batch_decode(model_output, skip_special_tokens=True)\n",
    "\n",
    "    metric = SacreBLEUScore()\n",
    "    return metric(pred_translations, targets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [11:03<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final BLEU score on test: 19.21\n"
     ]
    }
   ],
   "source": [
    "# Restore best model (checkpoint) found\n",
    "model = torch.load(\"best_model.pth\")\n",
    "\n",
    "bleu = evaluate_bleu_score(model, test_dataloader)\n",
    "print(f'\\nFinal BLEU score on test: {bleu.item()*100:.2f}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Traduzindo alguns exemplos\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "To have your own server, a company or professional, you may find different options, from buying the physical server, or hardware equipment, to hire a dedicated server on the Internet, through the possibility of hiring a VPS or even a Reseller service, depending on the actual needs that have or will have in the future. It also depends on the potential knowledge, professional or employee of the company, on the administration server.\n",
      "\tPortuguese Target: Para ter seu próprio servidor, uma empresa ou um profissional, você pode encontrar opções diferentes, desde a compra de servidores físicos, ou equipamento de hardware, para contratar os serviços de um servidor dedicado na Internet, através da possibilidade de contratar um VPS ou até mesmo um serviço de Revenda, dependendo das necessidades reais tem ou terá no futuro. Também depende do conhecimento que possa ter, ou funcionário profissional da empresa, sobre a administração de servidores.\n",
      "\tPortuguese Output: Para ter seu próprio escritório, uma empresa ou profissional, você pode encontrar diversas opções, de comprar o serviço técnico ou hardware, para ter um serviço dedicado na Internet, através da possibilidade de ter um serviço VPS ou até mesmo um serviço Reseller, independentemente das necessidades actualizadas que tiver ou terem no país. Também depende do conhecimento técnico que você tem no mercado.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Therefore, cars must become more economical and more environmentally sustainable.\n",
      "\tPortuguese Target: Portanto, os carros devem tornar-se mais econômicos e mais sustentáveis ambientalmente.\n",
      "\tPortuguese Output: Antes, os carros devem ser mais econômicos e mais seguros.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Main image belongs to Thomas Mc Gowan licensed under these terms.\n",
      "\tPortuguese Target: A imagem principal pertence a Thomas Mc Gowan com este license.\n",
      "\tPortuguese Output: Imagem de imagem de Thomas Mc Gowan, entre estes termos.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Status: A fix is already committed for this issue and it will be fixed in 8.1\n",
      "\tPortuguese Target: Situação: Uma correção já foi submetida para esse problema e ele será corrigido na 8.1\n",
      "\tPortuguese Output: Status: Um ficheiro é sempre relacionado com esta mensagem e será ficheiro em 8.1\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "The Government and PT Compras sign a protocol\n",
      "\tPortuguese Target: Governo e PT Compras assinam protocolo\n",
      "\tPortuguese Output: O governo e o PT Compras deram um protocol\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rotina de geração de sentenças inspirada no notebook de Mateus Lindino\n",
    "\n",
    "import random\n",
    "randomlist = random.sample(range(0, len(dataset_test)), 5)\n",
    "\n",
    "for i in randomlist:\n",
    "    item           = dataset_test[i]\n",
    "    input_ids      = item[0].to(device)\n",
    "    attention_mask = item[1].to(device)\n",
    "    sample_en      = item[-2]\n",
    "    sample_pt      = item[-1]\n",
    "\n",
    "    pred = model.generate(input_ids=input_ids.reshape(1, -1), attention_mask=attention_mask.reshape(1, -1), max_length=target_max_length)[0]\n",
    "    pred = tokenizer.decode(pred, skip_special_tokens=True)\n",
    "\n",
    "    print('-'*200)\n",
    "    print(f'{sample_en}\\n\\tPortuguese Target: {sample_pt}\\n\\tPortuguese Output: {pred}\\n\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}