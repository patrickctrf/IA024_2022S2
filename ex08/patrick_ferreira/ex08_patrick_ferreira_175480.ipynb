{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "pycharm-79e5302d",
   "language": "python",
   "display_name": "PyCharm (IA024_2022S2)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/patrickctrf/IA024_2022S2/blob/main/ex08/patrick_ferreira/ex08_patrick_ferreira_175480.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu nome é Patrick de Carvalho Tavares Rezende Ferreira\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nome = \"Patrick de Carvalho Tavares Rezende Ferreira\"\n",
    "print(f'Meu nome é {nome}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Exercício: Modelo de Linguagem com auto-atenção (versão eficiente)\n",
    "\n",
    "Este exercício é similar ao da aula 5, mas iremos agora treinar *eficientemente* uma rede neural com uma ou mais camadas de auto-atenção para prever a próxima palavra de um texto, data as palavras anteriores como entrada. \n",
    "\n",
    "Para tanto, deve-se implementar:\n",
    "1. A máscara causal de atenção. Ela possibilitará que, durante o treinamento, com apenas uma forward+backward pass na rede, tenhamos as losses para todos os tokens de entrada (slide 117).\n",
    "2. A máscara de PADs, que permite que usemos sequencias de comprimento variável no mesmo batch (slide 118).\n",
    "3. Múltiplas cabeças."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importação dos pacotes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from typing import List"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 15 06:05:34 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.129.06   Driver Version: 470.129.06   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   60C    P5    N/A /  N/A |    722MiB /  2004MiB |     17%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1054      G   /usr/lib/xorg/Xorg                 20MiB |\r\n",
      "|    0   N/A  N/A      1681      G   /usr/lib/xorg/Xorg                 57MiB |\r\n",
      "|    0   N/A  N/A      1864      G   /usr/bin/gnome-shell               67MiB |\r\n",
      "|    0   N/A  N/A      2404      G   ...415695869927615982,131072       24MiB |\r\n",
      "|    0   N/A  N/A      4080      G   ...f_3364.log --shared-files       18MiB |\r\n",
      "|    0   N/A  N/A      4412      C   ...da3/envs/ia025/bin/python      520MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Check which GPU we are using\n",
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "   dev = \"cuda:0\"\n",
    "else:\n",
    "   dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print('Using {}'.format(device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Carregamento do dataset \n",
    "\n",
    "Primeiro, fazemos download do dataset:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘aclImdb.tgz’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc http://files.fast.ai/data/aclImdb.tgz\n",
    "!tar -xzf aclImdb.tgz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ptk-patrickctrf in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (0.1.2)\r\n",
      "Requirement already satisfied: torch in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from ptk-patrickctrf) (1.8.1)\r\n",
      "Requirement already satisfied: tensorflow in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from ptk-patrickctrf) (2.2.0)\r\n",
      "Requirement already satisfied: numpy in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from ptk-patrickctrf) (1.19.1)\r\n",
      "Requirement already satisfied: typing_extensions in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from torch->ptk-patrickctrf) (3.7.4.3)\r\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (2.2.1)\r\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (1.4.1)\r\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (3.13.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (3.1.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (2.2.0)\r\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (1.31.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (1.11.2)\r\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (1.6.3)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (0.2.0)\r\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (0.35.1)\r\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (2.10.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (1.1.0)\r\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (0.10.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (1.15.0)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (1.1.0)\r\n",
      "Requirement already satisfied: gast==0.3.3 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (0.3.3)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (1.21.2)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (49.6.0.post20200814)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (2.24.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (3.2.2)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (1.6.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (0.4.1)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (1.0.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (4.6)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (0.2.8)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (4.1.1)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (1.25.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (2020.6.20)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (2.10)\r\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (1.7.0)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (1.3.0)\r\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (0.4.8)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (3.1.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (3.1.0)\r\n",
      "Requirement already satisfied: transformers in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (4.19.2)\r\n",
      "Requirement already satisfied: requests in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (2.24.0)\r\n",
      "Requirement already satisfied: filelock in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (3.0.12)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (4.48.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (2020.7.14)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (20.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (1.19.1)\r\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (1.7.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (0.12.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (0.6.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (5.3.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests->transformers) (1.25.10)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests->transformers) (2.10)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.7)\r\n",
      "Requirement already satisfied: six in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from packaging>=20.0->transformers) (1.15.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ptk-patrickctrf\n",
    "!pip install transformers\n",
    "\n",
    "from operator import itemgetter\n",
    "from transformers import BertTokenizer\n",
    "from ptk.utils import DataManager"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Carregando o dataset\n",
    "\n",
    "Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n",
    "\n",
    "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 amostras de treino.\n",
      "5000 amostras de desenvolvimento.\n",
      "25000 amostras de teste.\n",
      "3 primeiras amostras treino:\n",
      "Here it is.. the first EVER episode of Friends, Where we get introduced to Control Freak Monica Gell\n",
      "One of the last classics of the French New Wave. For direction, cineaste Jean Eustache drew from the\n",
      "After seeing \"Driven\" on a plane flight to America 3 years ago I truly believed I had seen the worst\n",
      "3 últimas amostras treino:\n",
      "Not very impressed. Its difficult to offer any spoilers to this film, because there is almost no dev\n",
      "I saw this film recently in a film festival. It's the romance of an ex-alcoholic unemployed man who \n",
      "Was'nt really bad for Raw's first PPV of 006. But the ending was really really shocking to everyone \n",
      "3 primeiras amostras validação:\n",
      "White Fire has so much going for it. With Larry Bird look-alike Robert Ginty leading the charge blaz\n",
      "There are several things wrong with this movie- Brenda Song's character being one of them. I do not \n",
      "WRITTEN ON THE WIND, directed by Douglas Sirk and released in 1956, is like all of Sirk's mid 50's f\n",
      "3 últimas amostras validação:\n",
      "First off, I dislike almost all Neil Simon movies. But there is something about this that is unique,\n",
      "In my opinion, October Sky is one of the best movies of 1999...It totally has everything an emotiona\n",
      "i two came home from school fast as i could to catch HRpuff and stuff on t.v. that was the most fun \n"
     ]
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f1cf0172e90>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_texts(folder):\n",
    "    texts = []\n",
    "    for path in os.listdir(folder):\n",
    "        with open(os.path.join(folder, path)) as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "x_train_pos = load_texts('aclImdb/train/pos')\n",
    "x_train_neg = load_texts('aclImdb/train/neg')\n",
    "x_test_pos = load_texts('aclImdb/test/pos')\n",
    "x_test_neg = load_texts('aclImdb/test/neg')\n",
    "\n",
    "x_train = x_train_pos + x_train_neg\n",
    "x_test = x_test_pos + x_test_neg\n",
    "\n",
    "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
    "random.shuffle(x_train)\n",
    "\n",
    "n_train = int(0.8 * len(x_train))\n",
    "\n",
    "x_valid = x_train[n_train:]\n",
    "x_train = x_train[:n_train]\n",
    "\n",
    "print(len(x_train), 'amostras de treino.')\n",
    "print(len(x_valid), 'amostras de desenvolvimento.')\n",
    "print(len(x_test), 'amostras de teste.')\n",
    "\n",
    "print('3 primeiras amostras treino:')\n",
    "for x in x_train[:3]:\n",
    "    print(x[:100])\n",
    "\n",
    "print('3 últimas amostras treino:')\n",
    "for x in x_train[-3:]:\n",
    "    print(x[:100])\n",
    "\n",
    "print('3 primeiras amostras validação:')\n",
    "for x in x_valid[:3]:\n",
    "    print(x[:100])\n",
    "\n",
    "print('3 últimas amostras validação:')\n",
    "for x in x_valid[-3:]:\n",
    "    print(x[:100])\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "### Criando classe do dataset\n",
    "\n",
    "class MyDataset():\n",
    "    def __init__(self, texts: List[str], tokenizer, context_size: int, vocab_size=1000):\n",
    "        self.context_size = context_size\n",
    "        try:\n",
    "            self.x = np.load(\"x_\" + str(len(texts)) + \".npy\", mmap_mode=\"r\", allow_pickle=True)\n",
    "            self.y = np.load(\"y_\" + str(len(texts)) + \".npy\", mmap_mode=\"r\", allow_pickle=True)\n",
    "            print(\"Carregando dataset preprocessado\")\n",
    "        except Exception as e:\n",
    "            # print(\"Excecao: \", e)\n",
    "            print(\"Montando dataset\")\n",
    "\n",
    "            self.x = list()\n",
    "            self.y = list()\n",
    "\n",
    "            for text in tqdm(texts):\n",
    "                tokens_key = tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
    "                for i in range(0, len(tokens_key)-context_size-1, context_size):\n",
    "                    self.x.append(tokens_key[i:i+context_size])\n",
    "                    self.y.append(tokens_key[i+1:i+context_size+1])\n",
    "\n",
    "            self.x = np.array(self.x)\n",
    "            self.y = np.array(self.y)\n",
    "\n",
    "            np.save(\"x_\" + str(len(texts)) + \".npy\", self.x)\n",
    "            np.save(\"y_\" + str(len(texts)) + \".npy\", self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.x[idx]).long(), torch.tensor(self.y[idx]).long(), torch.tensor([False] * self.context_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testando Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dataset preprocessado\n",
      "Passou no assert de tamanho do dataset\n",
      "Passou no assert de input\n",
      "Passou no assert de target\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
    "\n",
    "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, context_size=3, vocab_size=tokenizer.vocab_size)\n",
    "dummy_loader = DataLoader(dummy_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "assert len(dummy_dataset) == 3\n",
    "print('Passou no assert de tamanho do dataset')\n",
    "\n",
    "first_batch_input, first_batch_target, attention_mask = next(iter(dummy_loader))\n",
    "\n",
    "correct_first_batch_input = torch.LongTensor(\n",
    "    [[7327, 2175, 16033],\n",
    "     [3449, 2050, 2175]])\n",
    "\n",
    "correct_first_batch_target = torch.LongTensor(\n",
    "    [[2175, 16033, 2139],\n",
    "     [2050, 2175, 9153]])\n",
    "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
    "print('Passou no assert de input')\n",
    "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
    "print('Passou no assert de target')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dados de treino, validação e teste"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dataset preprocessado\n",
      "Carregando dataset preprocessado\n",
      "Carregando dataset preprocessado\n",
      "training examples: 680149\n",
      "valid examples: 169482\n",
      "test examples: 829937\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "context_size = 9\n",
    "\n",
    "# tokenizer = Tokenizador(x_train, tokenize, vocab_size=3000)\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "training_dataset = MyDataset(texts=x_train, tokenizer=tokenizer, context_size=context_size)\n",
    "valid_dataset = MyDataset(texts=x_valid, tokenizer=tokenizer, context_size=context_size)\n",
    "test_dataset = MyDataset(texts=x_test, tokenizer=tokenizer, context_size=context_size)\n",
    "\n",
    "print(f'training examples: {len(training_dataset)}')\n",
    "print(f'valid examples: {len(valid_dataset)}')\n",
    "print(f'test examples: {len(test_dataset)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "class MySequential(nn.Sequential):\n",
    "    def forward(self, *input):\n",
    "        for module in self._modules.values():\n",
    "            input = module(*input)\n",
    "        return input\n",
    "\n",
    "class _AttentionLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim: int, max_seq_length: int, ):\n",
    "        \"\"\"\n",
    "        Implements the Self-attention, decoder-only.\"\n",
    "\n",
    "        Args:\n",
    "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
    "            embedding_dim (int): Dimension of the embedding layer for each word in the context.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.context_size = max_seq_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Linear projections\n",
    "        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        # cast to probabilities\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Matriz triangular de mascara, convertida para Booleano\n",
    "        # Onde vale 1, o valor deve ser substituida por um valor negativo alto no tensor de scores.\n",
    "        self.causal_mask = torch.ones((max_seq_length, max_seq_length), device=device).triu(diagonal=1) == 1.0\n",
    "\n",
    "    def forward(self, x_embeddings, attention_mask):\n",
    "\n",
    "        k = self.w_k(x_embeddings)\n",
    "        v = self.w_v(x_embeddings)\n",
    "        q = self.w_q(x_embeddings)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(1, 2))\n",
    "\n",
    "        # Onde a mascara vale 1, retornamos um valor negativo grande.\n",
    "        # Onde a mascara vale zero, mantemos intacto.\n",
    "        probabilities = self.softmax(scores.masked_fill(self.causal_mask, -1e4).masked_fill(attention_mask.repeat_interleave(scores.shape[0] // attention_mask.shape[0],0).unsqueeze(-1), -1e4))\n",
    "\n",
    "        return torch.matmul(probabilities, v)\n",
    "\n",
    "class MultiHeadAttentionLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim: int, max_seq_length: int, n_heads: int):\n",
    "        \"\"\"\n",
    "        Implements the Self-attention, decoder-only.\"\n",
    "\n",
    "        Args:\n",
    "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
    "            embedding_dim (int): Dimension of the embedding layer for each word in the context.\n",
    "            n_heads (int): Number of self attention heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.context_size = max_seq_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_dim = embedding_dim // n_heads\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert embedding_dim % n_heads == 0, \"MultiHeadAttentionLayer Error: Embedding_dim must be an integer multiple of n_heads. \"\n",
    "\n",
    "        self.heads = _AttentionLayer(embedding_dim=self.head_dim, max_seq_length=max_seq_length)\n",
    "        self.w_0 = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x_embeddings, attention_mask):\n",
    "\n",
    "        return self.norm(\n",
    "            x_embeddings +\n",
    "            self.w_0(\n",
    "                self.heads(\n",
    "                    x_embeddings.reshape(x_embeddings.shape[0], x_embeddings.shape[1], self.n_heads, self.head_dim).movedim(1,2).reshape(-1, x_embeddings.shape[1], self.head_dim), attention_mask\n",
    "                ).reshape(x_embeddings.shape[0], self.n_heads, x_embeddings.shape[1], self.head_dim).movedim(1,2).reshape(x_embeddings.shape[0], x_embeddings.shape[1], self.embedding_dim)\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "class DecoderLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim: int, max_seq_length: int, n_heads: int, hidden_size: int):\n",
    "        \"\"\"\n",
    "        Implements the Self-attention, decoder-only.\"\n",
    "\n",
    "        Args:\n",
    "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
    "            embedding_dim (int): Dimension of the embedding layer for each word in the context.\n",
    "            n_heads (int): Number of self attention heads.\n",
    "            hidden_size (int): Number of neurons for feed-forward MLP.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.context_size = max_seq_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # output MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.multihead_selfattention = MultiHeadAttentionLayer(embedding_dim, max_seq_length, n_heads)\n",
    "\n",
    "        # cast to probabilities\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x_embeddings, attention_mask):\n",
    "\n",
    "        e = self.multihead_selfattention(x_embeddings, attention_mask)\n",
    "\n",
    "        logits = self.mlp(e)\n",
    "\n",
    "        return self.norm(logits + e), attention_mask\n",
    "\n",
    "class LanguageModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, max_seq_length: int, embedding_dim: int, n_layers: int, n_heads: int, hidden_size: int=2048):\n",
    "        \"\"\"\n",
    "        Implements the Self-attention, decoder-only.\"\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the input vocabulary.\n",
    "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
    "            embedding_dim (int): Dimension of the embedding layer for each word in the context.\n",
    "            n_layers (int): number of self-attention layers.\n",
    "            n_heads (int): Number of self attention heads.\n",
    "            hidden_size (int): Number of neurons for feed-forward MLP.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        context_size = max_seq_length\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # tokens (words indexes) embedding and positional embedding\n",
    "        self.c_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.p_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_indexes = torch.arange(self.context_size, device=device).view(1, -1)\n",
    "\n",
    "        self.decoder_layers = MySequential(*[DecoderLayer(embedding_dim=embedding_dim, max_seq_length=max_seq_length, n_heads=n_heads, hidden_size=hidden_size) for i in range(n_layers)])\n",
    "\n",
    "        self.linear_output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "        # cast to probabilities\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, inputs, attention_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs is a LongTensor of shape (batch_size, max_seq_length)\n",
    "\n",
    "        Returns:\n",
    "            logits of shape (batch_size, max_seq_length, vocab_size)\n",
    "        \"\"\"\n",
    "        input_embeddings = self.c_embedding(inputs)\n",
    "\n",
    "        positional_embeddings = self.p_embedding(self.positional_indexes.repeat(inputs.shape[0], 1))\n",
    "\n",
    "        x_embeddings = positional_embeddings + input_embeddings\n",
    "\n",
    "        logits, _ = self.decoder_layers(x_embeddings, attention_mask)\n",
    "\n",
    "        return self.linear_output(logits)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testando se o modelo processa os dados corretamente"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 9, 30522])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_length=context_size,\n",
    "    embedding_dim=32,\n",
    "    n_layers=1,\n",
    "    n_heads=2,\n",
    "    hidden_size=32,\n",
    ").to(device)\n",
    "\n",
    "sample_train, _, attention_mask = next(iter(DataLoader(training_dataset)))\n",
    "sample_train_gpu = sample_train.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "model(sample_train_gpu, attention_mask).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Verificando a perplexidade"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:  30522\n",
      "logits shape:  torch.Size([100, 9, 30522])\n",
      "my perplexity:              34155\n",
      "correct initial perplexity: 30522\n",
      "Passou o no assert da perplexidade\n"
     ]
    }
   ],
   "source": [
    "def perplexity(logits, target, ignore_token_id: int):\n",
    "    \"\"\"\n",
    "    Computes the perplexity.\n",
    "\n",
    "    Args:\n",
    "        logits: a FloatTensor of shape (batch_size, seq_length, vocab_size)\n",
    "        target: a LongTensor of shape (batch_size, seq_length)\n",
    "\n",
    "    Returns:\n",
    "        A float corresponding to the perplexity\n",
    "    \"\"\"\n",
    "    logits = logits.reshape(-1, logits.shape[-1])\n",
    "    target = target.reshape(-1)\n",
    "    loss = nn.functional.cross_entropy(logits, target, reduction='mean', ignore_index=ignore_token_id)\n",
    "    return torch.exp(loss)\n",
    "\n",
    "\n",
    "n_examples = 100\n",
    "\n",
    "sample_train, target_token_ids, attention_mask = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
    "sample_train_gpu = sample_train.to(device)\n",
    "target_token_ids = target_token_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "logits = model(sample_train_gpu, attention_mask)\n",
    "\n",
    "print(\"vocab_size: \", tokenizer.vocab_size)\n",
    "\n",
    "print(\"logits shape: \", logits.shape)\n",
    "\n",
    "my_perplexity = perplexity(logits=logits, target=target_token_ids, ignore_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "print(f'my perplexity:              {int(my_perplexity)}')\n",
    "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
    "\n",
    "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=6000)\n",
    "print('Passou o no assert da perplexidade')\n",
    "\n",
    "del sample_train_gpu\n",
    "del target_token_ids\n",
    "del logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TREINAMENTO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60000000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-31-0239eedb9d14>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     69\u001B[0m                 valid_ppl = np.exp(np.average([\n\u001B[1;32m     70\u001B[0m                     \u001B[0mvalidation_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mval_input_ids\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_target_ids\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 71\u001B[0;31m                     for val_input_ids, val_target_ids, attention_mask in DataManager(validation_loader, device=device, buffer_size=1, data_type=None)]))\n\u001B[0m\u001B[1;32m     72\u001B[0m                 \u001B[0;31m# Checkpoint to best models found.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mbest_validation_ppl\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0mvalid_ppl\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-31-0239eedb9d14>\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     69\u001B[0m                 valid_ppl = np.exp(np.average([\n\u001B[1;32m     70\u001B[0m                     \u001B[0mvalidation_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mval_input_ids\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_target_ids\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 71\u001B[0;31m                     for val_input_ids, val_target_ids, attention_mask in DataManager(validation_loader, device=device, buffer_size=1, data_type=None)]))\n\u001B[0m\u001B[1;32m     72\u001B[0m                 \u001B[0;31m# Checkpoint to best models found.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mbest_validation_ppl\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0mvalid_ppl\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-31-0239eedb9d14>\u001B[0m in \u001B[0;36mvalidation_step\u001B[0;34m(input_ids, target_ids, attention_mask)\u001B[0m\n\u001B[1;32m     48\u001B[0m             \u001B[0mtarget_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtarget_ids\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctional\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcross_entropy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlogits\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_ids\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 50\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     51\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[0mbest_validation_ppl\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m9999\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "max_examples = 30_000_000\n",
    "eval_every_steps = 5000\n",
    "lr = 3e-5\n",
    "use_amp = True\n",
    "\n",
    "unscaling_factor = 1\n",
    "\n",
    "model = LanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_length=context_size,\n",
    "    embedding_dim=512//unscaling_factor,\n",
    "    n_layers=8//unscaling_factor,\n",
    "    n_heads=8,\n",
    "    hidden_size=2048//unscaling_factor,\n",
    ").to(device)\n",
    "train_loader = DataLoader(training_dataset, batch_size=1024//unscaling_factor, shuffle=True, num_workers=1)\n",
    "validation_loader = DataLoader(valid_dataset, batch_size=1024//unscaling_factor, num_workers=1, )\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.8, min_lr=8e-6, patience=3, threshold=5e-3, verbose=True)\n",
    "scaler=GradScaler()\n",
    "\n",
    "\n",
    "def train_step(input_ids, target_ids, attention_mask):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    with autocast(enabled=use_amp):\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "        target_ids = target_ids.reshape(-1)\n",
    "    loss = nn.functional.cross_entropy(logits, target_ids, )\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation_step(input_ids, target_ids, attention_mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with autocast(enabled=use_amp):\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = logits.reshape(-1, logits.shape[-1])\n",
    "            target_ids = target_ids.reshape(-1)\n",
    "            loss = nn.functional.cross_entropy(logits, target_ids,)\n",
    "    return loss.item()\n",
    "\n",
    "best_validation_ppl = 9999\n",
    "train_losses = []\n",
    "n_examples = 0\n",
    "step = 0\n",
    "pbar = tqdm(total=max_examples)\n",
    "while n_examples < max_examples:\n",
    "    for train_input_ids, train_target_ids, attention_mask in DataManager(train_loader, device=device, buffer_size=1, data_type=None):\n",
    "        loss = train_step(train_input_ids.to(device), train_target_ids.to(device), attention_mask)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        if step % eval_every_steps == 0:\n",
    "            train_ppl = np.exp(np.average(train_losses))\n",
    "\n",
    "            # LR scheduler\n",
    "            scheduler.step(np.average(train_losses))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_ppl = np.exp(np.average([\n",
    "                    validation_step(val_input_ids.to(device), val_target_ids.to(device), attention_mask)\n",
    "                    for val_input_ids, val_target_ids, attention_mask in DataManager(validation_loader, device=device, buffer_size=1, data_type=None)]))\n",
    "                # Checkpoint to best models found.\n",
    "                if best_validation_ppl > valid_ppl:\n",
    "                    # Update the new best perplexity.\n",
    "                    best_validation_ppl = valid_ppl\n",
    "                    model.eval()\n",
    "                    torch.save(model, \"best_model.pth\")\n",
    "\n",
    "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
    "            train_losses = []\n",
    "\n",
    "        n_examples += len(train_input_ids)  # Increment of batch size\n",
    "        step += 1\n",
    "        pbar.update(len(train_input_ids))\n",
    "        if n_examples >= max_examples:\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Restore best model (checkpoint) found\n",
    "model = torch.load(\"best_model.pth\")\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Avaliação no dataset de Teste"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=64, num_workers=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_ppl = np.exp(np.average([\n",
    "        validation_step(input.to(device), target.to(device))\n",
    "        for input, target, attention_mask in tqdm(test_loader)\n",
    "    ]))\n",
    "\n",
    "print(f'test perplexity: {test_ppl}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testando gerar uma sentença"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = 'Frankenstein tells the story of gifted scientist Victor Frankenstein who succeeds in giving life to'\n",
    "max_output_tokens = 10\n",
    "\n",
    "for _ in range(max_output_tokens):\n",
    "    input_ids = tokenizer(prompt, return_tensors=None, add_special_tokens=False).input_ids\n",
    "    input_ids_truncated = input_ids[-context_size:]# Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
    "    logits = model(torch.LongTensor([input_ids_truncated]).to(device), torch.tensor([False] * context_size, device=device))[:, -1, :]\n",
    "    # Ao usarmos o argmax, a saída do modelo em cada passo é token de maior probabilidade.\n",
    "    # Isso se chama decodificação gulosa (greedy decoding).\n",
    "    predicted_id = torch.argmax(logits).item()\n",
    "    predicted_word = tokenizer.decode([predicted_id,])\n",
    "    prompt += predicted_word[0]\n",
    "    print(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}