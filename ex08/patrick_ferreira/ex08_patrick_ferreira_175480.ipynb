{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "pycharm-79e5302d",
   "language": "python",
   "display_name": "PyCharm (IA024_2022S2)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/patrickctrf/IA024_2022S2/blob/main/ex08/patrick_ferreira/ex08_patrick_ferreira_175480.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu nome é Patrick de Carvalho Tavares Rezende Ferreira\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nome = \"Patrick de Carvalho Tavares Rezende Ferreira\"\n",
    "print(f'Meu nome é {nome}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Exercício: Modelo de Linguagem com auto-atenção (versão eficiente)\n",
    "\n",
    "Este exercício é similar ao da aula 5, mas iremos agora treinar *eficientemente* uma rede neural com uma ou mais camadas de auto-atenção para prever a próxima palavra de um texto, data as palavras anteriores como entrada. \n",
    "\n",
    "Para tanto, deve-se implementar:\n",
    "1. A máscara causal de atenção. Ela possibilitará que, durante o treinamento, com apenas uma forward+backward pass na rede, tenhamos as losses para todos os tokens de entrada (slide 117).\n",
    "2. A máscara de PADs, que permite que usemos sequencias de comprimento variável no mesmo batch (slide 118).\n",
    "3. Múltiplas cabeças."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importação dos pacotes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from typing import List"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Check which GPU we are using\n",
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "   dev = \"cuda:0\"\n",
    "else: \n",
    "   dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print('Using {}'.format(device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Carregamento do dataset \n",
    "\n",
    "Primeiro, fazemos download do dataset:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘aclImdb.tgz’ already there; not retrieving.\r\n",
      "\r\n",
      "tar (child): v: Cannot open: No such file or directory\r\n",
      "tar (child): Error is not recoverable: exiting now\r\n",
      "tar: Child returned status 2\r\n",
      "tar: Error is not recoverable: exiting now\r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
    "!tar -xzf aclImdb.tgz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ptk-patrickctrf in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (0.1.2)\r\n",
      "Requirement already satisfied: torch in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from ptk-patrickctrf) (1.8.1)\r\n",
      "Requirement already satisfied: tensorflow in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from ptk-patrickctrf) (2.2.0)\r\n",
      "Requirement already satisfied: numpy in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from ptk-patrickctrf) (1.19.1)\r\n",
      "Requirement already satisfied: typing_extensions in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from torch->ptk-patrickctrf) (3.7.4.3)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (2.2.0)\r\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (1.4.1)\r\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (0.35.1)\r\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (2.10.0)\r\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (1.6.3)\r\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (1.31.0)\r\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (0.10.0)\r\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (3.13.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (0.2.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (1.1.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (3.1.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (1.11.2)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (1.15.0)\r\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (2.2.1)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (1.1.0)\r\n",
      "Requirement already satisfied: gast==0.3.3 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorflow->ptk-patrickctrf) (0.3.3)\r\n",
      "Requirement already satisfied: setuptools in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow->ptk-patrickctrf) (49.6.0.post20200814)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (2.24.0)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (1.21.2)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (0.4.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (3.2.2)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (1.6.0)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (1.0.1)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (2020.6.20)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (1.25.10)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (2.10)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (0.2.8)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (4.1.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (4.6)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (1.3.0)\r\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (1.7.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (3.1.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow->ptk-patrickctrf) (3.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ptk-patrickctrf\n",
    "!pip install transformers\n",
    "\n",
    "from operator import itemgetter\n",
    "from transformers import BertTokenizer\n",
    "from ptk.utils import DataManager"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Carregando o dataset\n",
    "\n",
    "Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n",
    "\n",
    "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 amostras de treino.\n",
      "5000 amostras de desenvolvimento.\n",
      "25000 amostras de teste.\n",
      "3 primeiras amostras treino:\n",
      "I saw this movie two weeks ago at the \"festival des nouvelles images du Japon\" in Paris. Though i wa\n",
      "Reading a wide variety of \"Scoop\" reviews over the past few days, I walked into the theater prepared\n",
      "Hilariously obvious \"drama\" about a bunch of high school (I think) kids who enjoy non-stop hip-hop, \n",
      "3 últimas amostras treino:\n",
      "This movie couldn't decide what it wanted to be. There were a couple of sub-plots that for awhile ma\n",
      "\"Written on the Wind\" is an irresistible, wonderfully kinky film, as only director Sirk could have d\n",
      "I have grown up reading Modesty Blaise, both the comics and the books, and she truly is a heroine to\n",
      "3 primeiras amostras validação:\n",
      "Jackie Chan name is synonomus to stunts. This movie never let you down.The opening best chase scene \n",
      "This rather formulaic swords and flying fists movie is a decent early display of John Woo's talents.\n",
      "Judy Davis shows us here why she is one of Australia's most respected and loved actors - her portray\n",
      "3 últimas amostras validação:\n",
      "'Stanley and Iris' show the triumph of the human spirit. For Stanley, it's the struggle to become li\n",
      "Okay. This has been a favourite since I was 14. Granted, I don't watch it multiple times a year anym\n",
      "This is one of the worst-written movies I've ever had to sit through.<br /><br />The story's nothing\n"
     ]
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7fd2370137f0>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_texts(folder):\n",
    "    texts = []\n",
    "    for path in os.listdir(folder):\n",
    "        with open(os.path.join(folder, path)) as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "x_train_pos = load_texts('aclImdb/train/pos')\n",
    "x_train_neg = load_texts('aclImdb/train/neg')\n",
    "x_test_pos = load_texts('aclImdb/test/pos')\n",
    "x_test_neg = load_texts('aclImdb/test/neg')\n",
    "\n",
    "x_train = x_train_pos + x_train_neg\n",
    "x_test = x_test_pos + x_test_neg\n",
    "\n",
    "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
    "random.shuffle(x_train)\n",
    "\n",
    "n_train = int(0.8 * len(x_train))\n",
    "\n",
    "x_valid = x_train[n_train:]\n",
    "x_train = x_train[:n_train]\n",
    "\n",
    "print(len(x_train), 'amostras de treino.')\n",
    "print(len(x_valid), 'amostras de desenvolvimento.')\n",
    "print(len(x_test), 'amostras de teste.')\n",
    "\n",
    "print('3 primeiras amostras treino:')\n",
    "for x in x_train[:3]:\n",
    "    print(x[:100])\n",
    "\n",
    "print('3 últimas amostras treino:')\n",
    "for x in x_train[-3:]:\n",
    "    print(x[:100])\n",
    "\n",
    "print('3 primeiras amostras validação:')\n",
    "for x in x_valid[:3]:\n",
    "    print(x[:100])\n",
    "\n",
    "print('3 últimas amostras validação:')\n",
    "for x in x_valid[-3:]:\n",
    "    print(x[:100])\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "### Criando classe do dataset\n",
    "\n",
    "class MyDataset():\n",
    "    def __init__(self, texts: List[str], tokenizer, context_size: int, vocab_size=1000):\n",
    "        try:\n",
    "            self.x = np.load(\"x_\" + str(len(texts)) + \".npy\", mmap_mode=\"r\", allow_pickle=True)\n",
    "            self.y = np.load(\"y_\" + str(len(texts)) + \".npy\", mmap_mode=\"r\", allow_pickle=True)\n",
    "            print(\"Carregando dataset preprocessado\")\n",
    "        except Exception as e:\n",
    "            # print(\"Excecao: \", e)\n",
    "            print(\"Montando dataset\")\n",
    "\n",
    "            self.x = list()\n",
    "            self.y = list()\n",
    "\n",
    "            for text in tqdm(texts):\n",
    "                tokens_key = tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
    "                for i in range(0, len(tokens_key)-context_size-1, context_size):\n",
    "                    self.x.append(tokens_key[i:i+context_size])\n",
    "                    self.y.append(tokens_key[i+1:i+context_size+1])\n",
    "\n",
    "            self.x = np.array(self.x)\n",
    "            self.y = np.array(self.y)\n",
    "\n",
    "            np.save(\"x_\" + str(len(texts)) + \".npy\", self.x)\n",
    "            np.save(\"y_\" + str(len(texts)) + \".npy\", self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.x[idx]).long(), torch.tensor(self.y[idx]).long()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testando Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1153.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montando dataset\n",
      "Passou no assert de tamanho do dataset\n",
      "Passou no assert de input\n",
      "Passou no assert de target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
    "\n",
    "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, context_size=3, vocab_size=tokenizer.vocab_size)\n",
    "dummy_loader = DataLoader(dummy_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "assert len(dummy_dataset) == 3\n",
    "print('Passou no assert de tamanho do dataset')\n",
    "\n",
    "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
    "\n",
    "correct_first_batch_input = torch.LongTensor(\n",
    "    [[7327, 2175, 16033],\n",
    "     [3449, 2050, 2175]])\n",
    "\n",
    "correct_first_batch_target = torch.LongTensor(\n",
    "    [[2175, 16033, 2139],\n",
    "     [2050, 2175, 9153]])\n",
    "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
    "print('Passou no assert de input')\n",
    "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
    "print('Passou no assert de target')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dados de treino, validação e teste"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1291 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 19/20000 [00:00<01:46, 187.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montando dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 4298/20000 [00:23<01:25, 184.61it/s]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-8-4cbccd36281b>\", line 6, in __init__\n",
      "    self.x = np.load(\"x_\" + str(len(texts)) + \".npy\", mmap_mode=\"r\", allow_pickle=True)\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages/numpy/lib/npyio.py\", line 416, in load\n",
      "    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'x_20000.npy'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3417, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-10-16d8f247e0b2>\", line 8, in <module>\n",
      "    training_dataset = MyDataset(texts=x_train, tokenizer=tokenizer, context_size=context_size)\n",
      "  File \"<ipython-input-8-4cbccd36281b>\", line 17, in __init__\n",
      "    tokens_key = tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 2515, in __call__\n",
      "    **kwargs,\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 2588, in encode_plus\n",
      "    **kwargs,\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/tokenization_utils.py\", line 646, in _encode_plus\n",
      "    first_ids = get_input_ids(text)\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/tokenization_utils.py\", line 615, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/tokenization_utils.py\", line 546, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\", line 224, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\", line 391, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\", line 491, in _clean_text\n",
      "    if cp == 0 or cp == 0xFFFD or _is_control(char):\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/tokenization_utils.py\", line 286, in _is_control\n",
      "    if cat.startswith(\"C\"):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/posixpath.py\", line 379, in abspath\n",
      "    if not isabs(path):\n",
      "  File \"/home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/posixpath.py\", line 66, in isabs\n",
      "    s = os.fspath(s)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-4cbccd36281b>\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, texts, tokenizer, context_size, vocab_size)\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"x_\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtexts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\".npy\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmmap_mode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mallow_pickle\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"y_\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtexts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\".npy\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmmap_mode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mallow_pickle\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/numpy/lib/npyio.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001B[0m\n\u001B[1;32m    415\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 416\u001B[0;31m             \u001B[0mfid\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstack\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menter_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos_fspath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"rb\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    417\u001B[0m             \u001B[0mown_fid\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'x_20000.npy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[0;32m<ipython-input-10-16d8f247e0b2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0mtraining_dataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mMyDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtexts\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcontext_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0mvalid_dataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mMyDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtexts\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx_valid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcontext_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-8-4cbccd36281b>\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, texts, tokenizer, context_size, vocab_size)\u001B[0m\n\u001B[1;32m     16\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mtext\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtexts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m                 \u001B[0mtokens_key\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturn_tensors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madd_special_tokens\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokens_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mcontext_size\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2514\u001B[0m                 \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mverbose\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2515\u001B[0;31m                 \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2516\u001B[0m             )\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36mencode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2587\u001B[0m             \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mverbose\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2588\u001B[0;31m             \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2589\u001B[0m         )\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001B[0m in \u001B[0;36m_encode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m    645\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 646\u001B[0;31m         \u001B[0mfirst_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_input_ids\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    647\u001B[0m         \u001B[0msecond_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_input_ids\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext_pair\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mtext_pair\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001B[0m in \u001B[0;36mget_input_ids\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m    614\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 615\u001B[0;31m                 \u001B[0mtokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    616\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_tokens_to_ids\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokens\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001B[0m in \u001B[0;36mtokenize\u001B[0;34m(self, text, **kwargs)\u001B[0m\n\u001B[1;32m    545\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 546\u001B[0;31m                 \u001B[0mtokenized_text\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    547\u001B[0m         \u001B[0;31m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\u001B[0m in \u001B[0;36m_tokenize\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m    223\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_basic_tokenize\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 224\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbasic_tokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnever_split\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mall_special_tokens\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    225\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\u001B[0m in \u001B[0;36mtokenize\u001B[0;34m(self, text, never_split)\u001B[0m\n\u001B[1;32m    390\u001B[0m         \u001B[0mnever_split\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnever_split\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnever_split\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mnever_split\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnever_split\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 391\u001B[0;31m         \u001B[0mtext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_clean_text\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    392\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\u001B[0m in \u001B[0;36m_clean_text\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m    490\u001B[0m             \u001B[0mcp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mord\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchar\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 491\u001B[0;31m             \u001B[0;32mif\u001B[0m \u001B[0mcp\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mcp\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0xFFFD\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_is_control\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchar\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    492\u001B[0m                 \u001B[0;32mcontinue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001B[0m in \u001B[0;36m_is_control\u001B[0;34m(char)\u001B[0m\n\u001B[1;32m    285\u001B[0m     \u001B[0mcat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0municodedata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcategory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchar\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 286\u001B[0;31m     \u001B[0;32mif\u001B[0m \u001B[0mcat\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"C\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    287\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2043\u001B[0m                         \u001B[0;31m# in the engines. This should return a list of strings.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2044\u001B[0;31m                         \u001B[0mstb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_render_traceback_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2045\u001B[0m                     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mshowtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2045\u001B[0m                     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2046\u001B[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001B[0;32m-> 2047\u001B[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001B[0m\u001B[1;32m   2048\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2049\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_showtraceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0metype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1434\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1435\u001B[0m         return FormattedTB.structured_traceback(\n\u001B[0;32m-> 1436\u001B[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001B[0m\u001B[1;32m   1437\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1438\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1334\u001B[0m             \u001B[0;31m# Verbose modes need a full traceback\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1335\u001B[0m             return VerboseTB.structured_traceback(\n\u001B[0;32m-> 1336\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0metype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtb_offset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnumber_of_lines_of_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1337\u001B[0m             )\n\u001B[1;32m   1338\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mmode\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'Minimal'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mstructured_traceback\u001B[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[1;32m   1191\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1192\u001B[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001B[0;32m-> 1193\u001B[0;31m                                                                tb_offset)\n\u001B[0m\u001B[1;32m   1194\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1195\u001B[0m         \u001B[0mcolors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mColors\u001B[0m  \u001B[0;31m# just a shorthand + quicker name lookup\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mformat_exception_as_a_whole\u001B[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001B[0m\n\u001B[1;32m   1148\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1149\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1150\u001B[0;31m         \u001B[0mlast_unique\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfind_recursion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0morig_etype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1151\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1152\u001B[0m         \u001B[0mframes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat_records\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_unique\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecursion_repeat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/IPython/core/ultratb.py\u001B[0m in \u001B[0;36mfind_recursion\u001B[0;34m(etype, value, records)\u001B[0m\n\u001B[1;32m    449\u001B[0m     \u001B[0;31m# first frame (from in to out) that looks different.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    450\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_recursion_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0metype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 451\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    452\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    453\u001B[0m     \u001B[0;31m# Select filename, lineno, func_name to track frames with\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "context_size = 9\n",
    "\n",
    "# tokenizer = Tokenizador(x_train, tokenize, vocab_size=3000)\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "training_dataset = MyDataset(texts=x_train, tokenizer=tokenizer, context_size=context_size)\n",
    "valid_dataset = MyDataset(texts=x_valid, tokenizer=tokenizer, context_size=context_size)\n",
    "test_dataset = MyDataset(texts=x_test, tokenizer=tokenizer, context_size=context_size)\n",
    "\n",
    "print(f'training examples: {len(training_dataset)}')\n",
    "print(f'valid examples: {len(valid_dataset)}')\n",
    "print(f'test examples: {len(test_dataset)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AttentionLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim: int, max_seq_length: int):\n",
    "        \"\"\"\n",
    "        Implements the Self-attention, decoder-only.\"\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the input vocabulary.\n",
    "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
    "            dim (int): Dimension of the embedding layer for each word in the context.\n",
    "            n_layers (int): number of self-attention layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.context_size = max_seq_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # hidden_size for MLP\n",
    "        hidden_size = 2048\n",
    "\n",
    "        # Linear projections\n",
    "        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_0 = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        # output MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "        # cast to probabilities\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Matriz triangular de mascara, convertida para Booleano\n",
    "        # Onde vale 1, o valor deve ser substituida por um valor negativo alto no tensor de scores.\n",
    "        self.causal_mask = torch.ones((max_seq_length, max_seq_length), device=device).triu(diagonal=1) == 1.0\n",
    "\n",
    "    def forward(self, x_embeddings):\n",
    "\n",
    "        k = self.w_k(x_embeddings)\n",
    "        v = self.w_v(x_embeddings)\n",
    "        q = self.w_q(x_embeddings)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(1, 2))\n",
    "\n",
    "        # Onde a mascara vale 1, retornamos um valor negativo grande.\n",
    "        # Onde a mascara vale zero, matemos intacto.\n",
    "        probabilities = self.softmax(scores.masked_fill(self.causal_mask, -1e4))\n",
    "\n",
    "        e = self.w_0(self.norm1(x_embeddings + torch.matmul(probabilities, v)))\n",
    "\n",
    "        logits = self.mlp(e)\n",
    "\n",
    "        return self.norm2(self.activation(logits + e))\n",
    "\n",
    "class LanguageModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, max_seq_length: int, dim: int, n_layers: int,):\n",
    "        \"\"\"\n",
    "        Implements the Self-attention, decoder-only.\"\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the input vocabulary.\n",
    "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
    "            dim (int): Dimension of the embedding layer for each word in the context.\n",
    "            n_layers (int): number of self-attention layers.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        embedding_dim = dim\n",
    "        context_size = max_seq_length\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # tokens (words indexes) embedding and positional embedding\n",
    "        self.c_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.p_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.attention = nn.Sequential(*[AttentionLayer(embedding_dim=embedding_dim, max_seq_length=max_seq_length) for i in range(n_layers)])\n",
    "\n",
    "        self.linear_output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "        # cast to probabilities\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.positional_indexes = torch.arange(self.context_size, device=device).view(1, -1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs is a LongTensor of shape (batch_size, max_seq_length)\n",
    "\n",
    "        Returns:\n",
    "            logits of shape (batch_size, max_seq_length, vocab_size)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        input_embeddings = self.c_embedding(inputs)\n",
    "\n",
    "        positional_embeddings = self.p_embedding(self.positional_indexes.repeat(inputs.shape[0], 1))\n",
    "\n",
    "        x_embeddings = positional_embeddings + input_embeddings\n",
    "\n",
    "        logits = self.attention(x_embeddings)\n",
    "\n",
    "        return self.linear_output(logits)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testando se o modelo processa os dados corretamente"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = LanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_length=context_size,\n",
    "    dim=32,\n",
    "    n_layers=1,\n",
    ").to(device)\n",
    "\n",
    "sample_train, _ = next(iter(DataLoader(training_dataset)))\n",
    "sample_train_gpu = sample_train.to(device)\n",
    "model(sample_train_gpu).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Verificando a perplexidade"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def perplexity(logits, target, ignore_token_id: int):\n",
    "    \"\"\"\n",
    "    Computes the perplexity.\n",
    "\n",
    "    Args:\n",
    "        logits: a FloatTensor of shape (batch_size, seq_length, vocab_size)\n",
    "        target: a LongTensor of shape (batch_size, seq_length)\n",
    "\n",
    "    Returns:\n",
    "        A float corresponding to the perplexity\n",
    "    \"\"\"\n",
    "    logits = logits.reshape(-1, logits.shape[-1])\n",
    "    target = target.reshape(-1)\n",
    "    loss = nn.functional.cross_entropy(logits, target, reduction='mean', ignore_index=ignore_token_id)\n",
    "    return torch.exp(loss)\n",
    "\n",
    "\n",
    "n_examples = 100\n",
    "\n",
    "sample_train, target_token_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
    "sample_train_gpu = sample_train.to(device)\n",
    "target_token_ids = target_token_ids.to(device)\n",
    "logits = model(sample_train_gpu)\n",
    "\n",
    "print(\"vocab_size: \", tokenizer.vocab_size)\n",
    "\n",
    "print(\"logits shape: \", logits.shape)\n",
    "\n",
    "my_perplexity = perplexity(logits=logits, target=target_token_ids, ignore_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "print(f'my perplexity:              {int(my_perplexity)}')\n",
    "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
    "\n",
    "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=6000)\n",
    "print('Passou o no assert da perplexidade')\n",
    "\n",
    "del sample_train_gpu\n",
    "del target_token_ids\n",
    "del logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TREINAMENTO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "max_examples = 60_000_000\n",
    "eval_every_steps = 5000\n",
    "lr = 4e-5\n",
    "use_amp = True\n",
    "\n",
    "model = LanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_length=context_size,\n",
    "    dim=256,\n",
    "    n_layers=4,\n",
    ").to(device)\n",
    "train_loader = DataLoader(training_dataset, batch_size=1024, shuffle=True, num_workers=1)\n",
    "validation_loader = DataLoader(valid_dataset, batch_size=1024, num_workers=1, )\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.8, min_lr=3e-5, patience=3, threshold=5e-3, verbose=True)\n",
    "scaler=GradScaler()\n",
    "\n",
    "\n",
    "def train_step(input_ids, target_ids):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    with autocast(enabled=use_amp):\n",
    "        logits = model(input_ids)\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "        target_ids = target_ids.reshape(-1)\n",
    "    loss = nn.functional.cross_entropy(logits, target_ids, )\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation_step(input_ids, target_ids):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with autocast(enabled=use_amp):\n",
    "            logits = model(input_ids)\n",
    "            logits = logits.reshape(-1, logits.shape[-1])\n",
    "            target_ids = target_ids.reshape(-1)\n",
    "            loss = nn.functional.cross_entropy(logits, target_ids,)\n",
    "    return loss.item()\n",
    "\n",
    "best_validation_ppl = 9999\n",
    "train_losses = []\n",
    "n_examples = 0\n",
    "step = 0\n",
    "pbar = tqdm(total=max_examples)\n",
    "while n_examples < max_examples:\n",
    "    for train_input_ids, train_target_ids in DataManager(train_loader, device=device, buffer_size=1, data_type=None):\n",
    "        loss = train_step(train_input_ids.to(device), train_target_ids.to(device))\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        if step % eval_every_steps == 0:\n",
    "            train_ppl = np.exp(np.average(train_losses))\n",
    "\n",
    "            # LR scheduler\n",
    "            scheduler.step(np.average(train_losses))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_ppl = np.exp(np.average([\n",
    "                    validation_step(val_input_ids.to(device), val_target_ids.to(device))\n",
    "                    for val_input_ids, val_target_ids in DataManager(validation_loader, device=device, buffer_size=1, data_type=None)]))\n",
    "                # Checkpoint to best models found.\n",
    "                if best_validation_ppl > valid_ppl:\n",
    "                    # Update the new best perplexity.\n",
    "                    best_validation_ppl = valid_ppl\n",
    "                    model.eval()\n",
    "                    torch.save(model, \"best_model.pth\")\n",
    "\n",
    "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
    "            train_losses = []\n",
    "\n",
    "        n_examples += len(train_input_ids)  # Increment of batch size\n",
    "        step += 1\n",
    "        pbar.update(len(train_input_ids))\n",
    "        if n_examples >= max_examples:\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Restore best model (checkpoint) found\n",
    "model = torch.load(\"best_model.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Avaliação no dataset de Teste"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=64, num_workers=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_ppl = np.exp(np.average([\n",
    "        validation_step(input.to(device), target.to(device))\n",
    "        for input, target in tqdm(test_loader)\n",
    "    ]))\n",
    "\n",
    "print(f'test perplexity: {test_ppl}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testando gerar uma sentença"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = 'Frankenstein tells the story of gifted scientist Victor Frankenstein who succeeds in giving life to'\n",
    "max_output_tokens = 10\n",
    "\n",
    "for _ in range(max_output_tokens):\n",
    "    input_ids = tokenizer(prompt, return_tensors=None, add_special_tokens=False).input_ids\n",
    "    input_ids_truncated = input_ids[-context_size:]# Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
    "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))[:, -1, :]\n",
    "    # Ao usarmos o argmax, a saída do modelo em cada passo é token de maior probabilidade.\n",
    "    # Isso se chama decodificação gulosa (greedy decoding).\n",
    "    predicted_id = torch.argmax(logits).item()\n",
    "    predicted_word = tokenizer.decode([predicted_id,])\n",
    "    prompt += predicted_word[0]\n",
    "    print(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}