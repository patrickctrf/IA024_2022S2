{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/patrickctrf/IA024_2022S2/blob/main/ex10/patrick_ferreira/ex10_patrick_ferreira_175480.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OG5DT_dm6mk"
   },
   "source": [
    "# Notebook de referência \n",
    "\n",
    "Nome: Patrick de Carvalho Tavares Rezende Ferreira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZ80hHaftwUd"
   },
   "source": [
    "## Instruções:\n",
    "\n",
    "O objetivo desse colab é entender o comportamento das seguintes variáveis importantes de treinamento:\n",
    "   - Batch size\n",
    "   - Learning rate\n",
    "   - FLOPs (computação gasta no treinamento)\n",
    "   - Tamanho do modelo\n",
    "\n",
    "Para tanto, iremos treinar e medir a loss e acurácia de 3 modelos BERT para análise de sentimento (classificação binária) usando o dataset do IMDB (20k/5k amostras de treino/validação).\n",
    "\n",
    "Iremos fazer os 3 x 5 x 5 = 75 treinamentos, cada um usando um modelo (dentre 3), uma learning rate (dentre 5 valores) e batch size (dentro 5 valores) diferentes.\n",
    "\n",
    "Os modelos sugeridos a serem usados são:\n",
    "\n",
    "*   google/bert_uncased_L-2_H-128_A-2 (BERT-tiny, 4M params, ~0.5M non-embeddings)\n",
    "*   google/bert_uncased_L-4_H-256_A-4 (BERT-mini, 11M params, ~3.5M non-embeddings)\n",
    "*   google/bert_uncased_L-8_H-512_A-8 (BERT-medium, 41M params, ~25M non-embeddings)\n",
    "\n",
    "Durante cada treinamento, iremos armezenar as seguntes informações: \n",
    "\n",
    "    - GPU usada\n",
    "    - FP16 ou 32?\n",
    "    - step atual\n",
    "    - tempo de treinamento até então (wall time)\n",
    "    - loss de treino\n",
    "    - loss de validação\n",
    "    - acurácia de validação\n",
    "    \n",
    "Iremos gravar essas informações _várias vezes por época_. Caso os treinamentos usem GPUs diferentes, podemos ajustar o wall time com base no FLOPs das GPUs.\n",
    "\n",
    "Ao final, iremos plotar os seguintes gráficos:\n",
    "\n",
    "1.   batch_size vs learning rate vs melhor loss de validação para cada modelo (usar gráfico 3D ou heatmap);\n",
    "2.   tempo de treinamento (wall time) vs loss de validação. Plotar uma série (curva) para cada modelo, todas no mesmo gráfico. Para gerar cada curva, usar os melhores batch size e learning rate encontrados no gráfico 1. \n",
    "\n",
    "Com isso conseguiremos responder às seguintes perguntas:\n",
    "\n",
    "    - Se você tiver T horas de GPU para usar, é melhor usar o modelo tiny, mini ou medium? Verifique se existe alguma faixa de valores de T em que é melhor usar o tiny. \n",
    "    - Qual modelo demora mais para atingir a sua melhor acurácia de validação em termos de épocas. E em termos de tempo de treino, wall time?\n",
    "    - Para cada X vezes que aumentamos o batch size, como que devemos ajustar a learning rate?\n",
    "    - Os melhores hiperparametros são parecidos para os 3 modelos?\n",
    "\n",
    "Notas:\n",
    "- Para entender melhor como batch size e learning rate se relacionam, procure fazer a varredura com passos de 5x ou 10x. Por exemplo:\n",
    "    \n",
    "    learning rate = {1e-2, 1e-3, ..., 1e-6}\n",
    "    \n",
    "    batch size = {1, 10, 100, 1000, 10000}\n",
    "\n",
    "- Caso o batch não caiba em memória, usar acumulo de gradiente.\n",
    "- Tempos estimados de treinamento para uma época do IMDB usando uma T4:\n",
    "    - BERT-tiny: menos de 1 minuto\n",
    "    - BERT-mini: 3 minutos\n",
    "    - BERT-medium: 10 minutos. Portanto, se treinarmos por 2 épocas, o tempo total para rodar os experimentos será de `2 épocas x 10 min x 25 treinamentos ~ 9 horas`.\n",
    "- Sugerimos fazer primeiro todos os experimentos com BERT-tiny e BERT-mini. Quando souber da faixa de hiperparametros \"bons\", não precisa fazer os 25 treinamentos para o BERT-medium.\n",
    "    - TFLOPs (FP32) de cada GPU:\n",
    "        T4: 8,141\n",
    "        K80: 4,113\n",
    "        A100: 19,49\n",
    "    - Usar time.perf_counter() para medir o wall time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhpAkifICdJo"
   },
   "source": [
    "# Fixando a seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1ozXD-xYCcrT"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Type\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VFq9e8uoOwv",
    "outputId": "6dedfcd3-0e99-45eb-b279-d583020240e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (4.19.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (5.3.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (20.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (2020.7.14)\r\n",
      "Requirement already satisfied: requests in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (2.24.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (0.12.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (4.48.2)\r\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (1.7.0)\r\n",
      "Requirement already satisfied: filelock in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (3.0.12)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (0.6.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from transformers) (1.19.1)\r\n",
      "Requirement already satisfied: six in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from packaging>=20.0->transformers) (1.15.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests->transformers) (2.10)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests->transformers) (1.25.10)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/patrickctrf/anaconda3/envs/ia025/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wHeZ9nAOEB0U",
    "outputId": "a2230043-3809-4129-df91-5850576666ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7fa448021db0>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9f3PfifAwpU",
    "outputId": "098c70e7-6e72-4358-c63a-58b64e11e377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov  1 14:57:05 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.129.06   Driver Version: 470.129.06   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   56C    P0    N/A /  N/A |    152MiB /  2004MiB |      2%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1066      G   /usr/lib/xorg/Xorg                 20MiB |\r\n",
      "|    0   N/A  N/A      1651      G   /usr/lib/xorg/Xorg                 60MiB |\r\n",
      "|    0   N/A  N/A      1864      G   /usr/bin/gnome-shell               20MiB |\r\n",
      "|    0   N/A  N/A      3248      G   ...238113352292484761,131072       26MiB |\r\n",
      "|    0   N/A  N/A      4160      G   ...f_3416.log --shared-files       12MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Check which GPU we are using\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whTCe2i7AtoV",
    "outputId": "1f8b6f79-6688-493b-87c4-762894b14c49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    "   dev = \"cuda:0\"\n",
    "else: \n",
    "   dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print('Using {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXFdJz2KVeQw"
   },
   "source": [
    "## Preparando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHMi_Kq65fPM"
   },
   "source": [
    "Primeiro, fazemos download do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2wbnfzst5O3k",
    "outputId": "5043b24c-213e-478a-d25a-a266a391eda3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘aclImdb.tgz’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
    "!tar -xzf aclImdb.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Giyi5Rv_NIm"
   },
   "source": [
    "## Carregando o dataset\n",
    "\n",
    "Criaremos uma divisão de treino (20k exemplos) e validação (5k exemplos) artificialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HIN_xLI_TuT",
    "outputId": "07a73855-b6fc-44ca-bb32-731066058db0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 amostras de treino.\n",
      "5000 amostras de desenvolvimento.\n",
      "25000 amostras de teste.\n",
      "3 primeiras amostras treino:\n",
      "0 A truly dreadful film. I did not know initially that this was a Kiwi effort - but very soon I starte\n",
      "0 well its official. they have just killed American Pie. The first 3 were absolutely hysterical, but t\n",
      "1 I've read some terrible things about this film, so I was prepared for the worst. \"Confusing. Muddled\n",
      "3 últimas amostras treino:\n",
      "0 I cannot stand this show! Has there ever been even one redeeming quality, one funny punchline, or on\n",
      "1 Exquisite comedy starring Marian Davies (with the affable William Haines). Young Peggy arrives in Ho\n",
      "1 Made me wish my own happy birds could talk. Tisk tisk on the reviewer who dissed the movie. A sweet \n",
      "3 primeiras amostras validação:\n",
      "1 Oh my, from the box description I thought it would be LA-crazy like 2 Days in the Valley or Hugo Poo\n",
      "1 Sure this movie wasn't like. 16 blocks, inside man, an American haunting. etc...<br /><br />But It w\n",
      "1 Back (again) in Scotland, Lassie is (again) on trial for her life. Because the faithful dog sleeps o\n",
      "3 últimas amostras validação:\n",
      "1 Currently on METOO's new schedule at 4 pm on weekdays, right after \"Maverick\" and right before \"Wild\n",
      "1 The British horror film was in terminal decline by the start of the Seventies, but out of the blackn\n",
      "1 Surprisingly not terrible and well animated for one of Disney's straight to video throw away sequels\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "max_valid = 5000\n",
    "\n",
    "def load_texts(folder):\n",
    "    texts = []\n",
    "    for path in os.listdir(folder):\n",
    "        with open(os.path.join(folder, path)) as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "x_train_pos = load_texts('aclImdb/train/pos')\n",
    "x_train_neg = load_texts('aclImdb/train/neg')\n",
    "x_test_pos = load_texts('aclImdb/test/pos')\n",
    "x_test_neg = load_texts('aclImdb/test/neg')\n",
    "\n",
    "x_train = x_train_pos + x_train_neg\n",
    "x_test = x_test_pos + x_test_neg\n",
    "y_train = [1] * len(x_train_pos) + [0] * len(x_train_neg)\n",
    "y_test = [1] * len(x_test_pos) + [0] * len(x_test_neg)\n",
    "\n",
    "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
    "c = list(zip(x_train, y_train))\n",
    "random.shuffle(c)\n",
    "x_train, y_train = zip(*c)\n",
    "\n",
    "x_valid = x_train[-max_valid:]\n",
    "y_valid = y_train[-max_valid:]\n",
    "x_train = x_train[:-max_valid]\n",
    "y_train = y_train[:-max_valid]\n",
    "\n",
    "print(len(x_train), 'amostras de treino.')\n",
    "print(len(x_valid), 'amostras de desenvolvimento.')\n",
    "print(len(x_test), 'amostras de teste.')\n",
    "\n",
    "print('3 primeiras amostras treino:')\n",
    "for x, y in zip(x_train[:3], y_train[:3]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 últimas amostras treino:')\n",
    "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 primeiras amostras validação:')\n",
    "for x, y in zip(x_valid[:3], y_test[:3]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 últimas amostras validação:')\n",
    "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
    "    print(y, x[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "### Criando classe do dataset\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts: List[str], labels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], torch.tensor([0., 1.]) if self.labels[idx] else torch.tensor([1., 0.])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dados de treino, validação e teste"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training examples: 20000\n",
      "valid examples: 5000\n",
      "test examples: 25000\n"
     ]
    }
   ],
   "source": [
    "training_dataset = MyDataset(x_train, y_train)\n",
    "valid_dataset = MyDataset(x_valid, y_valid)\n",
    "test_dataset = MyDataset(x_test, y_test)\n",
    "\n",
    "print(f'training examples: {len(training_dataset)}')\n",
    "print(f'valid examples: {len(valid_dataset)}')\n",
    "print(f'test examples: {len(test_dataset)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testando se o modelo processa os dados corretamente"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output shape:  torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "model = tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "model.train().to(device)\n",
    "\n",
    "sample_train, _ = next(iter(DataLoader(training_dataset, batch_size=4)))\n",
    "\n",
    "sample_train = tokenizer.batch_encode_plus(sample_train, padding=True, return_tensors=\"pt\", truncation=True, max_length=200).to(device)\n",
    "\n",
    "print(\"model output shape: \", model(**sample_train).logits.shape)\n",
    "\n",
    "del sample_train\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TREINAMENTO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b43a44eb3fb48bfb8baa7e79de7b950"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "multi-target not supported at /opt/conda/conda-bld/pytorch_1616554827596/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-11-aeea44487032>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     48\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mtrain_input_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_target_ids\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m         \u001B[0mtrain_input_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_encode_plus\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_input_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturn_tensors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"pt\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtruncation\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_length\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m200\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 50\u001B[0;31m         \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_input_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_target_ids\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     51\u001B[0m         \u001B[0mtrain_losses\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-11-aeea44487032>\u001B[0m in \u001B[0;36mtrain_step\u001B[0;34m(input_ids, target_ids)\u001B[0m\n\u001B[1;32m     21\u001B[0m         \u001B[0mlogits\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogits\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m         \u001B[0mlogits\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlogits\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogits\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m     \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctional\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcross_entropy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlogits\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     24\u001B[0m     \u001B[0mscaler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscale\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[0mscaler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001B[0m\n\u001B[1;32m   2691\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0msize_average\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mreduce\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2692\u001B[0m         \u001B[0mreduction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_Reduction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlegacy_get_string\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msize_average\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2693\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mnll_loss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlog_softmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mignore_index\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2694\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2695\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ia025/lib/python3.7/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mnll_loss\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001B[0m\n\u001B[1;32m   2386\u001B[0m         )\n\u001B[1;32m   2387\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mdim\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2388\u001B[0;31m         \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnll_loss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_Reduction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_enum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreduction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mignore_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2389\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mdim\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2390\u001B[0m         \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnll_loss2d\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_Reduction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_enum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreduction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mignore_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: multi-target not supported at /opt/conda/conda-bld/pytorch_1616554827596/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "max_examples = 20_000\n",
    "eval_every_steps = 1000\n",
    "lr = 4e-5\n",
    "use_amp = True\n",
    "\n",
    "train_loader = DataLoader(training_dataset, batch_size=2, shuffle=True, num_workers=1)\n",
    "validation_loader = DataLoader(valid_dataset, batch_size=2, num_workers=1, )\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.9, min_lr=3e-5, patience=15000, threshold=1e-1, verbose=True)\n",
    "scaler=GradScaler()\n",
    "\n",
    "\n",
    "def train_step(input_ids, target_ids):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    with autocast(enabled=use_amp):\n",
    "        logits = model(**input_ids).logits\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "    loss = nn.functional.cross_entropy(logits, target_ids, )\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation_step(input_ids, target_ids):\n",
    "    model.eval()\n",
    "    accuracy = 0\n",
    "    with autocast(enabled=use_amp):\n",
    "        logits = model(**input_ids).logits\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "        loss = nn.functional.cross_entropy(logits, target_ids,)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        accuracy += (preds == y).sum() / logits.shape[0]\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "best_validation_loss = 9999\n",
    "train_losses = []\n",
    "n_examples = 0\n",
    "step = 0\n",
    "pbar = tqdm(total=max_examples)\n",
    "while n_examples < max_examples:\n",
    "    for train_input_ids, train_target_ids in train_loader:\n",
    "        train_input_ids = tokenizer.batch_encode_plus(train_input_ids, padding=True, return_tensors=\"pt\", truncation=True, max_length=200).to(device)\n",
    "        loss = train_step(train_input_ids, train_target_ids.to(device))\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        # LR scheduler\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        if step % eval_every_steps == 0:\n",
    "            train_loss = np.average(train_losses)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_loss = np.average([\n",
    "                    validation_step(tokenizer.batch_encode_plus(val_input_ids, padding=True, return_tensors=\"pt\", truncation=True, max_length=200).to(device), val_target_ids.to(device))[0]\n",
    "                    for val_input_ids, val_target_ids in validation_loader])\n",
    "                # Checkpoint to best models found.\n",
    "                if best_validation_loss > valid_loss:\n",
    "                    # Update the new best perplexity.\n",
    "                    best_validation_loss = valid_loss\n",
    "                    model.eval()\n",
    "                    torch.save(model, \"best_model.pth\")\n",
    "\n",
    "            print(f'{step} steps; {n_examples} examples so far; train loss: {train_loss:.2f}, valid loss: {valid_loss:.2f}')\n",
    "            train_losses = []\n",
    "\n",
    "        n_examples += len(train_input_ids)  # Increment of batch size\n",
    "        step += 1\n",
    "        pbar.update(len(train_input_ids))\n",
    "        if n_examples >= max_examples:\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Restore best model (checkpoint) found\n",
    "model = torch.load(\"best_model.pth\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Avaliação no dataset de Teste"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1024)\n",
    "\n",
    "with torch.no_grad():\n",
    "    losses = [\n",
    "        validation_step(tokenizer.batch_encode_plus(input, padding=True, return_tensors=\"pt\", truncation=True, max_length=200).to(device), target.to(device))\n",
    "        for input, target in tqdm(test_loader)\n",
    "    ]\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "losses = list(zip(*losses))\n",
    "\n",
    "test_loss, test_accuracy = np.average(losses[0]), torch.mean(torch.tensor(losses[1]).float())\n",
    "\n",
    "print(f'test loss: {test_loss}', f'test acc: {test_accuracy}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "pycharm-79e5302d",
   "language": "python",
   "display_name": "PyCharm (IA024_2022S2)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}