{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/patrickctrf/IA024_2022S2/blob/main/ex10/patrick_ferreira/ex10_patrick_ferreira_175480.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OG5DT_dm6mk"
   },
   "source": [
    "# Notebook de referência \n",
    "\n",
    "Nome: Patrick de Carvalho Tavares Rezende Ferreira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZ80hHaftwUd"
   },
   "source": [
    "## Instruções:\n",
    "\n",
    "O objetivo desse colab é entender o comportamento das seguintes variáveis importantes de treinamento:\n",
    "   - Batch size\n",
    "   - Learning rate\n",
    "   - FLOPs (computação gasta no treinamento)\n",
    "   - Tamanho do modelo\n",
    "\n",
    "Para tanto, iremos treinar e medir a loss e acurácia de 3 modelos BERT para análise de sentimento (classificação binária) usando o dataset do IMDB (20k/5k amostras de treino/validação).\n",
    "\n",
    "Iremos fazer os 3 x 5 x 5 = 75 treinamentos, cada um usando um modelo (dentre 3), uma learning rate (dentre 5 valores) e batch size (dentro 5 valores) diferentes.\n",
    "\n",
    "Os modelos sugeridos a serem usados são:\n",
    "\n",
    "*   google/bert_uncased_L-2_H-128_A-2 (BERT-tiny, 4M params, ~0.5M non-embeddings)\n",
    "*   google/bert_uncased_L-4_H-256_A-4 (BERT-mini, 11M params, ~3.5M non-embeddings)\n",
    "*   google/bert_uncased_L-8_H-512_A-8 (BERT-medium, 41M params, ~25M non-embeddings)\n",
    "\n",
    "Durante cada treinamento, iremos armezenar as seguntes informações: \n",
    "\n",
    "    - GPU usada\n",
    "    - FP16 ou 32?\n",
    "    - step atual\n",
    "    - tempo de treinamento até então (wall time)\n",
    "    - loss de treino\n",
    "    - loss de validação\n",
    "    - acurácia de validação\n",
    "    \n",
    "Iremos gravar essas informações _várias vezes por época_. Caso os treinamentos usem GPUs diferentes, podemos ajustar o wall time com base no FLOPs das GPUs.\n",
    "\n",
    "Ao final, iremos plotar os seguintes gráficos:\n",
    "\n",
    "1.   batch_size vs learning rate vs melhor loss de validação para cada modelo (usar gráfico 3D ou heatmap);\n",
    "2.   tempo de treinamento (wall time) vs loss de validação. Plotar uma série (curva) para cada modelo, todas no mesmo gráfico. Para gerar cada curva, usar os melhores batch size e learning rate encontrados no gráfico 1. \n",
    "\n",
    "Com isso conseguiremos responder às seguintes perguntas:\n",
    "\n",
    "    - Se você tiver T horas de GPU para usar, é melhor usar o modelo tiny, mini ou medium? Verifique se existe alguma faixa de valores de T em que é melhor usar o tiny. \n",
    "    - Qual modelo demora mais para atingir a sua melhor acurácia de validação em termos de épocas. E em termos de tempo de treino, wall time?\n",
    "    - Para cada X vezes que aumentamos o batch size, como que devemos ajustar a learning rate?\n",
    "    - Os melhores hiperparametros são parecidos para os 3 modelos?\n",
    "\n",
    "Notas:\n",
    "- Para entender melhor como batch size e learning rate se relacionam, procure fazer a varredura com passos de 5x ou 10x. Por exemplo:\n",
    "    \n",
    "    learning rate = {1e-2, 1e-3, ..., 1e-6}\n",
    "    \n",
    "    batch size = {1, 10, 100, 1000, 10000}\n",
    "\n",
    "- Caso o batch não caiba em memória, usar acumulo de gradiente.\n",
    "- Tempos estimados de treinamento para uma época do IMDB usando uma T4:\n",
    "    - BERT-tiny: menos de 1 minuto\n",
    "    - BERT-mini: 3 minutos\n",
    "    - BERT-medium: 10 minutos. Portanto, se treinarmos por 2 épocas, o tempo total para rodar os experimentos será de `2 épocas x 10 min x 25 treinamentos ~ 9 horas`.\n",
    "- Sugerimos fazer primeiro todos os experimentos com BERT-tiny e BERT-mini. Quando souber da faixa de hiperparametros \"bons\", não precisa fazer os 25 treinamentos para o BERT-medium.\n",
    "    - TFLOPs (FP32) de cada GPU:\n",
    "        T4: 8,141\n",
    "        K80: 4,113\n",
    "        A100: 19,49\n",
    "    - Usar time.perf_counter() para medir o wall time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhpAkifICdJo"
   },
   "source": [
    "# Fixando a seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1ozXD-xYCcrT"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Type\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VFq9e8uoOwv",
    "outputId": "6dedfcd3-0e99-45eb-b279-d583020240e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (4.23.1)\r\n",
      "Requirement already satisfied: requests in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.1)\r\n",
      "Requirement already satisfied: filelock in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (0.10.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wHeZ9nAOEB0U",
    "outputId": "a2230043-3809-4129-df91-5850576666ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f94742f7130>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9f3PfifAwpU",
    "outputId": "098c70e7-6e72-4358-c63a-58b64e11e377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov  2 22:03:57 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   54C    P8    13W / 170W |    160MiB / 12288MiB |      1%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      2050      G   /usr/lib/xorg/Xorg                 89MiB |\r\n",
      "|    0   N/A  N/A      2315      G   /usr/bin/gnome-shell               20MiB |\r\n",
      "|    0   N/A  N/A      2823      G   ...mviewer/tv_bin/TeamViewer       13MiB |\r\n",
      "|    0   N/A  N/A     11967      G   ..._11367.log --shared-files        2MiB |\r\n",
      "|    0   N/A  N/A     13044      G   ...819940313893855229,131072       20MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Check which GPU we are using\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whTCe2i7AtoV",
    "outputId": "1f8b6f79-6688-493b-87c4-762894b14c49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    "   dev = \"cuda:0\"\n",
    "else: \n",
    "   dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print('Using {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXFdJz2KVeQw"
   },
   "source": [
    "## Preparando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHMi_Kq65fPM"
   },
   "source": [
    "Primeiro, fazemos download do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2wbnfzst5O3k",
    "outputId": "5043b24c-213e-478a-d25a-a266a391eda3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘aclImdb.tgz’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
    "!tar -xzf aclImdb.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Giyi5Rv_NIm"
   },
   "source": [
    "## Carregando o dataset\n",
    "\n",
    "Criaremos uma divisão de treino (20k exemplos) e validação (5k exemplos) artificialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HIN_xLI_TuT",
    "outputId": "07a73855-b6fc-44ca-bb32-731066058db0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 amostras de treino.\n",
      "5000 amostras de desenvolvimento.\n",
      "25000 amostras de teste.\n",
      "3 primeiras amostras treino:\n",
      "0 Barry, a medical transcriptionist has his mind corroding from his job coupled with memories of an ab\n",
      "0 The movie seemed to appeal me because of the new type of Pokemon Celebi. But the plot was out of cou\n",
      "1 I can't remember many films where a bumbling idiot of a hero was so funny throughout. Leslie Cheung \n",
      "3 últimas amostras treino:\n",
      "0 I sat down to watch a documentary about Puerto Rico, and I ended up watching one about Nuyoricans. W\n",
      "1 It's a great American martial arts movie. The fighting scenes were pretty impressive for American mo\n",
      "1 The appeal of ancient films like this one is that you get to see an actual moving image of life over\n",
      "3 primeiras amostras validação:\n",
      "1 Why is this movie not in the 250 best? This movie looks still astoundingly fresh 56 years after its \n",
      "1 I've always believed that David and Bathsheba was a film originally intended for Tyrone Power at 20t\n",
      "1 I enjoy watching western films but this movie takes the biscuit. The script and dialogue is laughabl\n",
      "3 últimas amostras validação:\n",
      "1 This is my favorite of the older Tom & Jerry cartoons from the early 40's. The original version with\n",
      "1 \"Masks\" is a moving film that works on many levels. At its simplest, it is the haunting story of a s\n",
      "1 This is a feel good film, about one person's dreams and the drive or push to realize them. It is a b\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "max_valid = 5000\n",
    "\n",
    "def load_texts(folder):\n",
    "    texts = []\n",
    "    for path in os.listdir(folder):\n",
    "        with open(os.path.join(folder, path)) as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "x_train_pos = load_texts('aclImdb/train/pos')\n",
    "x_train_neg = load_texts('aclImdb/train/neg')\n",
    "x_test_pos = load_texts('aclImdb/test/pos')\n",
    "x_test_neg = load_texts('aclImdb/test/neg')\n",
    "\n",
    "x_train = x_train_pos + x_train_neg\n",
    "x_test = x_test_pos + x_test_neg\n",
    "y_train = [1] * len(x_train_pos) + [0] * len(x_train_neg)\n",
    "y_test = [1] * len(x_test_pos) + [0] * len(x_test_neg)\n",
    "\n",
    "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
    "c = list(zip(x_train, y_train))\n",
    "random.shuffle(c)\n",
    "x_train, y_train = zip(*c)\n",
    "\n",
    "x_valid = x_train[-max_valid:]\n",
    "y_valid = y_train[-max_valid:]\n",
    "x_train = x_train[:-max_valid]\n",
    "y_train = y_train[:-max_valid]\n",
    "\n",
    "print(len(x_train), 'amostras de treino.')\n",
    "print(len(x_valid), 'amostras de desenvolvimento.')\n",
    "print(len(x_test), 'amostras de teste.')\n",
    "\n",
    "print('3 primeiras amostras treino:')\n",
    "for x, y in zip(x_train[:3], y_train[:3]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 últimas amostras treino:')\n",
    "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 primeiras amostras validação:')\n",
    "for x, y in zip(x_valid[:3], y_test[:3]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 últimas amostras validação:')\n",
    "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
    "    print(y, x[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "### Criando classe do dataset\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts: List[str], labels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], torch.tensor([0., 1.]) if self.labels[idx] else torch.tensor([1., 0.])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dados de treino, validação e teste"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training examples: 20000\n",
      "valid examples: 5000\n",
      "test examples: 25000\n"
     ]
    }
   ],
   "source": [
    "training_dataset = MyDataset(x_train, y_train)\n",
    "valid_dataset = MyDataset(x_valid, y_valid)\n",
    "test_dataset = MyDataset(x_test, y_test)\n",
    "\n",
    "print(f'training examples: {len(training_dataset)}')\n",
    "print(f'valid examples: {len(valid_dataset)}')\n",
    "print(f'test examples: {len(test_dataset)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testando se o modelo processa os dados corretamente"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output shape:  torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "model = tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "model.train().to(device)\n",
    "\n",
    "sample_train, _ = next(iter(DataLoader(training_dataset, batch_size=4)))\n",
    "\n",
    "sample_train = tokenizer.batch_encode_plus(sample_train, padding=True, return_tensors=\"pt\", truncation=True, max_length=200).to(device)\n",
    "\n",
    "print(\"model output shape: \", model(**sample_train).logits.shape)\n",
    "\n",
    "del sample_train\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TREINAMENTO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Setup:  google/bert_uncased_L-2_H-128_A-2_batch_size_16_lr_0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/200 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51aedf0fed1548c2b00d442a74497921"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps; 0 examples so far; train loss: 0.70, valid loss: 0.69\n",
      "SALVOOOOOUUUUU\n",
      "Current Setup:  google/bert_uncased_L-2_H-128_A-2_batch_size_16_lr_1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33841/2706262820.py:108: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  log_df = log_df.append({\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/200 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e782ccc21b28469d8a8825b3d88349c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps; 0 examples so far; train loss: 0.69, valid loss: 0.70\n",
      "SALVOOOOOUUUUU\n",
      "Current Setup:  google/bert_uncased_L-2_H-128_A-2_batch_size_32_lr_0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33841/2706262820.py:108: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  log_df = log_df.append({\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/200 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2aed7b1171342d291e18e9e41105f8b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps; 0 examples so far; train loss: 0.72, valid loss: 0.80\n",
      "SALVOOOOOUUUUU\n",
      "Current Setup:  google/bert_uncased_L-2_H-128_A-2_batch_size_32_lr_1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33841/2706262820.py:108: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  log_df = log_df.append({\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/200 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e8ae4b25f42455f884208b4c68b7779"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps; 0 examples so far; train loss: 0.71, valid loss: 0.71\n",
      "SALVOOOOOUUUUU\n",
      "Current Setup:  google/bert_uncased_L-4_H-256_A-4_batch_size_16_lr_0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33841/2706262820.py:108: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  log_df = log_df.append({\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/200 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a71d859a567c4a179c82d893162ec446"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps; 0 examples so far; train loss: 0.72, valid loss: 1.14\n",
      "SALVOOOOOUUUUU\n",
      "Current Setup:  google/bert_uncased_L-4_H-256_A-4_batch_size_16_lr_1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33841/2706262820.py:108: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  log_df = log_df.append({\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/200 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee94481e303d45c79effbcbeab16c30f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps; 0 examples so far; train loss: 0.73, valid loss: 0.70\n",
      "SALVOOOOOUUUUU\n",
      "Current Setup:  google/bert_uncased_L-4_H-256_A-4_batch_size_32_lr_0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33841/2706262820.py:108: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  log_df = log_df.append({\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/200 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9bba5313f5c343ea8e32a5216695301c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps; 0 examples so far; train loss: 0.69, valid loss: 1.19\n",
      "SALVOOOOOUUUUU\n",
      "Current Setup:  google/bert_uncased_L-4_H-256_A-4_batch_size_32_lr_1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33841/2706262820.py:108: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  log_df = log_df.append({\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/200 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce7b3d5cd0d1443ebde746c973c24c84"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps; 0 examples so far; train loss: 0.69, valid loss: 0.69\n",
      "SALVOOOOOUUUUU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33841/2706262820.py:108: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  log_df = log_df.append({\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "use_amp = True\n",
    "\n",
    "def train_step(input_ids, target_ids):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    with autocast(enabled=use_amp):\n",
    "        logits = model(**input_ids).logits\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "    loss = nn.functional.cross_entropy(logits, target_ids, )\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation_step(input_ids, target_ids):\n",
    "    model.eval()\n",
    "    with autocast(enabled=use_amp):\n",
    "        logits = model(**input_ids).logits\n",
    "        loss = nn.functional.cross_entropy(logits, target_ids,)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        accuracy = (preds == target_ids.argmax(dim=1)).sum().float() / logits.shape[0]\n",
    "    return loss.item(), accuracy.item()\n",
    "\n",
    "epochs = 0.01\n",
    "max_examples = int(len(training_dataset) * epochs)\n",
    "\n",
    "rtx3060_tflops = 12.74 # https://www.techpowerup.com/gpu-specs/geforce-rtx-3060.c3682\n",
    "\n",
    "model_list = [\"google/bert_uncased_L-2_H-128_A-2\",\n",
    "              \"google/bert_uncased_L-4_H-256_A-4\",]\n",
    "              # \"google/bert_uncased_L-8_H-512_A-8\",]\n",
    "batch_sizes = [16, 32]\n",
    "learning_rates = [1e-2, 1e-6]\n",
    "\n",
    "log_df = pd.DataFrame.from_dict({\n",
    "    \"model_name\": [],\n",
    "    \"batch_size\": [],\n",
    "    \"lr\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"best_acc\": [],\n",
    "    \"wall_time\": []\n",
    "})\n",
    "\n",
    "for model_name in model_list:\n",
    "    for batch_size in batch_sizes:\n",
    "        for lr in learning_rates:\n",
    "            model_identifier = model_name + \"_batch_size_\" + str(batch_size) + \"_lr_\" + str(lr)\n",
    "            print(\"Current Setup: \", model_identifier)\n",
    "\n",
    "            model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "            model.train().to(device)\n",
    "\n",
    "            eval_every_steps = 1000 * batch_sizes[0] // (batch_size // 10)\n",
    "\n",
    "            train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "            validation_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=1, )\n",
    "\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            scaler=GradScaler()\n",
    "\n",
    "            best_validation_loss = 9999\n",
    "            best_validation_acc = 0\n",
    "            convergence_time = 0\n",
    "            train_losses = []\n",
    "            n_examples = 0\n",
    "            step = 0\n",
    "            pbar = tqdm(total=max_examples)\n",
    "            start_time = time.time()\n",
    "            while n_examples < max_examples:\n",
    "                val_losses = []\n",
    "                wall_times = []\n",
    "                for train_input_ids, train_target_ids in train_loader:\n",
    "                    train_input_ids = tokenizer.batch_encode_plus(train_input_ids, padding=True, return_tensors=\"pt\", truncation=True, max_length=200).to(device)\n",
    "                    loss = train_step(train_input_ids, train_target_ids.to(device))\n",
    "                    train_losses.append(loss)\n",
    "\n",
    "                    if step % eval_every_steps == 0:\n",
    "                        train_loss = np.average(train_losses)\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            valid_result = list(zip(*[\n",
    "                                validation_step(tokenizer.batch_encode_plus(val_input_ids, padding=True, return_tensors=\"pt\", truncation=True, max_length=200).to(device), val_target_ids.to(device))\n",
    "                                for val_input_ids, val_target_ids in validation_loader]))\n",
    "                            valid_loss = np.average(valid_result[0])\n",
    "                            valid_acc = np.average(valid_result[1])\n",
    "                            # Checkpoint to best models found.\n",
    "                            if best_validation_loss > valid_loss:\n",
    "                                # Update the new best perplexity.\n",
    "                                best_validation_loss = valid_loss\n",
    "                                best_validation_acc = valid_acc\n",
    "                                convergence_time = time.time() - start_time\n",
    "\n",
    "                        val_losses.append(valid_loss)\n",
    "                        wall_times.append((time.time()-start_time) * rtx3060_tflops)\n",
    "                        print(f'{step} steps; {n_examples} examples so far; train loss: {train_loss:.2f}, valid loss: {valid_loss:.2f}')\n",
    "                        train_losses = []\n",
    "\n",
    "                    n_examples += train_input_ids.input_ids.shape[0] # Increment of batch size\n",
    "                    step += 1\n",
    "                    pbar.update(train_input_ids.input_ids.shape[0])\n",
    "                    if n_examples >= max_examples:\n",
    "                        break\n",
    "            print(\"SALVOOOOOUUUUU\")\n",
    "            log_df = log_df.append({\n",
    "                \"model_name\": model_name,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"lr\": lr,\n",
    "                \"val_loss\": best_validation_loss,\n",
    "                \"best_acc\": best_validation_acc,\n",
    "                \"wall_time\": convergence_time * rtx3060_tflops,\n",
    "                \"losses_list\": val_losses,\n",
    "                \"times\": wall_times,\n",
    "            }, ignore_index=True)\n",
    "\n",
    "            pbar.close()\n",
    "\n",
    "# Restore best model (checkpoint) found\n",
    "# model = torch.load(\"best_model.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "                          model_name  batch_size        lr  val_loss  \\\n0  google/bert_uncased_L-2_H-128_A-2        16.0  0.010000  0.693602   \n1  google/bert_uncased_L-2_H-128_A-2        16.0  0.000001  0.696576   \n2  google/bert_uncased_L-2_H-128_A-2        32.0  0.010000  0.800803   \n3  google/bert_uncased_L-2_H-128_A-2        32.0  0.000001  0.709038   \n\n   best_acc   wall_time           losses_list                 times  \n0  0.507788  226.556708   [0.693602053121256]  [226.55689306735994]  \n1  0.469050  215.680036  [0.6965758107340755]  [215.68022449970246]  \n2  0.507166  204.462019  [0.8008026317426353]   [204.4622039604187]  \n3  0.492635  203.797734  [0.7090380723309365]   [203.7979252052307]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model_name</th>\n      <th>batch_size</th>\n      <th>lr</th>\n      <th>val_loss</th>\n      <th>best_acc</th>\n      <th>wall_time</th>\n      <th>losses_list</th>\n      <th>times</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>google/bert_uncased_L-2_H-128_A-2</td>\n      <td>16.0</td>\n      <td>0.010000</td>\n      <td>0.693602</td>\n      <td>0.507788</td>\n      <td>226.556708</td>\n      <td>[0.693602053121256]</td>\n      <td>[226.55689306735994]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>google/bert_uncased_L-2_H-128_A-2</td>\n      <td>16.0</td>\n      <td>0.000001</td>\n      <td>0.696576</td>\n      <td>0.469050</td>\n      <td>215.680036</td>\n      <td>[0.6965758107340755]</td>\n      <td>[215.68022449970246]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>google/bert_uncased_L-2_H-128_A-2</td>\n      <td>32.0</td>\n      <td>0.010000</td>\n      <td>0.800803</td>\n      <td>0.507166</td>\n      <td>204.462019</td>\n      <td>[0.8008026317426353]</td>\n      <td>[204.4622039604187]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>google/bert_uncased_L-2_H-128_A-2</td>\n      <td>32.0</td>\n      <td>0.000001</td>\n      <td>0.709038</td>\n      <td>0.492635</td>\n      <td>203.797734</td>\n      <td>[0.7090380723309365]</td>\n      <td>[203.7979252052307]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df.loc[log_df['model_name'] == \"google/bert_uncased_L-2_H-128_A-2\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}