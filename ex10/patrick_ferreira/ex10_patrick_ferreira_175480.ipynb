{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/patrickctrf/IA024_2022S2/blob/main/ex10/patrick_ferreira/ex10_patrick_ferreira_175480.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OG5DT_dm6mk"
   },
   "source": [
    "# Notebook de referência \n",
    "\n",
    "Nome: Patrick de Carvalho Tavares Rezende Ferreira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZ80hHaftwUd"
   },
   "source": [
    "## Instruções:\n",
    "\n",
    "O objetivo desse colab é entender o comportamento das seguintes variáveis importantes de treinamento:\n",
    "   - Batch size\n",
    "   - Learning rate\n",
    "   - FLOPs (computação gasta no treinamento)\n",
    "   - Tamanho do modelo\n",
    "\n",
    "Para tanto, iremos treinar e medir a loss e acurácia de 3 modelos BERT para análise de sentimento (classificação binária) usando o dataset do IMDB (20k/5k amostras de treino/validação).\n",
    "\n",
    "Iremos fazer os 3 x 5 x 5 = 75 treinamentos, cada um usando um modelo (dentre 3), uma learning rate (dentre 5 valores) e batch size (dentro 5 valores) diferentes.\n",
    "\n",
    "Os modelos sugeridos a serem usados são:\n",
    "\n",
    "*   google/bert_uncased_L-2_H-128_A-2 (BERT-tiny, 4M params, ~0.5M non-embeddings)\n",
    "*   google/bert_uncased_L-4_H-256_A-4 (BERT-mini, 11M params, ~3.5M non-embeddings)\n",
    "*   google/bert_uncased_L-8_H-512_A-8 (BERT-medium, 41M params, ~25M non-embeddings)\n",
    "\n",
    "Durante cada treinamento, iremos armezenar as seguntes informações: \n",
    "\n",
    "    - GPU usada\n",
    "    - FP16 ou 32?\n",
    "    - step atual\n",
    "    - tempo de treinamento até então (wall time)\n",
    "    - loss de treino\n",
    "    - loss de validação\n",
    "    - acurácia de validação\n",
    "    \n",
    "Iremos gravar essas informações _várias vezes por época_. Caso os treinamentos usem GPUs diferentes, podemos ajustar o wall time com base no FLOPs das GPUs.\n",
    "\n",
    "Ao final, iremos plotar os seguintes gráficos:\n",
    "\n",
    "1.   batch_size vs learning rate vs melhor loss de validação para cada modelo (usar gráfico 3D ou heatmap);\n",
    "2.   tempo de treinamento (wall time) vs loss de validação. Plotar uma série (curva) para cada modelo, todas no mesmo gráfico. Para gerar cada curva, usar os melhores batch size e learning rate encontrados no gráfico 1. \n",
    "\n",
    "Com isso conseguiremos responder às seguintes perguntas:\n",
    "\n",
    "    - Se você tiver T horas de GPU para usar, é melhor usar o modelo tiny, mini ou medium? Verifique se existe alguma faixa de valores de T em que é melhor usar o tiny. \n",
    "    - Qual modelo demora mais para atingir a sua melhor acurácia de validação em termos de épocas. E em termos de tempo de treino, wall time?\n",
    "    - Para cada X vezes que aumentamos o batch size, como que devemos ajustar a learning rate?\n",
    "    - Os melhores hiperparametros são parecidos para os 3 modelos?\n",
    "\n",
    "Notas:\n",
    "- Para entender melhor como batch size e learning rate se relacionam, procure fazer a varredura com passos de 5x ou 10x. Por exemplo:\n",
    "    \n",
    "    learning rate = {1e-2, 1e-3, ..., 1e-6}\n",
    "    \n",
    "    batch size = {1, 10, 100, 1000, 10000}\n",
    "\n",
    "- Caso o batch não caiba em memória, usar acumulo de gradiente.\n",
    "- Tempos estimados de treinamento para uma época do IMDB usando uma T4:\n",
    "    - BERT-tiny: menos de 1 minuto\n",
    "    - BERT-mini: 3 minutos\n",
    "    - BERT-medium: 10 minutos. Portanto, se treinarmos por 2 épocas, o tempo total para rodar os experimentos será de `2 épocas x 10 min x 25 treinamentos ~ 9 horas`.\n",
    "- Sugerimos fazer primeiro todos os experimentos com BERT-tiny e BERT-mini. Quando souber da faixa de hiperparametros \"bons\", não precisa fazer os 25 treinamentos para o BERT-medium.\n",
    "    - TFLOPs (FP32) de cada GPU:\n",
    "        T4: 8,141\n",
    "        K80: 4,113\n",
    "        A100: 19,49\n",
    "    - Usar time.perf_counter() para medir o wall time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9f3PfifAwpU",
    "outputId": "098c70e7-6e72-4358-c63a-58b64e11e377"
   },
   "outputs": [],
   "source": [
    "# Check which GPU we are using\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fixando a seed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Type\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whTCe2i7AtoV",
    "outputId": "1f8b6f79-6688-493b-87c4-762894b14c49"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "   dev = \"cuda:0\"\n",
    "else: \n",
    "   dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print('Using {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXFdJz2KVeQw"
   },
   "source": [
    "## Preparando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHMi_Kq65fPM"
   },
   "source": [
    "Primeiro, fazemos download do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2wbnfzst5O3k",
    "outputId": "5043b24c-213e-478a-d25a-a266a391eda3"
   },
   "outputs": [],
   "source": [
    "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
    "!tar -xzf aclImdb.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Giyi5Rv_NIm"
   },
   "source": [
    "## Carregando o dataset\n",
    "\n",
    "Criaremos uma divisão de treino (20k exemplos) e validação (5k exemplos) artificialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HIN_xLI_TuT",
    "outputId": "07a73855-b6fc-44ca-bb32-731066058db0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "max_valid = 5000\n",
    "\n",
    "def load_texts(folder):\n",
    "    texts = []\n",
    "    for path in os.listdir(folder):\n",
    "        with open(os.path.join(folder, path)) as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "x_train_pos = load_texts('aclImdb/train/pos')\n",
    "x_train_neg = load_texts('aclImdb/train/neg')\n",
    "x_test_pos = load_texts('aclImdb/test/pos')\n",
    "x_test_neg = load_texts('aclImdb/test/neg')\n",
    "\n",
    "x_train = x_train_pos + x_train_neg\n",
    "x_test = x_test_pos + x_test_neg\n",
    "y_train = [1] * len(x_train_pos) + [0] * len(x_train_neg)\n",
    "y_test = [1] * len(x_test_pos) + [0] * len(x_test_neg)\n",
    "\n",
    "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
    "c = list(zip(x_train, y_train))\n",
    "random.shuffle(c)\n",
    "x_train, y_train = zip(*c)\n",
    "\n",
    "x_valid = x_train[-max_valid:]\n",
    "y_valid = y_train[-max_valid:]\n",
    "x_train = x_train[:-max_valid]\n",
    "y_train = y_train[:-max_valid]\n",
    "\n",
    "print(len(x_train), 'amostras de treino.')\n",
    "print(len(x_valid), 'amostras de desenvolvimento.')\n",
    "print(len(x_test), 'amostras de teste.')\n",
    "\n",
    "print('3 primeiras amostras treino:')\n",
    "for x, y in zip(x_train[:3], y_train[:3]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 últimas amostras treino:')\n",
    "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 primeiras amostras validação:')\n",
    "for x, y in zip(x_valid[:3], y_test[:3]):\n",
    "    print(y, x[:100])\n",
    "\n",
    "print('3 últimas amostras validação:')\n",
    "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
    "    print(y, x[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Criando classe do dataset\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts: List[str], labels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], torch.tensor([0., 1.]) if self.labels[idx] else torch.tensor([1., 0.])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dados de treino, validação e teste"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_dataset = MyDataset(x_train, y_train)\n",
    "valid_dataset = MyDataset(x_valid, y_valid)\n",
    "test_dataset = MyDataset(x_test, y_test)\n",
    "\n",
    "print(f'training examples: {len(training_dataset)}')\n",
    "print(f'valid examples: {len(valid_dataset)}')\n",
    "print(f'test examples: {len(test_dataset)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testando se o modelo processa os dados corretamente"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "model.train().to(device)\n",
    "\n",
    "sample_train, _ = next(iter(DataLoader(training_dataset, batch_size=4)))\n",
    "\n",
    "sample_train = tokenizer.batch_encode_plus(sample_train, padding=True, return_tensors=\"pt\", truncation=True, max_length=200).to(device)\n",
    "\n",
    "print(\"model output shape: \", model(**sample_train).logits.shape)\n",
    "\n",
    "del sample_train\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TREINAMENTO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "use_amp = True\n",
    "\n",
    "n_grads_accumulated = 0\n",
    "accumulate = 8\n",
    "\n",
    "def train_step(input_ids, target_ids):\n",
    "    model.train()\n",
    "    with autocast(enabled=use_amp):\n",
    "        logits = model(**input_ids).logits\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "    loss = nn.functional.cross_entropy(logits, target_ids, )\n",
    "    scaler.scale(loss).backward()\n",
    "    if n_grads_accumulated % accumulate == 0:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        model.zero_grad()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation_step(input_ids, target_ids):\n",
    "    model.eval()\n",
    "    with autocast(enabled=use_amp):\n",
    "        logits = model(**input_ids).logits\n",
    "        loss = nn.functional.cross_entropy(logits, target_ids,)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        accuracy = (preds == target_ids.argmax(dim=1)).sum().float() / logits.shape[0]\n",
    "    return loss.item(), accuracy.item()\n",
    "\n",
    "epochs = 2\n",
    "max_examples = int(len(training_dataset) * epochs)\n",
    "\n",
    "rtx3060_tflops = 12.74 # https://www.techpowerup.com/gpu-specs/geforce-rtx-3060.c3682\n",
    "\n",
    "model_list = [\"google/bert_uncased_L-2_H-128_A-2\",\n",
    "              \"google/bert_uncased_L-4_H-256_A-4\",\n",
    "              \"google/bert_uncased_L-8_H-512_A-8\",][::-1]\n",
    "batch_sizes = [8, 16, 32, 64, 128, 256][::-1]\n",
    "learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "log_df = pd.DataFrame.from_dict({\n",
    "    \"model_name\": [],\n",
    "    \"batch_size\": [],\n",
    "    \"lr\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"best_acc\": [],\n",
    "    \"wall_time\": []\n",
    "})\n",
    "\n",
    "for model_name in tqdm(model_list):\n",
    "    for batch_size in batch_sizes:\n",
    "        for lr in learning_rates:\n",
    "            model_identifier = model_name + \"_batch_size_\" + str(batch_size) + \"_lr_\" + str(lr)\n",
    "            print(\"Current Setup: \", model_identifier)\n",
    "\n",
    "            model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "            model.train().to(device)\n",
    "\n",
    "            eval_every_steps = 984 * 64// batch_size\n",
    "\n",
    "            train_loader = DataLoader(training_dataset, batch_size=batch_size//accumulate, shuffle=True, num_workers=1)\n",
    "            validation_loader = DataLoader(valid_dataset, batch_size=batch_size//accumulate, num_workers=1, )\n",
    "\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            scaler=GradScaler()\n",
    "\n",
    "            best_validation_loss = 9999\n",
    "            best_validation_acc = 0\n",
    "            convergence_time = 0\n",
    "            train_losses = []\n",
    "            n_examples = 0\n",
    "            step = 0\n",
    "            pbar = tqdm(total=max_examples)\n",
    "            start_time = time.time()\n",
    "            while n_examples < max_examples:\n",
    "                n_grads_accumulated = 0\n",
    "                val_losses = []\n",
    "                wall_times = []\n",
    "                for train_input_ids, train_target_ids in train_loader:\n",
    "                    train_input_ids = tokenizer.batch_encode_plus(train_input_ids, padding=True, return_tensors=\"pt\", truncation=True, max_length=200).to(device)\n",
    "                    n_grads_accumulated = n_grads_accumulated + 1\n",
    "                    loss = train_step(train_input_ids, train_target_ids.to(device))\n",
    "                    train_losses.append(loss)\n",
    "\n",
    "                    if step % eval_every_steps == 0:\n",
    "                        train_loss = np.average(train_losses)\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            valid_result = list(zip(*[\n",
    "                                validation_step(tokenizer.batch_encode_plus(val_input_ids, padding=True, return_tensors=\"pt\", truncation=True, max_length=200).to(device), val_target_ids.to(device))\n",
    "                                for val_input_ids, val_target_ids in validation_loader]))\n",
    "                            valid_loss = np.average(valid_result[0])\n",
    "                            valid_acc = np.average(valid_result[1])\n",
    "                            # Checkpoint to best models found.\n",
    "                            if best_validation_loss > valid_loss:\n",
    "                                # Update the new best perplexity.\n",
    "                                best_validation_loss = valid_loss\n",
    "                                best_validation_acc = valid_acc\n",
    "                                convergence_time = time.time() - start_time\n",
    "\n",
    "                        val_losses.append(valid_loss)\n",
    "                        wall_times.append((time.time()-start_time) * rtx3060_tflops)\n",
    "                        print(f'{step} steps; {n_examples} examples so far; train loss: {train_loss:.2f}, valid loss: {valid_loss:.2f}')\n",
    "                        train_losses = []\n",
    "\n",
    "                    n_examples += train_input_ids.input_ids.shape[0] # Increment of batch size\n",
    "                    step += 1\n",
    "                    pbar.update(train_input_ids.input_ids.shape[0])\n",
    "                    if n_examples >= max_examples:\n",
    "                        break\n",
    "\n",
    "            log_df = log_df.append({\n",
    "                \"model_name\": model_name,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"lr\": lr,\n",
    "                \"val_loss\": best_validation_loss,\n",
    "                \"best_acc\": best_validation_acc,\n",
    "                \"wall_time\": convergence_time * rtx3060_tflops,\n",
    "                \"losses_list\": val_losses,\n",
    "                \"times\": wall_times,\n",
    "            }, ignore_index=True)\n",
    "\n",
    "            pbar.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exibindo a melhor loss de Validação para cada modelo, batch_size e learning rate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for model_name in model_list:\n",
    "    print(\"Modelo: \", model_name)\n",
    "\n",
    "    df = log_df.loc[log_df['model_name'] == model_name]\n",
    "\n",
    "    df = df[[\"batch_size\", \"lr\", \"val_loss\"]]\n",
    "\n",
    "    sns.heatmap(df.pivot(index='batch_size', columns='lr', values='val_loss'), annot=True, cmap=\"viridis\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Curvas de loss para os melhores resultados de cada modelo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model_name in model_list:\n",
    "\n",
    "\n",
    "    idx = log_df.loc[log_df['model_name'] == model_name][\"val_loss\"].idxmin()\n",
    "\n",
    "    y = log_df.at[idx, \"losses_list\"]\n",
    "    x = log_df.at[idx, \"times\"]\n",
    "\n",
    "    plt.plot(x, y, label=model_name)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.xlabel('Wall Time')\n",
    "plt.ylabel('Val Loss')\n",
    "\n",
    "plt.grid()\n",
    "plt.savefig(\"roxo.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}